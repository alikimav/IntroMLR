---
title: "Working with the mlr package"
output: html_document
---

To get acquainted with the mlr package, I created an Rmarkdown document with basic and advanced features from the mlr tutorial on regression and classification problems with data available in R. 

```{r, echo=FALSE, message=FALSE}
library(mlr)
library(randomForest)
library(gbm)
library(mlbench)
library(rattle)
```
## Getting started

The first step is to create a **task**; this can be a classification, regression, survival, cluster, cost-sensitive classification or multilabel task. For example, let us start with the `iris` data set from which we wish to predict the `Species` based on features comprised of width and length measurements of sepals and petals. 

```{r}
task <- makeClassifTask(data = iris, target = "Species")
```

There exist other options that can be set when calling the task function like, for instance, weights and blocking. The latter is for observations that are required to be considered together such that when resampling or cross-validating, grouped observations are included either all in the train or all in the test set.

Next, we choose a learner. Here, we choose to learn based on a classification tree fitted through the `rpart` function. 

```{r}
lrn <- makeLearner("classif.rpart")
```

We can get a description of all possible parameter settings for a learner using `?getParamSet(lrn)`:

```{r, results='markup'}
getParamSet(lrn)
```

Further, we can create a description object for a resampling startegy using `makeResampleDesc`. For example, we may wish to carry out a 3-fold cross-validation of `rpart` on `iris`.

```{r}
cv3f <- makeResampleDesc("CV", iters = 3, stratify = TRUE )
```

Finally, we can fit the model specified by `lrn` on the `task` and calculate predictions and performance measures for all training and all test sets specified by the above resampling description.

```{r, message=FALSE}
set.seed(123)
r <- resample(lrn, task, cv3f)
r$aggr 
```

The mean misclassification error is given by `r r$aggr`. The default measure for a classification task is the misclassification error, `mmce` but it is possible to choose a different measure, say *accuracy*, `acc` which is given by  1 - `mmce`. We will see later that we can also define our own measures. 

```{r, message=FALSE}
set.seed(123)
r <- resample(lrn, task, cv3f, measures = acc)
r$aggr 
```



## Basics

### Tasks

As previously mentioned, depending on the type of problem at hand, one can define an appropriate task. We have seen an instance of a classification problem above, let us now look at a supervised regression problem using the `BostonHousing` data. By printing task, we get some information on the task object we have just created and the associated data.

```{r}
data(BostonHousing, package = "mlbench")
task <- makeRegrTask(data = BostonHousing, target = "medv")
print(task)
```

#### Modifying a task

Once the task is created, it can be modified in several ways. For example, the function `subsetTask` allows the selection of certain observations and/or features. 

```{r}
names(BostonHousing)
modifiedtask <- subsetTask(task, subset = 1:400, features = c("crim", "age", "dis", "lstat"))
str(getTaskData(modifiedtask))
```

Using `getTaskData` we can see that the modified task now includes 400 observations and 5 variables (the 4 included in the features character vector above and the target variable which will always be included). Some other useful functions are the following:

  1. `removeConstantFeatures(<task name>)`:
  
  constant features may arise dur to an inherent feature in the data collected or as a result of choosing a subset of observations.
  
  2. `dropFeatures(task, c("rm", "nox"))`:
  
  remove selected features from the task.
  
  3. `normalizeFeatures(task, method = "standardize")`:
  
  normalize numerical features by different methods (nonnumerical features are left untouched). The normalizing method can be one of `center` (subtract mean), `scale` (divide by standard deviation), `standardize` (center and scale), `range` (scale to a given range - default is [0, 1]). The optional argument `exclude` may be used to supply a character vector of columns to be excluded from the normalization. Type `?normalizeFeatures` for more info.

### Learners

Many of the popular learning algorithms are already implemented in `mlr`. The `makeLearner` function requires the user to specify the learning method. Additionally, it is possible to modify defaults on the prediction type (i.e. for classification, we may choose `predict.type = "prob"` for probabilities), or set hyperparameters using a `list` passed to the `par.vals` argument.

```{r}
class.lrn <- makeLearner("classif.randomForest", predict.type = "prob", fix.factors.prediction = TRUE)
regr.lrn <- makeLearner("regr.gbm", par.vals = list(n.trees = 500, interaction.depth = 4))
#cluster.lrn <- makeLearner("cluster.SimpleKMeans", N = 5)
```

Note that the `fix.factors.prediction = TRUE` argument is useful in situations where a factor level is present in the training data set but not the test data set; it adds a factor level for missing data in the test set thus avoiding problems. 

The Learner object is a list and information can be extracted using the `$`, e.g. `regr.lrn$par.vals`. This information can also be accessed using other functions in `mlr`, for instance, `getHyperPars(lrn)` retrieves the current hyperparameter settings of the learner `lrn`. We also used above `getParamSet(lrn)` to get a description of all possible parameter settings for `lrn`. 

#### Modifying a learner

Just like it was possible to modify an existing task, we can also do so for a learner. Modifications include changing the `id` (this is either user specified or, if omitted, it is automatically set to the algorithm name), the prediction type, hyperparameter values, and more.

```{r}
class.lrn <- setPredictType(class.lrn, "response")
regr.lrn <- setHyperPars(regr.lrn, n.trees = 400)
regr.lrn <- removeHyperPars(regr.lrn, c("n.trees", "interaction.depth"))
```

Note `removeHyperPars` sets the hyperparameters back to their default values.

### Train

Once we create the task from the data set and identify the learning algorithm, the next step is to train the learner using the `train` command.

```{r}
task <- makeClassifTask(data = iris, target = "Species")
lrn <- makeLearner("classif.lda")
mod <- train(lrn, task)
mod
```

The function `train` returns a list. The fitted model can be extracted using the `getLearnerModel(mod)` command. It is possible to choose a subset of observations to be used to train the model; this is achieved via the `subset` argument in `train`. Note that this is usually not needed since resampling strategies are supported. In the following example, we use the `BostonHousing` data to fit a linear model to the regression task. Note that `getTaskSize` returns the number of observations in a task.

```{r}
data(BostonHousing, package = "mlbench")
task <- makeRegrTask(data = BostonHousing, target = "medv")
lrn <- makeLearner("regr.lm")
train.set <- sample(getTaskSize(task), size = getTaskSize(task)/3)
mod <- train(lrn, task, subset = train.set)
getLearnerModel(mod)
```

Finally, a note on weights passed as an argument in the `train` function. As an example for an application of weights ^[note that mlr offers alternatives with more functionality for imbalanced classification problems] used in `train`, consider the `BreastCancer` data for which the target variable `Class` identifies 241 malignant and 458 benign cases. To deal with the imbalanced classes, we can incorporate weights in an attempt to allow the two classes to be equally represented in training the classifier. Here we use the predefined task `bc.task` in mlr. The `getTaskTargets` function gets the target data from a task (in this example, this is equivalent to the vector `BreastCancer$Class`). Note that if weights are defined in task as well then those would be overwritten by the weights in `train`. 

```{r}
target <- getTaskTargets(bc.task)
tab <- as.numeric(table(target))
#obtain inverse class frequencies for the weights
w <- 1/tab[target] 
mod <- train("classif.rpart", task = bc.task, weights = w)
fancyRpartPlot(getLearnerModel(mod), sub = "")
```

Weights can also be useful as a means to grant more importance to recently collected data versus older data or to reduce the influence of outliers.

### Predict

To predict target values, we use the `predict` function which takes as input the object returned by `train` and data for which we want predictions. The data can either come from the task (using the `task` argument) or it can be a data frame (passed using the `newdata` argument). Similarly to the `train` function, the `subset` argument may be used to pass different portions of the data in task. 

```{r}
n <- getTaskSize(bh.task)
train.set <- seq(1, n, by = 2)
test.set <- seq(2, n, by = 2)
lrn <- makeLearner("regr.gbm", n.trees = 100, interaction.depth = 4)
mod <- train(lrn, bh.task, subset = train.set)
preds <- predict(mod, task = bh.task, subset = test.set)
preds
```

The function `predict` returns a list; The `$data` element of that list contains the true values of the target variable (in case of supervised learning) and the predictions. A direct way to obtain the true and predicted values of the target variable is through the `getPredictionTruth(preds)` and `getPredictionResponse(preds)` commands where `preds` is the list returned by the `predict` function.

```{r}
head(getPredictionTruth(preds), 10)
head(getPredictionResponse(preds), 10)
```

For classification problems, class labels are predicted. We can obtain a confusion matrix through the command `getConfMatrix`. To get predicted posterior probabilities, we need to create the learner with `predict.type = "prob"`.

```{r}
lrn <- makeLearner("classif.rpart", predict.type = "prob")
mod <- train(lrn, iris.task)
preds <- predict(mod, newdata = iris)
head(as.data.frame(preds))
head(getPredictionProbabilities(preds))
```

We can also adjust the threshold value that is used to map the predicted posterior probabilities to the class labels. The default value for binary classification is 0.5; however, it may be necessary to increase/decrease this value in various situations such as in the case of classifying cancer rates where one would be concerned with false negatives (prediction of no cancer when the truth is yes cancer). By changing the threshold we change the sensitivity of the model. An example for adjusting the threshold in a binary classification setting is shown below. 

```{r}
data(Sonar)
table(Sonar$Class)
getTaskDescription(sonar.task)$positive
lrn <- makeLearner("classif.rpart", predict.type = "prob")
mod <- train(lrn, task = sonar.task)
# predict with default threshold
preds <- predict(mod, sonar.task)
preds$threshold
# set threshold value for +ve class
preds2 <- setThreshold(preds, 0.9)
# confusion matrices
getConfMatrix(preds)
getConfMatrix(preds2)
```

For multiclass classification problems, the threshold is given by a named vector specifying the values by which each probability will be divided. 

### Performance

There are many available performance measures implemented in mlr but one can also create their own performance measures. For a particular prediction object, say `preds` calling `performance(preds)` will give the calculated performance measure. It is also possible to calculate the time needed to train the learner (passing `timetrain` as an argument - see below), the time needed to compute the prediction (`timepredict`) or both (`timeboth`). 

To obtain a list of available measures suitable for a particular problem type or `task`, use the `listMeasures` argument. The function `getDefaultMeasure` shows the defaults for a particular learner or task.

```{r}
listMeasures("classif", properties = "classif.multi")
getDefaultMeasure(bh.task)
```

The following piece of `R` code shows how to obtain the performance measure from the prediction object.

```{r}
n <- getTaskSize(bh.task)
train.set <- seq(1, n, by = 2)
test.set <- seq(1, n, by = 2)
lrn <- makeLearner("regr.gbm", n.trees = 1000)
mod <- train(lrn, task = bh.task, subset = train.set)
preds <- predict(mod, task = bh.task, subset = test.set)
performance(preds)
```

To change the performance measure, we can do so via the `measures` argument. It is possible to calculate several performance measures by passing them as a list.

```{r}
performance(preds, measures = medse)
performance(preds, measures = list(mse, medse, mae))
```

It is a necessary requirement to pass the model or the task in order to calculate some performance measures. For instance, for `timetrain` calculations, the model also needs to be passed as ar argument.

```{r}
performance(preds, measures = timetrain, model = mod)
```

For clustering problems, the task is required.

```{r}
lrn <- makeLearner("cluster.kmeans", centers = 3)
mod <- train(lrn, task = mtcars.task)
preds <- predict(mod, task = mtcars.task)
performance(preds, measures = dunn, task = mtcars.task)
```


As previously mentioned, the threshold for classification problems alters the sensitivity of the model and therefore affects performance. The command `generateThreshVsPerfData` used on the prediction object along with a performance measure (or a list of them) generates data on the learner performance versus the threshold. T

```{r}
lrn <- makeLearner("classif.lda", predict.type = "prob")
n <- getTaskSize(sonar.task)
train.set <- seq(1, n, by = 2)
test.set <- seq (2, n, by = 2)
mod <- train(lrn, task = sonar.task, subset = train.set)
preds <- predict(mod, task = sonar.task, subset = test.set)
performance(preds, measures = list(fpr, fnr, mmce))
d <- generateThreshVsPerfData(preds, measures = list(fpr, fnr, mmce))
plotThreshVsPerf(d)
```

