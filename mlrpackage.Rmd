---
title: "Working with the mlr package"
output: html_document
---

To become familiar with the mlr package and its widespread functionality, I created an Rmarkdown document with basic and advanced features from the mlr tutorial on mostly regression and classification problems with data available in R. 

```{r, echo=FALSE, message=FALSE}
library(mlr)
library(randomForest)
library(gbm)
library(mlbench)
library(rattle)
```
## Getting started

The first step is to create a **task**; this can be a classification, regression, survival, cluster, cost-sensitive classification or multilabel task. For example, let us start with the `iris` data set from which we wish to predict the `Species` based on features comprised of width and length measurements of sepals and petals. 

```{r}
task <- makeClassifTask(data = iris, target = "Species")
```

There exist other options that can be set when calling the task function like, for instance, weights and blocking. The latter is for observations that are required to be considered together such that when resampling or cross-validating, grouped observations are included either all in the train or all in the test set.

Next, we choose a learner. Here, we choose to learn based on a classification tree fitted through the `rpart` function. 

```{r}
lrn <- makeLearner("classif.rpart")
```

We can get a description of all possible parameter settings for a learner using `?getParamSet(lrn)`:

```{r, results='markup'}
getParamSet(lrn)
```

Further, we can create a description object for a resampling startegy using `makeResampleDesc`. For example, we may wish to carry out a 3-fold cross-validation of `rpart` on `iris`.

```{r}
cv3f <- makeResampleDesc("CV", iters = 3, stratify = TRUE )
```

Finally, we can fit the model specified by `lrn` on the `task` and calculate predictions and performance measures for all training and all test sets specified by the above resampling description.

```{r, message=FALSE}
set.seed(123)
r <- resample(lrn, task, cv3f)
r$aggr 
```

The mean misclassification error is given by `r r$aggr`. The default measure for a classification task is the misclassification error, `mmce` but it is possible to choose a different measure, say *accuracy*, `acc` which is given by  1 - `mmce`. We will see later that we can also define our own measures. 

```{r, message=FALSE}
set.seed(123)
r <- resample(lrn, task, cv3f, measures = acc)
r$aggr 
```



## Basics

### Tasks

As previously mentioned, depending on the type of problem at hand, one can define an appropriate task. We have seen an instance of a classification problem above, let us now look at a supervised regression problem using the `BostonHousing` data. By printing task, we get some information on the task object we have just created and the associated data.

```{r}
data(BostonHousing, package = "mlbench")
task <- makeRegrTask(data = BostonHousing, target = "medv")
print(task)
```

#### Modifying a task

Once the task is created, it can be modified in several ways. For example, the function `subsetTask` allows the selection of certain observations and/or features. 

```{r}
names(BostonHousing)
modifiedtask <- subsetTask(task, subset = 1:400, features = c("crim", "age", "dis", "lstat"))
str(getTaskData(modifiedtask))
```

Using `getTaskData` we can see that the modified task now includes 400 observations and 5 variables (the 4 included in the features character vector above and the target variable which will always be included). Some other useful functions are the following:

  1. `removeConstantFeatures(<task name>)`:
  
  constant features may arise dur to an inherent feature in the data collected or as a result of choosing a subset of observations.
  
  2. `dropFeatures(task, c("rm", "nox"))`:
  
  remove selected features from the task.
  
  3. `normalizeFeatures(task, method = "standardize")`:
  
  normalize numerical features by different methods (nonnumerical features are left untouched). The normalizing method can be one of `center` (subtract mean), `scale` (divide by standard deviation), `standardize` (center and scale), `range` (scale to a given range - default is [0, 1]). The optional argument `exclude` may be used to supply a character vector of columns to be excluded from the normalization. Type `?normalizeFeatures` for more info.

### Learners

Many of the popular learning algorithms are already implemented in `mlr`. The `makeLearner` function requires the user to specify the learning method. Additionally, it is possible to modify defaults on the prediction type (i.e. for classification, we may choose `predict.type = "prob"` for probabilities), or set hyperparameters using a `list` passed to the `par.vals` argument.

```{r}
class.lrn <- makeLearner("classif.randomForest", predict.type = "prob", fix.factors.prediction = TRUE)
regr.lrn <- makeLearner("regr.gbm", par.vals = list(n.trees = 500, interaction.depth = 4))
#cluster.lrn <- makeLearner("cluster.SimpleKMeans", N = 5)
```

Note that the `fix.factors.prediction = TRUE` argument is useful in situations where a factor level is present in the training data set but not the test data set; it adds a factor level for missing data in the test set thus avoiding problems. 

The Learner object is a list and information can be extracted using the `$`, e.g. `regr.lrn$par.vals`. This information can also be accessed using other functions in `mlr`, for instance, `getHyperPars(lrn)` retrieves the current hyperparameter settings of the learner `lrn`. We also used above `getParamSet(lrn)` to get a description of all possible parameter settings for `lrn`. 

#### Modifying a learner

Just like it was possible to modify an existing task, we can also do so for a learner. Modifications include changing the `id` (this is either user specified or, if omitted, it is automatically set to the algorithm name), the prediction type, hyperparameter values, and more.

```{r}
class.lrn <- setPredictType(class.lrn, "response")
regr.lrn <- setHyperPars(regr.lrn, n.trees = 400)
regr.lrn <- removeHyperPars(regr.lrn, c("n.trees", "interaction.depth"))
```

Note `removeHyperPars` sets the hyperparameters back to their default values.

### Train

Once we create the task from the data set and identify the learning algorithm, the next step is to train the learner using the `train` command.

```{r}
task <- makeClassifTask(data = iris, target = "Species")
lrn <- makeLearner("classif.lda")
mod <- train(lrn, task)
mod
```

The function `train` returns a list. The fitted model can be extracted using the `getLearnerModel(mod)` command. It is possible to choose a subset of observations to be used to train the model; this is achieved via the `subset` argument in `train`. Note that this is usually not needed since resampling strategies are supported. In the following example, we use the `BostonHousing` data to fit a linear model to the regression task. Note that `getTaskSize` returns the number of observations in a task.

```{r}
data(BostonHousing, package = "mlbench")
task <- makeRegrTask(data = BostonHousing, target = "medv")
lrn <- makeLearner("regr.lm")
train.set <- sample(getTaskSize(task), size = getTaskSize(task)/3)
mod <- train(lrn, task, subset = train.set)
getLearnerModel(mod)
```

Finally, a note on weights passed as an argument in the `train` function. As an example for an application of weights ^[note that mlr offers alternatives with more functionality for imbalanced classification problems] used in `train`, consider the `BreastCancer` data for which the target variable `Class` identifies 241 malignant and 458 benign cases. To deal with the imbalanced classes, we can incorporate weights in an attempt to allow the two classes to be equally represented in training the classifier. Here we use the predefined task `bc.task` in mlr. The `getTaskTargets` function gets the target data from a task (in this example, this is equivalent to the vector `BreastCancer$Class`). Note that if weights are defined in task as well then those would be overwritten by the weights in `train`. 

```{r}
target <- getTaskTargets(bc.task)
tab <- as.numeric(table(target))
#obtain inverse class frequencies for the weights
w <- 1/tab[target] 
mod <- train("classif.rpart", task = bc.task, weights = w)
fancyRpartPlot(getLearnerModel(mod), sub = "")
```

Weights can also be useful as a means to grant more importance to recently collected data versus older data or to reduce the influence of outliers.

### Predict

To predict target values, we use the `predict` function which takes as input the object returned by `train` and data for which we want predictions. The data can either come from the task (using the `task` argument) or it can be a data frame (passed using the `newdata` argument). Similarly to the `train` function, the `subset` argument may be used to pass different portions of the data in task. 

```{r}
n <- getTaskSize(bh.task)
train.set <- seq(1, n, by = 2)
test.set <- seq(2, n, by = 2)
lrn <- makeLearner("regr.gbm", n.trees = 100, interaction.depth = 4)
mod <- train(lrn, bh.task, subset = train.set)
preds <- predict(mod, task = bh.task, subset = test.set)
preds
```

The function `predict` returns a list; The `$data` element of that list contains the true values of the target variable (in case of supervised learning) and the predictions. A direct way to obtain the true and predicted values of the target variable is through the `getPredictionTruth(preds)` and `getPredictionResponse(preds)` commands where `preds` is the list returned by the `predict` function.

```{r}
head(getPredictionTruth(preds), 10)
head(getPredictionResponse(preds), 10)
```

For classification problems, class labels are predicted. We can obtain a confusion matrix through the command `getConfMatrix`. To get predicted posterior probabilities, we need to create the learner with `predict.type = "prob"`.

```{r}
lrn <- makeLearner("classif.rpart", predict.type = "prob")
mod <- train(lrn, iris.task)
preds <- predict(mod, newdata = iris)
head(as.data.frame(preds))
head(getPredictionProbabilities(preds))
```

We can also adjust the threshold value that is used to map the predicted posterior probabilities to the class labels. The default value for binary classification is 0.5; however, it may be necessary to increase/decrease this value in various situations such as in the case of classifying cancer rates where one would be concerned with false negatives (prediction of no cancer when the truth is yes cancer). By changing the threshold we change the sensitivity of the model. An example for adjusting the threshold in a binary classification setting is shown below. 

```{r}
data(Sonar)
table(Sonar$Class)
getTaskDescription(sonar.task)$positive
lrn <- makeLearner("classif.rpart", predict.type = "prob")
mod <- train(lrn, task = sonar.task)
# predict with default threshold
preds <- predict(mod, sonar.task)
preds$threshold
# set threshold value for +ve class
preds2 <- setThreshold(preds, 0.9)
# confusion matrices
getConfMatrix(preds)
getConfMatrix(preds2)
```

For multiclass classification problems, the threshold is given by a named vector specifying the values by which each probability will be divided. 

### Performance

There are many available performance measures implemented in mlr but one can also create their own performance measures. For a particular prediction object, say `preds` calling `performance(preds)` will give the calculated performance measure. It is also possible to calculate the time needed to train the learner (passing `timetrain` as an argument - see below), the time needed to compute the prediction (`timepredict`) or both (`timeboth`). 

To obtain a list of available measures suitable for a particular problem type or `task`, use the `listMeasures` argument. The function `getDefaultMeasure` shows the defaults for a particular learner or task.

```{r}
listMeasures("classif", properties = "classif.multi")
getDefaultMeasure(bh.task)
```

The following piece of `R` code shows how to obtain the performance measure from the prediction object.

```{r}
n <- getTaskSize(bh.task)
train.set <- seq(1, n, by = 2)
test.set <- seq(1, n, by = 2)
lrn <- makeLearner("regr.gbm", n.trees = 1000)
mod <- train(lrn, task = bh.task, subset = train.set)
preds <- predict(mod, task = bh.task, subset = test.set)
performance(preds)
```

To change the performance measure, we can do so via the `measures` argument. It is possible to calculate several performance measures by passing them as a list.

```{r}
performance(preds, measures = medse)
performance(preds, measures = list(mse, medse, mae))
```

It is a necessary requirement to pass the model or the task in order to calculate some performance measures. For instance, for `timetrain` calculations, the model also needs to be passed as ar argument.

```{r}
performance(preds, measures = timetrain, model = mod)
```

For clustering problems, the task is required.

```{r}
lrn <- makeLearner("cluster.kmeans", centers = 3)
mod <- train(lrn, task = mtcars.task)
preds <- predict(mod, task = mtcars.task)
performance(preds, measures = dunn, task = mtcars.task)
```


As previously mentioned, the threshold for classification problems alters the sensitivity of the model and therefore affects performance. The command `generateThreshVsPerfData` used on the prediction object along with a performance measure (or a list of them) generates data on the learner performance versus the threshold. T

```{r}
lrn <- makeLearner("classif.lda", predict.type = "prob")
n <- getTaskSize(sonar.task)
train.set <- seq(1, n, by = 2)
test.set <- seq (2, n, by = 2)
mod <- train(lrn, task = sonar.task, subset = train.set)
preds <- predict(mod, task = sonar.task, subset = test.set)
performance(preds, measures = list(fpr, fnr, mmce))
d <- generateThreshVsPerfData(preds, measures = list(fpr, fnr, mmce))
plotThreshVsPerf(d)
```

### Resampling

Resampling strategies are often used to assess the performance of a learning algorithm by splitting the data into multiple training and test sets. Each training set is used to train a learner and each test set is reserved for predictions. We get the mean performance measure obtained by agreggating all individual performances. The `makeResampleDesc` function is used to choose the resampling strategy; the method argument can be set to one of cross-validation, leave-one-out cross-validation, repeated cross-validation, out-of-bag bootstrap, subsampling (a.k.a. *Monte-Carlo* cross-validation), and holdout. Additional arguments can be passed to the function depending on the chosen method. Once the resampling description is specified, we use the function `resample` to fit a model specified by a learner on a task, which calculates predictions and performance measures for all training and test sets as specified by the resampling description.

```{r}
rdesc <- makeResampleDesc("CV", iters = 3, stratify = TRUE)
task <- makeClassifTask(data = iris, target = "Species")
lrn <- makeLearner("classif.rpart")
r <- resample(lrn, task, rdesc)
r
r$measures.test
r$aggr
r$measures.train
```

The `resample` function returns a list whose elements we can access using the `$` notation. In the above exmaple `r$measures.test` gives the performance on the 3 individual test sets (as specified in the resampling description) and `r$measures.train` returns missing values since no predictions on the training sets were made. If we wish to have predictions on the training sets we can set `predict = "both"` or `predict = "train"` as an argument in `makeResampleDesc`. 

Next, we can access `r$pred$data` which gives a data frame of the predictions and true (in supervised learning) values of the target variable. Note that it is possible to pass multiple measures as a list in `resample` (including `timetrain`). 

```{r}
head(r$pred$data)
```

*Stratified resampling* ensures that the same proportion of the classes falls in all partitions of the data such that, in a classification setting, each training/test set trains a model with no class being under-represented. This is especially important in small data sets as well as imbalanced classification problems. The `stratify = TRUE` argument is passed when making the resampling description (as in the `R` code above). The `stratify.cols = <col. name>` argument is used to stratify factor variable inputs to ensure that all subgroups are represented in the data partitions.

#### Accessing individual learners

By default, `resample` does not return the individual learners but can do so by passing the argument `models = TRUE` when calling `resample`. More useful still, is to extract certain information from each model like, for instance, the variable importance for chosen models. This is achieved via the `extract` argument.

```{r}
rdesc <- makeResampleDesc("CV", iters = 3)
lrn <- makeLearner("regr.rpart")
r <- resample(lrn, task = bh.task, resampling = rdesc, extract = function(x)
  x$learner.model$variable.importance, models = FALSE)
r$extract
```

Now, `r$extract` is going to give information on the given function from which we asked to obtain the variable importance. With `models = FALSE` we do not obtain other information on the model so attempting to call `getLearnerModel(r$models[[1]])` would return an error. 

#### Resample instance

The command `makeResampleInstance` takes as arguments an object of class ResampleDesc and a task [or the size of the data set i.e. nrow(<data>)]. This creates a ResampleInstance object which mainly stores the indices of the training and test sets used in each iteration. This feature may be useful in situations where we want to perform paired experiments like testing the performance of several learners on exactly the same data. 

```{r}
rdesc <- makeResampleDesc("CV", iters = 3, stratify = TRUE)
rin <- makeResampleInstance(rdesc, task = iris.task)
rin$train.inds[[2]]
```

In `makeResampleInstance` the indices are drawn randomly; the function `makeFixedHoldoutInstance` allows the training and test sets to be specified manually. 

```{r}
rin <- makeFixedHoldoutInstance(train.inds = 1:100, test.inds = 101:150, size = 150)
rin
```

In resampling startegies, we get a performance measure which is aggregated over all the measures calculated for each iteration. By default, the aggregated score is the mean error on the test set. We can change the aggregation method for a measure via `setAggregation(measure, aggr)`. For the different options for `measure` and `aggr` type `?measures` and `?aggregations`, respectively. 

```{r}
m1 <- mmce
m2 <- setAggregation(tpr, test.median) # tpr = true positive rate
rdesc <- makeResampleDesc("CV", iters = 3)
r <- resample("classif.rpart", task = sonar.task, resampling = rdesc, measures = list(m1,m2))
r$aggr
```

To get predictions on both training and test sets, we need to set `predict = "both"` in `makeResampleDesc` and set the aggregation method to `train.mean` if we wish to calculate the mean. An example is shown below.

```{r}
train.mmce <- setAggregation(mmce, train.mean)
rdesc <- makeResampleDesc("CV", iters = 3, predict = "both")
r <- resample("classif.rpart", task = sonar.task, resampling = rdesc, measures = list(mmce, train.mmce))
r$aggr
```

There exist functions which act as convenience wrappers for the various existing resampling strategies. They don't offer as much flexibility as `resample` but can be quick and useful if trying out a number of learners initially. An example with `crossval` is shown below.

```{r}
cviris <- crossval("classif.lda", task = iris.task, iters = 3, measures = list(mmce, ber))
cviris
```


### Benchmark experiments

Using `benchmark`, we can compare different learning algorithms across one or more tasks w.r.t. a given resampling strategy. The function has the advantage of conducting *paired* experiments thus comparing the same training/test sets for the different learners. In the following `R` code, we specify a single task (here, we use the built in `sonar.task`) and apply a LDA and a classification tree learner. The resampling strategy is chosen as `Holdout` hence the performance is calculated on a single randomly sampled test set.

```{r}
lrns <- list(makeLearner("classif.lda"), makeLearner("classif.rpart"))
rdesc <- makeResampleDesc("Holdout")
bmr <- benchmark(learners = lrns, tasks = sonar.task, resamplings = rdesc)
bmr
```

The function `benchmark` returns an object of class BenchmarkResult which contains a list of lists of ResampleResult objects ordered by task and followed by learner. The mlr `getBMR<...>` commands
allows access to the benchmark results.

#### Learner performances

```{r}
getBMRPerformances(bmr, as.df = TRUE)
getBMRAggrPerformances(bmr, as.df = TRUE)
```

The two results (top: individual performance in resampling runs, bottom: aggregated performance values) coincide since `Holdout` was used as the resampling strategy. The optional argument `as.df = TRUE` returns the results in the form of a data frame which is often more convenient. 

#### Predictions

By default, `keep.pred = TRUE` in `benchmark` which allows the user to access the predictions with  `getBMRPredictions`. If `keep.pred = FALSE`, the following command will result in an error.

```{r}
head(getBMRPredictions(bmr, as.df = TRUE))
```

The learner and task ID is automatically set to the name of the algorithm and task if not explicitly specified. Using the ID in `getBMRPredictions`, it is possible to access results for certain learners or tasks. For instance below, we obtain the predictions from the classification tree learner. 

```{r}
head(getBMRPredictions(bmr, learner.ids = "classif.rpart", as.df = TRUE))
```

If unsure of the ID for the learner, task, and performance measure, they can be accessed using `getBMRLearnerIds`,  `getBMRTaskIds`, and `getBMRMeasureIds`, respectively.

#### Models

Similarly, to *Predictions* above, the argument `models = FALSE` must be set in `benchmark` if the user does not want to keep the fitted models for all learners and tasks. If set to true (default), the models may be accessed through `getBMRModels`.

```{r}
getBMRModels(bmr, learner.ids = "classif.lda")
```

More `getBMR<...>` functions exist to extract information on learners, measures, and more.

#### Additional benchmark experiments and merging results

Once a benchmark experiment has been conducted on a task, we may need to add more learners (or, alternatively, we may wish to extend existing learners to other tasks). We can perform another benchmark experiment and then merge the results (through either `mergeBenchmarkResultLearner` or `mergeBenchmarkResultTask`). As an example, we perform another benchmark experiment on `sonar.task` now with random forest and quadratic discriminant analysis learning algorithms. We then fuse this with the  `bmr` object we obtained above to get a single `BenchmarkResult` object.

```{r}
lrns2 <- list(makeLearner("classif.randomForest"), makeLearner("classif.qda"))
bmr2 <- benchmark(learners = lrns2, tasks = sonar.task, resamplings = rdesc, show.info = FALSE)
bmr2
bmrsingle <- mergeBenchmarkResultLearner(bmr, bmr2)
bmrsingle
```

However, note that the resampling description was passed to `benchmark` twice, once to obtain the `bmr` and then the `bmr2` objects. The training/test pairs therefore were most likely different in the first benchmark call than the second. For more accurate merging of benchmark results, we can opt to work with `ResampleInstance` from the start, or extract the `ResampleInstance` from the resample description in the first benchmark call and pass it as argument in later benchmark calls. An example is given below.

```{r}
rin <- getBMRPredictions(bmr)[[1]][[1]]$instance
rin
bmr3 <- benchmark(learners = lrns2, tasks = sonar.task, resamplings = rin, show.info = FALSE)
mergeBenchmarkResultLearner(bmr, bmr3)
```

#### Benchmark analysis & visualization

Once benchmark experiments are conducted for the various learners and tasks, we may wish to rank and assess the performance of various algorithms, perform hypotheses tests or visualize the results. The mlr package offers various functions to do so and we explore some of these below using a longer benchmark example than the one above.

```{r}
# list of 3 learners
lrns <-list(makeLearner("classif.lda", id = "lda"), makeLearner("classif.rpart", id = "rpart"), makeLearner("classif.randomForest", id = "randomForest"))
# convertMLBenchObjToTask does exactly what it says... here we create 2 tasks
ring.task <- convertMLBenchObjToTask("mlbench.ringnorm", n = 600)
wave.task <- convertMLBenchObjToTask("mlbench.waveform", n = 600)
tasks <- list(iris.task, sonar.task, pid.task, ring.task, wave.task)
# 10-fold cross-validation 
rdesc <- makeResampleDesc("CV", iters = 10)
meas <- list(mmce, ber, timetrain)
bmr <- benchmark(learners = lrns, tasks = tasks, resamplings = rdesc, measures = meas, show.info = FALSE)
bmr
```

The individual performances on each iteration for each learner, task, and measure can be accessed via `getBMRPerformances` as shown below:

```{r}
perf <- getBMRPerformances(bmr, as.df = TRUE)
head(perf)
```

Performance tables like the one shown above get increasingly harder to read and comprehend with more experiments. A more convenient way to view the results is by plotting and visualization.

#### Plots

The function `plotBMRBoxplots` takes as input a benchmark object and displays a selected performance measure for all tasks and for all learners as a box or violin plot using the `ggplot2` graphics. 

```{r}
plotBMRBoxplots(bmr, measure = mmce) + facet_wrap(~ task.id, nrow = 2)
```

```{r}
plotBMRBoxplots(bmr, measure = ber, style = "violin") + aes(color = learner.id) +
  facet_wrap(~ task.id, nrow = 2)
```

The aggregated measure score on the test set (e.g. `mmce.test.mean`) is retrieved from the `benchmark` output for each learner and task and displayed through `plotBMRSummary`. By default the first measure is used. Note that the argument `jitter = 0.05` is a vertical distance added between points to prevent overplotting.

```{r}
plotBMRSummary(bmr)
```

Clearly, functionality showing the relative performance is of interest and what we are usually after with the benchmark experiments. The function `convertBMRToRankMatrix` calculates the rank based on a selected learner aggregated performance measure. 

```{r}
m <- convertBMRToRankMatrix(bmr, measure = mmce)
m
```

Alternatively, we can visualize the ranking results as a bar chart using `plotBMRRanksAsBarChart`. The ranks are displayed from best to worst on the horizontal axis and the tasks are shown on the vertical axis.

```{r}
plotBMRRanksAsBarChart(bmr, pos = "tile")
```

#### Hypothesis tests

Hypothesis tests can be used to conclude whether there is a significant difference between the performance of the various learners. While parametric hypothesis tests may have more power over nonparametric tests, they make assumptions about the underlying distributions from which the sample was drawn from which often means that, in order for the results to be at all reliable, we would need many data sets to show significance differences at reasonable significance levels. The mlr package provides the **Overall Friedman test** and the **Friedman-Nemenyi post hoc test**.

The Friedman test is the nonparametric alternative to a one-way ANOVA with repeated measures. We use it to compare three or more learners where the data used is the same in each learning algorithm. Unlike the ANOVA which requires the sample is drawn from a normal distribution and equal variances of the residuals, the Friedman test is free from such restrictions (but, as mentioned above, less powerful). The hypotheses for the comparison are $H_0$: The distributions (whatever they are) are the same across repeated measures and $H_1$: The distributions across repeated measures are different.

```{r}
friedmanTestBMR(bmr)
```

Next, if the Friedman test results show a significant $p$-value (depending on the significance level you set i.e. $p$ < $\alpha$ for significance), then this would mean that we can reject the null that all learners perform the same but at this point we don't know which ones are superior. Therefore, our next step will be to try and find out which pairs of our groups are significantly different then each other with a post hoc analysis. We carry this out with `friedmanPostHocTestBMR`. The following `R` code demonstrates this with a choice of a significance level of 0.1.

```{r}
friedmanPostHocTestBMR(bmr, p.value = 0.1)
```

The results show that a significance level of 0.1, we can reject the null that there exists no performance difference between `rpart` and `randomForest`. 

#### Custom plots

Examples of custom plots using the objects returned by `getBMRPerformances` and `getBMRAggrPerformances`. 

**Density plots**

```{r}
perf <- getBMRPerformances(bmr, as.df = TRUE)
qplot(mmce, colour = learner.id, facets = . ~ task.id,
data = perf[perf$task.id %in% c("iris-example", "Sonar-example"), ],
geom = "density")
```

In the following `R` code, we reshape the `perf` data frame by keeping the variables `task.id`, `learner.id`, and `iter` and collect all measures into a single column (`variable`) and their corresponding values in a second column (`value`).

```{r}
head(perf)
perfdf <- reshape2::melt(perf, id.vars = c("task.id", "learner.id", "iter"))
head(perfdf)
```

We plot boxplots for `mmce` and `timetrain` for each task.

```{r}
perfdf <- perfdf[perfdf$variable != "ber", ]
qplot(variable, value, data = perfdf, colour = learner.id, geom = "boxplot",
      xlab = "measure", ylab = "performance") +
  facet_wrap(~ task.id, nrow = 2)
```

Further insight may be gained on learner performance by comparing the performance in each fold for a particular task; one learner could be performing exceptionally well in a single iteration while another may be performing exceptionally bad. We do this on the sonar data below by collecting the misclassification errors computed by the three learners we used. 

```{r}
perf <- getBMRPerformances(bmr, task.ids = "Sonar-example", as.df = TRUE)
df <- reshape2::melt(perf, id.vars = c("task.id", "learner.id", "iter"))
df <- df[df$variable == "mmce",]
df <- reshape2::dcast(df, task.id + iter ~ variable + learner.id)
head(df)
# scatterplot matrix
GGally::ggpairs(df, 3:5)
```







