---
title: "Working with the mlr package"
output: html_document
---

To become familiar with the mlr package and its widespread functionality, I created an Rmarkdown document with basic and advanced features from the mlr tutorial on mostly regression and classification problems with data available in R. 

```{r, echo=FALSE, message=FALSE}
library(plyr)
library(dplyr)
library(caret)
library(randomForest)
library(gbm)
library(mlbench)
library(rattle)
library(parallelMap)
library(RWeka)
library(kernlab)
library(irace)
library(mlr)
```
## Getting started

The first step is to create a **task**; this can be a classification, regression, survival, cluster, cost-sensitive classification or multilabel task. For example, let us start with the `iris` data set from which we wish to predict the `Species` based on features comprised of width and length measurements of sepals and petals. 

```{r}
task <- makeClassifTask(data = iris, target = "Species")
```

There exist other options that can be set when calling the task function like, for instance, weights and blocking. The latter is for observations that are required to be considered together such that when resampling or cross-validating, grouped observations are included either all in the train or all in the test set.

Next, we choose a learner. Here, we choose to learn based on a classification tree fitted through the `rpart` function. 

```{r}
lrn <- makeLearner("classif.rpart")
```

We can get a description of all possible parameter settings for a learner using `?getParamSet(lrn)`:

```{r, results='markup'}
getParamSet(lrn)
```

Further, we can create a description object for a resampling startegy using `makeResampleDesc`. For example, we may wish to carry out a 3-fold cross-validation of `rpart` on `iris`.

```{r}
cv3f <- makeResampleDesc("CV", iters = 3, stratify = TRUE )
```

Finally, we can fit the model specified by `lrn` on the `task` and calculate predictions and performance measures for all training and all test sets specified by the above resampling description.

```{r, message=FALSE}
set.seed(123)
r <- resample(lrn, task, cv3f)
r$aggr 
```

The mean misclassification error is given by `r r$aggr`. The default measure for a classification task is the misclassification error, `mmce` but it is possible to choose a different measure, say *accuracy*, `acc` which is given by  1 - `mmce`. We will see later that we can also define our own measures. 

```{r, message=FALSE}
set.seed(123)
r <- resample(lrn, task, cv3f, measures = acc)
r$aggr 
```



## Basics

### Tasks

As previously mentioned, depending on the type of problem at hand, one can define an appropriate task. We have seen an instance of a classification problem above, let us now look at a supervised regression problem using the `BostonHousing` data. By printing task, we get some information on the task object we have just created and the associated data.

```{r}
data(BostonHousing, package = "mlbench")
task <- makeRegrTask(data = BostonHousing, target = "medv")
print(task)
```

#### Modifying a task

Once the task is created, it can be modified in several ways. For example, the function `subsetTask` allows the selection of certain observations and/or features. 

```{r}
names(BostonHousing)
modifiedtask <- subsetTask(task, subset = 1:400, features = c("crim", "age", "dis", "lstat"))
str(getTaskData(modifiedtask))
```

Using `getTaskData` we can see that the modified task now includes 400 observations and 5 variables (the 4 included in the features character vector above and the target variable which will always be included). Some other useful functions are the following:

  1. `removeConstantFeatures(<task name>)`:
  
  constant features may arise dur to an inherent feature in the data collected or as a result of choosing a subset of observations.
  
  2. `dropFeatures(task, c("rm", "nox"))`:
  
  remove selected features from the task.
  
  3. `normalizeFeatures(task, method = "standardize")`:
  
  normalize numerical features by different methods (nonnumerical features are left untouched). The normalizing method can be one of `center` (subtract mean), `scale` (divide by standard deviation), `standardize` (center and scale), `range` (scale to a given range - default is [0, 1]). The optional argument `exclude` may be used to supply a character vector of columns to be excluded from the normalization. Type `?normalizeFeatures` for more info.

### Learners

Many of the popular learning algorithms are already implemented in `mlr`. The `makeLearner` function requires the user to specify the learning method. Additionally, it is possible to modify defaults on the prediction type (i.e. for classification, we may choose `predict.type = "prob"` for probabilities), or set hyperparameters using a `list` passed to the `par.vals` argument.

```{r}
class.lrn <- makeLearner("classif.randomForest", predict.type = "prob", fix.factors.prediction = TRUE)
regr.lrn <- makeLearner("regr.gbm", par.vals = list(n.trees = 500, interaction.depth = 4))
#cluster.lrn <- makeLearner("cluster.SimpleKMeans", N = 5)
```

Note that the `fix.factors.prediction = TRUE` argument is useful in situations where a factor level is present in the training data set but not the test data set; it adds a factor level for missing data in the test set thus avoiding problems. 

The Learner object is a list and information can be extracted using the `$`, e.g. `regr.lrn$par.vals`. This information can also be accessed using other functions in `mlr`, for instance, `getHyperPars(lrn)` retrieves the current hyperparameter settings of the learner `lrn`. We also used above `getParamSet(lrn)` to get a description of all possible parameter settings for `lrn`. 

#### Modifying a learner

Just like it was possible to modify an existing task, we can also do so for a learner. Modifications include changing the `id` (this is either user specified or, if omitted, it is automatically set to the algorithm name), the prediction type, hyperparameter values, and more.

```{r}
class.lrn <- setPredictType(class.lrn, "response")
regr.lrn <- setHyperPars(regr.lrn, n.trees = 400)
regr.lrn <- removeHyperPars(regr.lrn, c("n.trees", "interaction.depth"))
```

Note `removeHyperPars` sets the hyperparameters back to their default values.

### Train

Once we create the task from the data set and identify the learning algorithm, the next step is to train the learner using the `train` command.

```{r}
task <- makeClassifTask(data = iris, target = "Species")
lrn <- makeLearner("classif.lda")
mod <- mlr::train(lrn, task)
mod
```

The function `train` returns a list. The fitted model can be extracted using the `getLearnerModel(mod)` command. It is possible to choose a subset of observations to be used to train the model; this is achieved via the `subset` argument in `train`. Note that this is usually not needed since resampling strategies are supported. In the following example, we use the `BostonHousing` data to fit a linear model to the regression task. Note that `getTaskSize` returns the number of observations in a task.

```{r}
data(BostonHousing, package = "mlbench")
task <- makeRegrTask(data = BostonHousing, target = "medv")
lrn <- makeLearner("regr.lm")
train.set <- sample(getTaskSize(task), size = getTaskSize(task)/3)
mod <- mlr::train(lrn, task, subset = train.set)
getLearnerModel(mod)
```

Finally, a note on weights passed as an argument in the `train` function. As an example for an application of weights ^[note that mlr offers alternatives with more functionality for imbalanced classification problems] used in `train`, consider the `BreastCancer` data for which the target variable `Class` identifies 241 malignant and 458 benign cases. To deal with the imbalanced classes, we can incorporate weights in an attempt to allow the two classes to be equally represented in training the classifier. Here we use the predefined task `bc.task` in mlr. The `getTaskTargets` function gets the target data from a task (in this example, this is equivalent to the vector `BreastCancer$Class`). Note that if weights are defined in task as well then those would be overwritten by the weights in `train`. 

```{r}
target <- getTaskTargets(bc.task)
tab <- as.numeric(table(target))
#obtain inverse class frequencies for the weights
w <- 1/tab[target] 
mod <-mlr:: train("classif.rpart", task = bc.task, weights = w)
fancyRpartPlot(getLearnerModel(mod), sub = "")
```

Weights can also be useful as a means to grant more importance to recently collected data versus older data or to reduce the influence of outliers.

### Predict

To predict target values, we use the `predict` function which takes as input the object returned by `train` and data for which we want predictions. The data can either come from the task (using the `task` argument) or it can be a data frame (passed using the `newdata` argument). Similarly to the `train` function, the `subset` argument may be used to pass different portions of the data in task. 

```{r}
n <- getTaskSize(bh.task)
train.set <- seq(1, n, by = 2)
test.set <- seq(2, n, by = 2)
lrn <- makeLearner("regr.gbm", n.trees = 100, interaction.depth = 4)
mod <- mlr::train(lrn, bh.task, subset = train.set)
preds <- predict(mod, task = bh.task, subset = test.set)
preds
```

The function `predict` returns a list; The `$data` element of that list contains the true values of the target variable (in case of supervised learning) and the predictions. A direct way to obtain the true and predicted values of the target variable is through the `getPredictionTruth(preds)` and `getPredictionResponse(preds)` commands where `preds` is the list returned by the `predict` function.

```{r}
head(getPredictionTruth(preds), 10)
head(getPredictionResponse(preds), 10)
```

For classification problems, class labels are predicted. We can obtain a confusion matrix through the command `getConfMatrix`. To get predicted posterior probabilities, we need to create the learner with `predict.type = "prob"`.

```{r}
lrn <- makeLearner("classif.rpart", predict.type = "prob")
mod <- mlr::train(lrn, iris.task)
preds <- predict(mod, newdata = iris)
head(as.data.frame(preds))
head(getPredictionProbabilities(preds))
```

We can also adjust the threshold value that is used to map the predicted posterior probabilities to the class labels. The default value for binary classification is 0.5; however, it may be necessary to increase/decrease this value in various situations such as in the case of classifying cancer rates where one would be concerned with false negatives (prediction of no cancer when the truth is yes cancer). By changing the threshold we change the sensitivity of the model. An example for adjusting the threshold in a binary classification setting is shown below. 

```{r}
data(Sonar)
table(Sonar$Class)
getTaskDescription(sonar.task)$positive
lrn <- makeLearner("classif.rpart", predict.type = "prob")
mod <- mlr::train(lrn, task = sonar.task)
# predict with default threshold
preds <- predict(mod, sonar.task)
preds$threshold
# set threshold value for +ve class
preds2 <- setThreshold(preds, 0.9)
# confusion matrices
getConfMatrix(preds)
getConfMatrix(preds2)
```

For multiclass classification problems, the threshold is given by a named vector specifying the values by which each probability will be divided. 

### Performance

There are many available performance measures implemented in mlr but one can also create their own performance measures. For a particular prediction object, say `preds` calling `performance(preds)` will give the calculated performance measure. It is also possible to calculate the time needed to train the learner (passing `timetrain` as an argument - see below), the time needed to compute the prediction (`timepredict`) or both (`timeboth`). 

To obtain a list of available measures suitable for a particular problem type or `task`, use the `listMeasures` argument. The function `getDefaultMeasure` shows the defaults for a particular learner or task.

```{r}
listMeasures("classif", properties = "classif.multi")
getDefaultMeasure(bh.task)
```

The following piece of `R` code shows how to obtain the performance measure from the prediction object.

```{r}
n <- getTaskSize(bh.task)
train.set <- seq(1, n, by = 2)
test.set <- seq(1, n, by = 2)
lrn <- makeLearner("regr.gbm", n.trees = 1000)
mod <- mlr::train(lrn, task = bh.task, subset = train.set)
preds <- predict(mod, task = bh.task, subset = test.set)
performance(preds)
```

To change the performance measure, we can do so via the `measures` argument. It is possible to calculate several performance measures by passing them as a list.

```{r}
performance(preds, measures = medse)
performance(preds, measures = list(mse, medse, mae))
```

It is a necessary requirement to pass the model or the task in order to calculate some performance measures. For instance, for `timetrain` calculations, the model also needs to be passed as ar argument.

```{r}
performance(preds, measures = timetrain, model = mod)
```

For clustering problems, the task is required.

```{r}
lrn <- makeLearner("cluster.kmeans", centers = 3)
mod <- mlr::train(lrn, task = mtcars.task)
preds <- predict(mod, task = mtcars.task)
performance(preds, measures = dunn, task = mtcars.task)
```


As previously mentioned, the threshold for classification problems alters the sensitivity of the model and therefore affects performance. The command `generateThreshVsPerfData` used on the prediction object along with a performance measure (or a list of them) generates data on the learner performance versus the threshold. T

```{r}
lrn <- makeLearner("classif.lda", predict.type = "prob")
n <- getTaskSize(sonar.task)
train.set <- seq(1, n, by = 2)
test.set <- seq (2, n, by = 2)
mod <- mlr::train(lrn, task = sonar.task, subset = train.set)
preds <- predict(mod, task = sonar.task, subset = test.set)
performance(preds, measures = list(fpr, fnr, mmce))
d <- generateThreshVsPerfData(preds, measures = list(fpr, fnr, mmce))
plotThreshVsPerf(d)
```

### Resampling

Resampling strategies are often used to assess the performance of a learning algorithm by splitting the data into multiple training and test sets. Each training set is used to train a learner and each test set is reserved for predictions. We get the mean performance measure obtained by agreggating all individual performances. The `makeResampleDesc` function is used to choose the resampling strategy; the method argument can be set to one of cross-validation, leave-one-out cross-validation, repeated cross-validation, out-of-bag bootstrap, subsampling (a.k.a. *Monte-Carlo* cross-validation), and holdout. Additional arguments can be passed to the function depending on the chosen method. Once the resampling description is specified, we use the function `resample` to fit a model specified by a learner on a task, which calculates predictions and performance measures for all training and test sets as specified by the resampling description.

```{r}
rdesc <- makeResampleDesc("CV", iters = 3, stratify = TRUE)
task <- makeClassifTask(data = iris, target = "Species")
lrn <- makeLearner("classif.rpart")
r <- resample(lrn, task, rdesc)
r
r$measures.test
r$aggr
r$measures.train
```

The `resample` function returns a list whose elements we can access using the `$` notation. In the above exmaple `r$measures.test` gives the performance on the 3 individual test sets (as specified in the resampling description) and `r$measures.train` returns missing values since no predictions on the training sets were made. If we wish to have predictions on the training sets we can set `predict = "both"` or `predict = "train"` as an argument in `makeResampleDesc`. 

Next, we can access `r$pred$data` which gives a data frame of the predictions and true (in supervised learning) values of the target variable. Note that it is possible to pass multiple measures as a list in `resample` (including `timetrain`). 

```{r}
head(r$pred$data)
```

*Stratified resampling* ensures that the same proportion of the classes falls in all partitions of the data such that, in a classification setting, each training/test set trains a model with no class being under-represented. This is especially important in small data sets as well as imbalanced classification problems. The `stratify = TRUE` argument is passed when making the resampling description (as in the `R` code above). The `stratify.cols = <col. name>` argument is used to stratify factor variable inputs to ensure that all subgroups are represented in the data partitions.

#### Accessing individual learners

By default, `resample` does not return the individual learners but can do so by passing the argument `models = TRUE` when calling `resample`. More useful still, is to extract certain information from each model like, for instance, the variable importance for chosen models. This is achieved via the `extract` argument.

```{r}
rdesc <- makeResampleDesc("CV", iters = 3)
lrn <- makeLearner("regr.rpart")
r <- resample(lrn, task = bh.task, resampling = rdesc, extract = function(x)
  x$learner.model$variable.importance, models = FALSE)
r$extract
```

Now, `r$extract` is going to give information on the given function from which we asked to obtain the variable importance. With `models = FALSE` we do not obtain other information on the model so attempting to call `getLearnerModel(r$models[[1]])` would return an error. 

#### Resample instance

The command `makeResampleInstance` takes as arguments an object of class ResampleDesc and a task [or the size of the data set i.e. nrow(<data>)]. This creates a ResampleInstance object which mainly stores the indices of the training and test sets used in each iteration. This feature may be useful in situations where we want to perform paired experiments like testing the performance of several learners on exactly the same data. 

```{r}
rdesc <- makeResampleDesc("CV", iters = 3, stratify = TRUE)
rin <- makeResampleInstance(rdesc, task = iris.task)
rin$train.inds[[2]]
```

In `makeResampleInstance` the indices are drawn randomly; the function `makeFixedHoldoutInstance` allows the training and test sets to be specified manually. 

```{r}
rin <- makeFixedHoldoutInstance(train.inds = 1:100, test.inds = 101:150, size = 150)
rin
```

In resampling startegies, we get a performance measure which is aggregated over all the measures calculated for each iteration. By default, the aggregated score is the mean error on the test set. We can change the aggregation method for a measure via `setAggregation(measure, aggr)`. For the different options for `measure` and `aggr` type `?measures` and `?aggregations`, respectively. 

```{r}
m1 <- mmce
m2 <- setAggregation(tpr, test.median) # tpr = true positive rate
rdesc <- makeResampleDesc("CV", iters = 3)
r <- resample("classif.rpart", task = sonar.task, resampling = rdesc, measures = list(m1,m2))
r$aggr
```

To get predictions on both training and test sets, we need to set `predict = "both"` in `makeResampleDesc` and set the aggregation method to `train.mean` if we wish to calculate the mean. An example is shown below.

```{r}
train.mmce <- setAggregation(mmce, train.mean)
rdesc <- makeResampleDesc("CV", iters = 3, predict = "both")
r <- resample("classif.rpart", task = sonar.task, resampling = rdesc, measures = list(mmce, train.mmce))
r$aggr
```

There exist functions which act as convenience wrappers for the various existing resampling strategies. They don't offer as much flexibility as `resample` but can be quick and useful if trying out a number of learners initially. An example with `crossval` is shown below.

```{r}
cviris <- crossval("classif.lda", task = iris.task, iters = 3, measures = list(mmce, ber))
cviris
```


### Benchmark experiments

Using `benchmark`, we can compare different learning algorithms across one or more tasks w.r.t. a given resampling strategy. The function has the advantage of conducting *paired* experiments thus comparing the same training/test sets for the different learners. In the following `R` code, we specify a single task (here, we use the built in `sonar.task`) and apply a LDA and a classification tree learner. The resampling strategy is chosen as `Holdout` hence the performance is calculated on a single randomly sampled test set.

```{r}
lrns <- list(makeLearner("classif.lda"), makeLearner("classif.rpart"))
rdesc <- makeResampleDesc("Holdout")
bmr <- benchmark(learners = lrns, tasks = sonar.task, resamplings = rdesc)
bmr
```

The function `benchmark` returns an object of class BenchmarkResult which contains a list of lists of ResampleResult objects ordered by task and followed by learner. The mlr `getBMR<...>` commands
allows access to the benchmark results.

#### Learner performances

```{r}
getBMRPerformances(bmr, as.df = TRUE)
getBMRAggrPerformances(bmr, as.df = TRUE)
```

The two results (top: individual performance in resampling runs, bottom: aggregated performance values) coincide since `Holdout` was used as the resampling strategy. The optional argument `as.df = TRUE` returns the results in the form of a data frame which is often more convenient. 

#### Predictions

By default, `keep.pred = TRUE` in `benchmark` which allows the user to access the predictions with  `getBMRPredictions`. If `keep.pred = FALSE`, the following command will result in an error.

```{r}
head(getBMRPredictions(bmr, as.df = TRUE))
```

The learner and task ID is automatically set to the name of the algorithm and task if not explicitly specified. Using the ID in `getBMRPredictions`, it is possible to access results for certain learners or tasks. For instance below, we obtain the predictions from the classification tree learner. 

```{r}
head(getBMRPredictions(bmr, learner.ids = "classif.rpart", as.df = TRUE))
```

If unsure of the ID for the learner, task, and performance measure, they can be accessed using `getBMRLearnerIds`,  `getBMRTaskIds`, and `getBMRMeasureIds`, respectively.

#### Models

Similarly, to *Predictions* above, the argument `models = FALSE` must be set in `benchmark` if the user does not want to keep the fitted models for all learners and tasks. If set to true (default), the models may be accessed through `getBMRModels`.

```{r}
getBMRModels(bmr, learner.ids = "classif.lda")
```

More `getBMR<...>` functions exist to extract information on learners, measures, and more.

#### Additional benchmark experiments and merging results

Once a benchmark experiment has been conducted on a task, we may need to add more learners (or, alternatively, we may wish to extend existing learners to other tasks). We can perform another benchmark experiment and then merge the results (through either `mergeBenchmarkResultLearner` or `mergeBenchmarkResultTask`). As an example, we perform another benchmark experiment on `sonar.task` now with random forest and quadratic discriminant analysis learning algorithms. We then fuse this with the  `bmr` object we obtained above to get a single `BenchmarkResult` object.

```{r}
lrns2 <- list(makeLearner("classif.randomForest"), makeLearner("classif.qda"))
bmr2 <- benchmark(learners = lrns2, tasks = sonar.task, resamplings = rdesc, show.info = FALSE)
bmr2
bmrsingle <- mergeBenchmarkResultLearner(bmr, bmr2)
bmrsingle
```

However, note that the resampling description was passed to `benchmark` twice, once to obtain the `bmr` and then the `bmr2` objects. The training/test pairs therefore were most likely different in the first benchmark call than the second. For more accurate merging of benchmark results, we can opt to work with `ResampleInstance` from the start, or extract the `ResampleInstance` from the resample description in the first benchmark call and pass it as argument in later benchmark calls. An example is given below.

```{r}
rin <- getBMRPredictions(bmr)[[1]][[1]]$instance
rin
bmr3 <- benchmark(learners = lrns2, tasks = sonar.task, resamplings = rin, show.info = FALSE)
mergeBenchmarkResultLearner(bmr, bmr3)
```

#### Benchmark analysis & visualization

Once benchmark experiments are conducted for the various learners and tasks, we may wish to rank and assess the performance of various algorithms, perform hypotheses tests or visualize the results. The mlr package offers various functions to do so and we explore some of these below using a longer benchmark example than the one above.

```{r}
# list of 3 learners
lrns <-list(makeLearner("classif.lda", id = "lda"), makeLearner("classif.rpart", id = "rpart"), makeLearner("classif.randomForest", id = "randomForest"))
# convertMLBenchObjToTask does exactly what it says... here we create 2 tasks
ring.task <- convertMLBenchObjToTask("mlbench.ringnorm", n = 600)
wave.task <- convertMLBenchObjToTask("mlbench.waveform", n = 600)
tasks <- list(iris.task, sonar.task, pid.task, ring.task, wave.task)
# 10-fold cross-validation 
rdesc <- makeResampleDesc("CV", iters = 10)
meas <- list(mmce, ber, timetrain)
bmr <- benchmark(learners = lrns, tasks = tasks, resamplings = rdesc, measures = meas, show.info = FALSE)
bmr
```

The individual performances on each iteration for each learner, task, and measure can be accessed via `getBMRPerformances` as shown below:

```{r}
perf <- getBMRPerformances(bmr, as.df = TRUE)
head(perf)
```

Performance tables like the one shown above get increasingly harder to read and comprehend with more experiments. A more convenient way to view the results is by plotting and visualization.

#### Plots

The function `plotBMRBoxplots` takes as input a benchmark object and displays a selected performance measure for all tasks and for all learners as a box or violin plot using the `ggplot2` graphics. 

```{r}
plotBMRBoxplots(bmr, measure = mmce) + facet_wrap(~ task.id, nrow = 2)
```

```{r}
plotBMRBoxplots(bmr, measure = ber, style = "violin") + aes(color = learner.id) +
  facet_wrap(~ task.id, nrow = 2)
```

The aggregated measure score on the test set (e.g. `mmce.test.mean`) is retrieved from the `benchmark` output for each learner and task and displayed through `plotBMRSummary`. By default the first measure is used. Note that the argument `jitter = 0.05` is a vertical distance added between points to prevent overplotting.

```{r}
plotBMRSummary(bmr)
```

Clearly, functionality showing the relative performance is of interest and what we are usually after with the benchmark experiments. The function `convertBMRToRankMatrix` calculates the rank based on a selected learner aggregated performance measure. 

```{r}
m <- convertBMRToRankMatrix(bmr, measure = mmce)
m
```

Alternatively, we can visualize the ranking results as a bar chart using `plotBMRRanksAsBarChart`. The ranks are displayed from best to worst on the horizontal axis and the tasks are shown on the vertical axis.

```{r}
plotBMRRanksAsBarChart(bmr, pos = "tile")
```

#### Hypothesis tests

Hypothesis tests can be used to conclude whether there is a significant difference between the performance of the various learners. While parametric hypothesis tests may have more power over nonparametric tests, they make assumptions about the underlying distributions from which the sample was drawn from which often means that, in order for the results to be at all reliable, we would need many data sets to show significance differences at reasonable significance levels. The mlr package provides the **Overall Friedman test** and the **Friedman-Nemenyi post hoc test**.

The Friedman test is the nonparametric alternative to a one-way ANOVA with repeated measures. We use it to compare three or more learners where the data used is the same in each learning algorithm. Unlike the ANOVA which requires the sample is drawn from a normal distribution and equal variances of the residuals, the Friedman test is free from such restrictions (but, as mentioned above, less powerful). The hypotheses for the comparison are $H_0$: The distributions (whatever they are) are the same across repeated measures and $H_1$: The distributions across repeated measures are different.

```{r}
friedmanTestBMR(bmr)
```

Next, if the Friedman test results show a significant $p$-value (depending on the significance level you set i.e. $p$ < $\alpha$ for significance), then this would mean that we can reject the null that all learners perform the same but at this point we don't know which ones are superior. Therefore, our next step will be to try and find out which pairs of our groups are significantly different then each other with a post hoc analysis. We carry this out with `friedmanPostHocTestBMR`. The following `R` code demonstrates this with a choice of a significance level of 0.1.

```{r}
friedmanPostHocTestBMR(bmr, p.value = 0.1)
```

The results show that a significance level of 0.1, we can reject the null that there exists no performance difference between `rpart` and `randomForest`. 

#### Custom plots

Examples of custom plots using the objects returned by `getBMRPerformances` and `getBMRAggrPerformances`. 

**Density plots**

```{r}
perf <- getBMRPerformances(bmr, as.df = TRUE)
qplot(mmce, colour = learner.id, facets = . ~ task.id,
data = perf[perf$task.id %in% c("iris-example", "Sonar-example"), ],
geom = "density")
```

In the following `R` code, we reshape the `perf` data frame by keeping the variables `task.id`, `learner.id`, and `iter` and collect all measures into a single column (`variable`) and their corresponding values in a second column (`value`).

```{r}
head(perf)
perfdf <- reshape2::melt(perf, id.vars = c("task.id", "learner.id", "iter"))
head(perfdf)
```

We plot boxplots for `mmce` and `timetrain` for each task.

```{r}
perfdf <- perfdf[perfdf$variable != "ber", ]
qplot(variable, value, data = perfdf, colour = learner.id, geom = "boxplot",
      xlab = "measure", ylab = "performance") +
  facet_wrap(~ task.id, nrow = 2)
```

Further insight may be gained on learner performance by comparing the performance in each fold for a particular task; one learner could be performing exceptionally well in a single iteration while another may be performing exceptionally bad. We do this on the sonar data below by collecting the misclassification errors computed by the three learners we used. 

```{r}
perf <- getBMRPerformances(bmr, task.ids = "Sonar-example", as.df = TRUE)
df <- reshape2::melt(perf, id.vars = c("task.id", "learner.id", "iter"))
df <- df[df$variable == "mmce",]
df <- reshape2::dcast(df, task.id + iter ~ variable + learner.id)
head(df)
# scatterplot matrix
GGally::ggpairs(df, 3:5)
```


### Parallelization

A number of instances are parallelizable with mlr. Parallelization is activated using `parallelMap::parallelStart`, the first loop mlr encounters (which is parallel executable) will be automatically parallelized. 

```{r}
# parallelStartSocket(2) 
parallelStartMulticore(cpus = 2) # better for macosx?
rdesc <- makeResampleDesc("CV", iters = 3)
r <- resample("classif.lda", task = iris.task, rdesc)
parallelStop()
```

The `parallelStart` functions have an optional argument `levels`; by setting it, we can control which level gets parallelized. For instance, we may be running a few benchmark experiments which use an elaborate resampling description. In such a case, it would be best to have the resampling parallelized and not the benchmark so we could set `level = "mlr.resample"` in the `parallelStart` function. Not all levels are supported, the current version (2.8) supports `mlr.benchmark`, `mlr.resample`, `mlr.selectFeatures`, and `mlr.tuneParams`.

For custom learners, they need to be exported to the slave before calling the `parallelStart` functions:

`parallelExport("trainLearner.regr.<myregrlearner>", "predictLearner.regr.<myregrlearner>")`

#### Visualizations

We have already seen various `generate` functions within mlr which generate data that can then be used for plotting and visualization using `plotting` functions. Let us revisit the binary classification task with the sonar data set and plot the classifier performance against the boundary decision threshold.

```{r}
lrn <- makeLearner("classif.lda", predict.type = "prob")
n <- getTaskSize(sonar.task)
mod <- mlr::train(learner = lrn, task = sonar.task, subset = seq(1, n, by = 2))
pred <- predict(mod, task = sonar.task, subset = seq(2, n, by = 2))
d <- generateThreshVsPerfData(pred, measures = list(fpr, fnr, mmce))
head(d$data)
plotThreshVsPerf(d) #GGVIS is also possible but currently experimental
```

Alternatively, the plot can be manually created using the data generated from the `generate` function. Using ggplot we show this below.

```{r}
dnew <- reshape2::melt(data = d$data, id.vars= c("threshold"))
ggplot(data = dnew, aes(threshold, value)) + geom_line() + facet_grid(. ~ variable,
                                                                      scales = "free_y")
```

In this second example, we `generate` partial data on what we are interested in visualizing from the fitted model using `generatePartialPredictionData` and then create plot using `plotPartialPrediction`.

```{r}
sonar <- getTaskData(sonar.task)
pd <- generatePartialPredictionData(mod, input = sonar, features = "V11")
plt <- plotPartialPrediction(pd)
head(plt$data)
plt
```


## Advanced

### Wrappers

#### Introductory example

It is possible to add pre-processing, missing values imputation, tuning, feature selection and other functionality to a learner through the use of wrappers. Each time a wrapper is used around an mlr learner, a new learner is returned and this procedure can be repeated many times.

As a first example we use a bagging wrapper to create a random forest which supports weights and compare it to the unwrapped learner (`base.lrn` below). 

```{r}
data(iris)
task <- makeClassifTask(data = iris, target = "Species", weights = as.integer(iris$Species))
base.lrn <- makeLearner("classif.rpart")
```

The `makeBaggingWrapper` function takes as inputs the learner we want wrapped, the number of iterations which is the number of fitted models in bagging (for `rpart` learner, this would be `ntree` = N base learners), an option for sampling with/without replacement, proportion of randomly selected features (this would be the equivalent of `mtry`, here we set it at .5)

```{r}
wrapped.lrn <- makeBaggingWrapper(learner = base.lrn, bw.iters = 100, bw.feats = 0.5 )
print(wrapped.lrn)
```

Next, we run a `benchmark` experiment with the list of learners comprised of the base and wrapped learners. Note that the default resampling strategy is 10-fold cross-validation.

```{r}
bmr <- benchmark(learners = list(base.lrn, wrapped.lrn), task = task)
bmr
```

We may choose to carry out hyperparameter tuning to hopefully improve on the performance of the new learner. The function `makeTuneWrapper` fuses a learner with a search strategy to select its hyperparameters. This is set in effect when `train` is called (see below). First the algorithm for the hyperparameter optimization must be chosen - this is done via the `makeTuneControl` functions. A grid algorithm is traditional but it may be computationally expensive in high-dimensional spaces. In this example we only explore a small parameter space (we tune `minsplit` and `bw.feats`) with *random search*. The `minsplit` parameter is the minimum number of observations that must exist in a node in order for a split to be attempted and `bw.feats` was discussed above. In the random search, we choose to have sampled settings randomly selected 10 times (`maxit = 10`)

```{r}
ctrl <- makeTuneControlRandom(maxit = 10)
rdesc <- makeResampleDesc("CV", iters = 3)
par.set <- makeParamSet(makeIntegerParam("minsplit", lower = 1, upper = 10),
                        makeNumericParam("bw.feats", lower = 0.25, upper = 1))

tuned.lrn <- makeTuneWrapper(learner = wrapped.lrn, resampling = rdesc,
                             measures = mmce, par.set = par.set, control = ctrl)
print(tuned.lrn)
```

Now we have a learner object like before but this learner internally used `tuneParams` such as when called with `train`, the search strategy and resampling are invoked to choose an optimal set of parameters. The output from `train` is a tuned, bagged learner. 

```{r}
lrn <- mlr::train(learner = tuned.lrn, task = task)
print(lrn)
getTuneResult(lrn)
```


#### Data preprocessing

The mlr package offers many options for data preprocessing. Some of them are directly applied to a task to modify it. These include `capLargeValues`, `createDummyFeatures` (for factor variables), `dropFeatures`, `joinClassLevels`, `mergeSmallFactorLevels`, `normalizeFeatures`, `removeConstantFeatures`, and `subsetTask` which are all pretty self explanatory from their name. 

With the mlr wrapper functionality, the preprocessing steps are done at the time the learner is trained or predictions are made. This is important as it avoids the mistake of integrating processing steps which are data-dependent on the whole data set while the learner is trained only on training/test sets. A more honest performance of the learner is obtained if all preprocessing steps are included in the resampling. This is automatically done when fusing a learner with preprocessing. 

`makePreprocWrapperCaret` permits access to all preprocessing options offered by `caret`'s `preProcess` function while`makePreprocWrapper` is used when writing custom preprocessing methods by defining the actions to be taken before training and before prediction. With these two functions, the preprocessing steps then belong to the learner. This is in contrast to the functions defined above (e.g `normalizeFeatures`) which alters the task. So, here, the task remains unchanged, preprocessing is done for every pair of training/test sets in resampling and not globally on the whole data set, and any parameters pertinent to preprocessing can be tuned with the learner's parameters. 

Firstly, the usage of`makePreprocWrapperCaret` is similar to `caret`'s `preProcess` in the sense that it takes almost all of the formal arguments though their names are prefixed by `ppc.`, e.g. `knnImpute` becomes `ppc.knnImpute`. Secondly, there is no `method` argument; instead, the preprocessing options are passed as individual logical arguments, i.e. `pcc.knnImpute = TRUE`. 

For knn imputation and pca in caret:

`preProcess(x, method = c("knnImpute", "pca"), pcaComp = 10)`

With the mlr wrapper:

`makePreprocWrapperCaret(learner, ppc.knnImpute = TRUE, ppc.pca = TRUE, ppc.pcaComp = 10)`

We show an example where we apply PCA on the sonar data (this poses a binary classification problem with 208 obs and 60 features) for dimensionality reduction. The `makePreprocWrapperCaret` function below is used to fuse quadratic discriminant analysis with PCA preprocessing where we set the threshold for PCA to retain 90 \% (cumulative percent) of variance. Note that the data is automatically standardized prior to applying PCA.

```{r}
lrn <- makePreprocWrapperCaret(learner = "classif.qda", ppc.pca = TRUE, ppc.thresh = 0.9)
lrn
```

Now, that we have a wrapped learner, calling `train` with the task will train with the principal components returned after PCA has been applied.

```{r}
mod <- mlr::train(lrn, sonar.task)
mod$learner.model
# or, for more info:
getLearnerModel(model = mod, more.unwrap = TRUE)
```

Finally, we carry out a benchmark experiment to explore whether preprocessing with PCA has improved the performance of qda. We choose `stratify = TRUE` in resampling due to each class being represented by a small number of observations.

```{r}
rin <- makeResampleInstance("CV", iters = 3, stratify = TRUE, task = sonar.task)
bmr <- benchmark(learners = list(makeLearner("classif.qda"), lrn), tasks = sonar.task, resamplings = rin, show.info = FALSE)
bmr
```

So far so good; it seems that PCA preprocessing is beneficial for the qda. However, the `thresh` value was chosen somewhat arbitrarily and resulted in 22 principal components. Perhaps with further tuning of the parameter, we can improve the performance of qda. Preprocessing and learner parameters can be tuned jointly. Calling `getParamSet` on the wrapped learner gives us all options.

```{r}
getParamSet(lrn)
```

In what follows, we tune the number of principal components (instead of `ppc.thresh`) and we try two different ways to estimate the posterior probabilities in qda: *plug-in estimates* and *unbiased estimates*. This is controled via the `predict.method` parameter. The hyperparameter tuning is done via a *grid search* this time with a resolution of 10 (consider finer resolutions in real problems).

```{r}
ps <- makeParamSet(
  makeIntegerParam("ppc.pcaComp", lower = 1, upper = getTaskNFeats(sonar.task)),
  makeDiscreteParam("predict.method", values = c("plug-in", "debiased"))
)

ctrl <- makeTuneControlGrid(resolution = 10)
res <- tuneParams(lrn, sonar.task, rin, par.set = ps, control = ctrl, show.info = FALSE)
res
```

The following example shows how to create a custom preprocessing wrapper using `makePreprocWrapper` which adds a scaling option to a learner by coupling it with the function `scale` (note this is actually possible through `makePreprocWrapperCaret`). Since wrappers are implemented using a *train* and *predict* method, we specify custom train and predict functions. The *train* function has to return a list with the preprocessed data set (`$data`) and an element which stores the information required to preprocess the data before prediction (`$control`). In our example, `$control` stores the scaling parameters which are to be used in the prediction.

```{r}
trainfun <- function(data, target, args = list(center, scale)){
  cns <- colnames(data)
  # identify num data - exclude target
  nums <- setdiff(cns[sapply(data, is.numeric)], target) 
  # extract numerical feats
  x <- as.matrix(data[, nums, drop = FALSE])
  x <- scale(x, center = args$center, scale = args$scale)
  # store the scaling parameters in control, needed to preprocess the data before preds.
  control <- args
  if (is.logical(control$center) && control$center)
    control$center = attr(x, "scaled:center")
  if (is.logical(control$scale) && control$scale)
    control$scale = attr(x, "scaled:scale")
  # recombine the data
  data <- data[, setdiff(cns, nums), drop = FALSE]
  data <- cbind(data, as.data.frame(x))
  return(list(data = data, control = control))
}
```

Next, the *predict* function takes in the data (without target variable), the name of the target variable, the`args` that were passed to `trainfun` and the `control` object returned by `trainfun`. 

```{r}
predictfun <- function(data, target, args, control){
  cns <- colnames(data)
  nums <- cns[sapply(data, is.numeric)]
  x <- as.matrix(data[, nums, drop = FALSE])
  x <- scale(x, center = control$center, scale = control$scale)
  data <- data[, setdiff(cns, nums), drop = FALSE]
  data <- cbind(data, as.data.frame(x))
  return(data)
}
```

Now we are going to use the functions we specified in a preprocessing wrapper. We use a regression neural network which does not have a scaling option and couple it with our own center + scale. 

```{r}
lrn <- makeLearner("regr.nnet", trace = FALSE, decay = 1e-02)
lrn <- makePreprocWrapper(lrn, train = trainfun, predict = predictfun, 
                          par.vals = list(center = TRUE, scale = TRUE))
lrn
```


We now compare the performance of a `nnet` with and without scaling using the cross-validated mean squared error as a measure.

```{r}
rdesc <- makeResampleDesc("CV", iters = 3)
r <- resample(lrn, bh.task, rdesc, measures = mse, show.info = FALSE)
r
```

And without scaling.

```{r}
lrn <- makeLearner("regr.nnet", trace = FALSE, decay = 1e-02)
r <- resample(lrn, bh.task, rdesc, measures = mse, show.info = FALSE)
r
```

#### Joint tuning of preprocessing and learner parameters

Usually we have some idea of what preprocessing we want to apply but it's not clear what options work best for each algorithm. It is possible to tune the preprocesssing and learner parameters as we have previously done.

```{r}
lrn <- makeLearner("regr.nnet", trace = FALSE)
lrn <- makePreprocWrapper(lrn, train = trainfun, predict = predictfun, 
                          par.set = makeParamSet(
                            makeLogicalLearnerParam("center"),
                            makeLogicalLearnerParam("scale")
                          ),
                          par.vals = list(center = TRUE, scale = TRUE)
                          )
lrn
getParamSet(lrn)
```

We now tune via a grid search the decay parameter for the learner as well as the center and scale parameters for the preprocessing.

```{r}
rdesc <- makeResampleDesc("Holdout")
ps <- makeParamSet(
  makeLogicalParam("center"),
  makeLogicalParam("scale"),
  makeDiscreteParam("decay", c(0, 0.05, 0.1))
)
ctrl <- makeTuneControlGrid()
res <- tuneParams(lrn, bh.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE)
res
as.data.frame(res$opt.path)
```

### Imputation of missing values

In the mlr package, methods of imputation of missing values include imputation by a constant (mean, median, mode or some other constant), random numbers (some distribution), or based on predictions by a supervised learner. The possibility of custom imputation methods is also available. Note that some of the learning algorithms in mlr which have the `missings` property integrated, can deal with missing values in a sensible way (i.e. not simply deleting observations). Typing `listLearners("regr", properties = "missings")[c("class", "package")]` will give info on those packages if they are installed.

We first look at a simple example making use of the function `impute` on the `airquality` data set. To further demonstrate the functionality of `impute`, we add some missing values in the `Wind` column and then coerce into a factor using `cut`.

```{r}
data("airquality")
airq <- airquality
ind <- sample(nrow(airquality), 10)
airq$Wind[ind] <- NA
airq$Wind <- cut(airq$Wind, c(0, 8, 16, 24))
summary(airq)
sapply(airq, class)
```

The `Ozone` and `Solar.R` variables are of class integer and the `Wind` variable is of class factor. We choose to impute the integer features by the mean and the factor feature by the mode. We also choose to create dummy variables for all integer features indicating which observations were missing. 

```{r}
imp <- impute(airq, classes = list(integer = imputeMean(), factor = imputeMode()), dummy.classes = "integer")
head(imp$data)
```

The function `impute` returns an object with the elements `$data` and `$desc`; the latter stores the information for the imputation. Note that "Imputed" refers to the class of features for which an imputation method was specified (here, 5 integers + 1 factor) and not the features which contain NA values. Here, the target variable was not specified. Next, let us look at a learning task example where the target variable is involved. In the `airquality` data set, we wish to predict the ozone pollution based on meteorological features. Our dataset therefore will have 4 columns: `Ozone`, `Solar.R`, `Wind`, and `Temp`.

```{r}
# prepare data set
# remove Day and Month 
airq <- dplyr::select(airq, -c(Day, Month))
# choose first 100 obs to make up the training set
airq.train <- airq[1:100, ]
airq.test <- airq[-c(1:100), ]
```

In this example, we do the following:

1) impute missing values in `Solar.R` with random numbers drawn from an empirical distribution;

2) Use function `imputeLearner`, which allows the user to use all supervised learning algorithms integrated in mlr, to impute missing values in the factor variable `Wind`. With `imputeLearner`, we need to create a learner whose type must match the variable to be imputed. So, here we go for `classif.<learning_algorithm>` since our feature variable is a factor. Then, all columns apart from the one imputed and the target variable are used as features in the learning algorithm chosen. Note that there are algorithms that can deal with missing values so in our example, to impute `NA`'s in `Wind` using say, a classification tree, only `Temp` and `Solar.R` will be used as features in the tree. The `rpart` algorithm will be able to handle the missing values in `Solar.R`. 

```{r}
imp <- impute(data = airq.train, target = "Ozone", cols = list(Solar.R = imputeHist(), Wind = imputeLearner(makeLearner("classif.rpart"))), dummy.cols = c("Solar.R", "Wind"))
imp$desc
summary(imp$data)
```

To impute the test data the same way as the train data, we can simply use the `imp$desc` object with the function `reimpute`.

```{r}
airq.test.imp <- reimpute(airq.test, desc = imp$desc)
head(airq.test.imp)
```

#### Fuse a learner with imputation

When creating a learner with `makeImputeWrapper`, before training the resulting learner, impute is applied to the training set and, subsequently, before prediction, `reimpute` is called on the test set using the `$desc` object from the training stage. In what follows, we ask for the same imputation as above and we choose a linear regression model as the learner.

```{r}
lrn <- makeImputeWrapper(learner = "regr.lm", cols = list(Solar.R = imputeHist(), Wind = imputeLearner(makeLearner("classif.rpart"))), dummy.cols = c("Solar.R", "Wind")
                         )
lrn
```

To create a task, we need to delete any observations from the target variable (`Ozone`) which are missing. 

```{r}
airq <- subset(airq, subset = !is.na(airq$Ozone))
task <- makeRegrTask(data = airq, target = "Ozone")
rdesc <- makeResampleDesc("CV", iters = 3)
r <- resample(lrn, task, rdesc, show.info = FALSE, models = TRUE)
r$aggr
```

```{r}
lapply(r$models, getLearnerModel, more.unwrap = TRUE)
```

Note that `makePreprocWrapperCaret` is also available for fusion with imputation and a learner but it is somewhat limited. 

### Generic Bagging

With `makeBaggingWrapper`, it is possible to bag an mlr learner to use bagging as a technique to gain more stability. The `makeBaggingWrapper` function takes arguments which decide on the subsets to be chosen for each iteration of the bagging process. So, just like in `randomForest`, we need to train a learner on a subset of data.

- `bw.iters`: the number of subsets (samples) we want to train our learner
- `bw.replace`: logical. sample with replacement (bootstrapping) or without
- `bw.size`: percentage size of sampled bags
- `bw.feats`: percentage size of randomly selected features in bags

In the example below, we compare the performance of a pruned tree (using `RWeka`'s `PART`) with and without bagging on the `Sonar` data.

```{r}
lrn <- makeLearner("classif.rpart")
bag.lrn <- makeBaggingWrapper(lrn, bw.iters = 50, bw.replace = TRUE, bw.size = 0.8, bw.feats = 3/4)
rdesc <- makeResampleDesc("CV", iters = 5)
r <- resample(learner = lrn, task = sonar.task, resampling = rdesc, show.info = FALSE)
r$aggr
rdesc <- makeResampleDesc("CV", iters = 5)
r2 <- resample(learner = bag.lrn, task = sonar.task, resampling = rdesc, show.info = FALSE)
r2$aggr
```

#### Changing the type of prediction

Using `setPredictType`, we can change the type of prediction. For classification problems we can either have labels (these are determined by majority voting over the preductions of the individual models) or posterior class probabilities. Note that the predict type for the base learner always has to be "response". 

```{r}
bag.lrn <- setPredictType(bag.lrn, predict.type = "prob")
```

In the case of a regression problem, the options for prediction type are numeric, response or standard errors.

```{r}
n <- getTaskSize(bh.task)
train.inds <- seq(1, n, 3)
test.inds <- setdiff(1:n, train.inds)
lrn <- makeLearner("regr.rpart")
bag.lrn <- makeBaggingWrapper(learner = lrn) # using defaults for the rest
bag.lrn <- setPredictType(bag.lrn, predict.type = "se")
mod <- mlr::train(bag.lrn, bh.task, subset = train.inds)
head(getLearnerModel(mod), 2)
```

Next we predict the response and calculate the standard deviation for each prediction:

```{r}
pred <- predict(mod, task = bh.task, subset = test.inds)
head(as.data.frame(pred))
```

We can then plot the percentage of lower status of the population against the predicted response (predicted `medv`) with its calculated standard errors using `ggplot2`. 

```{r}
data <- cbind(as.data.frame(pred), getTaskData(bh.task, subset = test.inds))
g <- ggplot(data, aes(x = lstat, y = response, ymin = response - se, ymax = response + se), col = age)
g + geom_point() + geom_linerange(alpha = 0.5)
```

### Tuning

We can set selected hyperparameters in machine learning algorithms by passing them on to `makeLearner`. By tuning the hyperparameters, we can automatically identify values that lead to the best performance. In order to do the tuning, we need to specify the *search space*, the *optimization algorithm*, and the *evaluation method* (i.e. a resampling strategy + a performance measure).

#### Grid search

1. Create the `ParamSet` object which describes the parameter space we want to search.

```{r}
ps <- makeParamSet(
  makeDiscreteParam("C", values = 2^(-2:2)),
  makeDiscreteParam("sigma", values = 2^(-2:2))
)
```

2. Create the `TuneControl` object which describes the optimization strategy to be used and its settings.

```{r}
ctrl <- makeTuneControlGrid()
```

3. Create the resampling description.

```{r}
rdesc <- makeResampleDesc("CV", iters = 3L)
```

4. Tuning the parameters with `tuneParams`.

```{r}
result <- tuneParams("classif.ksvm", task = iris.task, resampling = rdesc, par.set = ps, control = ctrl)
result
```

The `show.info` argument can be set via `configureMlr`; the command `getMlrOptions()` shows the current settings. In the aboce example, `tuneParams` 
performs cross-validation for every element of the cross product (so in this case, we have 5 x 5 different combinations). The optimal parameters (which give the best mean performance) are selected in the end. In the example above, since no measure was selected, the default for classification was used (`mmce`). Alternatively, we can set a different measure; this is shown below.

```{r}
result <- tuneParams("classif.ksvm", task = iris.task, resampling = rdesc, par.set = ps, control = ctrl, measures = list(acc, setAggregation(acc, test.sd)), show.info = FALSE)
result
```

The object returned by `tuneParams` is a list which includes the best parameter settings (accessed via `$x`) and their estimated performance (accessed via `$y`). Through `$opt.path`, we can obtain the performance of all points evaluated.

```{r}
opt.grid <- as.data.frame(result$opt.path)
opt.grid
```

Further, we can visualize a selected performance measure using `geom_tile()` from `ggplot2`. In the code below, we ask for the color of the tiles to represent the achieved accuracy while the labels on the tiles represent the standard deviation for each of the settings attempted.

```{r}
g <- ggplot(opt.grid, aes(x = C, y = sigma, fill = acc.test.mean, label = round(acc.test.sd, 3)))
g + geom_tile() + geom_text(color = "white")
```

#### Using the optimal parameters

Once we have the optimal hyperameter set, we generate a learner and pass the settings as an argument in `setHyperPars`.

```{r}
lrn <- setHyperPars(makeLearner("classif.ksvm"), par.vals = result$x)
mod <- mlr::train(learner = lrn, task = iris.task)
predict(mod, task = iris.task)
```

Above, we used `mkeDiscreteParam` to discretize manually the parameters we wanted to set (here, `C` and `sigma`). We can also use their true numeric value and set `resolution = TRUE` in the control strategy to automatically discretize them. When setting the numerical parameter range, we can also pass a transformation function (`trafo`).

```{r}
ps <- makeParamSet(
  makeNumericParam("C", lower = 12, upper = 12, trafo = function(x) 2^x),
  makeNumericParam("sigma", lower = -12, upper = 12)
)
ctrl <- makeTuneControlGrid(resolution = 3L)
rdesc <- makeResampleDesc("CV", iters = 2L)
res <- tuneParams("classif.ksvm", task = iris.task, resampling = rdesc, par.set = ps, control = ctrl)
res 
```

Note that calling `res$opt.path` as before returns the parameter values on the original scale before the transformation applied as set in the `trafo` option. The transformed parameters can be retrieved using `trafoOptPath`, as shown below.

```{r}
# original scale
as.data.frame(res$opt.path)
# transformed
as.data.frame(trafoOptPath(res$opt.path))
```

The mlr package supports additional tuning algorithms to the traditional grid. In previous parts of this tutorial, we used `makeTuneControlRandom` which uses random search. An iterated F-racing algorithm is also included; the algorithm starts by considering a set of candidate parameters and completely discards any for which any statistical evidence arises against them. We show an example below with the `iris.task`. In this example, we experiment with different kernels in the svm algorithm. However, we wish to tune certain parameters with particular kernels; we can set this using the `requires` argument which states requirements on other parameters. For instance, below we make `sigma` and `degree` dependent on `rbfdot` and `polydot` kernels, respectively.

```{r}
ps <- makeParamSet(
  makeNumericParam("C", lower = -12, upper = 12, trafo = function(x) 2^x),
  makeDiscreteParam("kernel", values = c("vanilladot", "polydot", "rbfdot")),
  makeNumericParam("sigma", lower = -12, upper = 12, trafo = function(x) 2^x, 
                   requires = quote(kernel == "rbfdot")),
  makeIntegerParam("degree", lower = 2L, upper = 5L, 
                   requires = quote(kernel == "polydot"))
)
ctrl <- makeTuneControlIrace(maxExperiments = 200L)
rdesc <- makeResampleDesc("Holdout")
res <- tuneParams(learner = "classif.ksvm", task = iris.task, resampling = rdesc, par.set = ps, control = ctrl, show.info = FALSE)
head(as.data.frame(res$opt.pat))
res
head(as.data.frame(res$opt.path))
```

#### One step further with ModelMultiplexer

We can tune over different models at the same time and set parameters for each model using `makeModelMultiplexerParameterSet`. Once more we look at the `iris.task` classification problem set and tune SVM and randomForest algorithms at the same time. 

```{r, message=FALSE}
base.learners <- list(
  makeLearner("classif.ksvm"),
  makeLearner("classif.randomForest")
)
lrn <- makeModelMultiplexer(base.learners)
ps <- makeModelMultiplexerParamSet(lrn,
       makeNumericParam("sigma", lower = -12, upper = 12, trafo = function(x) 2^x),
       makeIntegerParam("ntree", lower = 1L, upper = 500L)
       )
rdesc <- makeResampleDesc("CV", iters = 2L)
ctrl <- makeTuneControlIrace(maxExperiments = 200L)
res <- tuneParams(lrn, iris.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE)
res
```

#### Control structures for multi-criteria tuning

It is possible that we may want to optimize multiple performance measures simultaneously when tuning parameters. The mlr package offers multi-criteria tuning algorithms as control structures which are then passed onto `tuneParamsMultiCrit`. As an example, we tune SVM hyperparameters on the `sonar.task` classification task where we aim to optimize both `fpr` and `fnr`. 

```{r, message=FALSE}
ps <- makeParamSet(
  makeNumericParam("C", lower = -12, upper = 12, trafo = function(x) 2^x),
  makeNumericParam("sigma", lower = -12, upper = 12, trafo = function(x) 2^x)
)

ctrl <- makeTuneMultiCritControlRandom(maxit = 30L)
rdesc <- makeResampleDesc("Holdout")
res <- tuneParamsMultiCrit("classif.ksvm", sonar.task, rdesc, measures = list(fpr, fnr), par.set = ps, control = ctrl, show.info = FALSE)
res
```

In the case of nontrivial multi-objective optimization problems, the objective functions are said to be conflicting. This could potentially result in the existence of a large number of optimal solutions. Here, as hyperparameters are tuned, the `fpr` and `fnr` measures are calculated and recorded. A *Pareto improvement* is said to occur if, the tuning of a certain parameter set, improves the lot of at least one of the measures. A solution is concerned *Pareto efficient* if no further Pareto improvements can be made. The Pareto efficient solutions make the **Pareto front**. The number of points on the front is the result shown when calling the object of the `tuneParamsMultiCrit`. Finally, the optimal solutions can be accessed using `res$x` (for the parameter sets) and `res$y` (for the estimated performance measures). All solutions cab be visualized using `plotTuneMultiCritResult` with the solutions on the Pareto front shown as bigger points (note that some - if more than one - may completely overlap). 

```{r}
plotTuneMultiCritResult(res)
```

Note that tuning works similarly in tasks other than classification and see also later section on *nested resampling* for unbiased performance estimation.

### Feature Selection

The mlr package supports two different methods for feature selection which are discussed here in this section.

#### Filter methods

Filter methods assign the features a value of **importance** which allows a ranking of the features which allows to subsequent subsetting of features to be used in a learning algorithm. A number of methods are available for calculating feature importance for classification, regression, and survival tasks (current support). The method is passed as an argument in `generateFilterValuesData`; the default is `rf.importance`.

```{r}
fv <- generateFilterValuesData(task = iris.task, method = "information.gain")
fv
```

A vector of methods can also be supplied to obtain feature importance values for each specified method.

```{r}
fv2 <- generateFilterValuesData(task = iris.task, method = c( "information.gain", "chi.squared"))
fv2
```

And these can be visualized by passing the resulting object into `plotFilterValues`.

```{r}
plotFilterValues(fv2)
```

In this example, according to both measures, `Petal.Width` and `Petal.Length` contain the most information about the target variable `Species`. 

Using this information, we can create a new task that includes a subset of the features specified by the user. To select the subset, we pass arguments to `filterFeatures`. We can choose from: `abs = K` where `K` is equal to some number of features to keep, `perc = r` where 0 < `r` < 1 is a percentage of the most important features to keep, and `threshold = m` where `m` denotes a threshold importance value (we keep features with values above `m`). In `filterFeatures`, we can either specify the method of calculating feature importance or, we can pass the object returned from `generateFilterValuesData` through the `fval` argument. 

```{r}
filtered.task <- filterFeatures(iris.task, method = "information.gain", abs = 2)

filtered.task <- filterFeatures(iris.task, fval = fv, perc = 0.25)

filtered.task <- filterFeatures(iris.task, fval = fv, threshold = 0.5)
filtered.task
```

#### Fuse a learner with a filter method

In an experiment with objectives to train a learner, we employ a resampling startegy to identify a validation method. Obviously feature selection is an important step contributing to the overall learner performance and we often want to carry out feature selection prior to each iteration identified in our resampling strategy. In what follows, we return the `iris.task` to train a fast k nearest neighbor classifier. The resampling strategy chosen is 10-fold cross-validation: in each iteration feature selection is applied via the filter method using `information.gain` as the measure and always keeping the 2 most important features.

```{r, message=FALSE}
lrn <- makeFilterWrapper(learner = "classif.fnn", fw.method = "information.gain", fw.abs = 2)
rdesc <- makeResampleDesc("CV", iters = 10)
r <- resample(lrn, iris.task, rdesc, show.info = FALSE, models = TRUE)
r$aggr
```

By calling `models = TRUE`, we store information on the models which allows us to access information like the two selected features for each model trained. We use `getFilteredFeatures` on each of the model fitted.

```{r}
sfeats <- sapply(r$models, getFilteredFeatures)
table(sfeats)
```

In this data set, the importance of `Petal.Length` and `Petal.Width` on `Species` seems to be very stable as they are consistently selected as the top 2 features. 

#### Tuning the size of the feature subset

Just as we tuned the hyperparameters of learning algorithms in the previous section, we can also tune the size of the feature subset. We pass on a control strategy and a range of values corresponding to the number/percentage of features or threshold into `tuneParams`.

```{r}
lrn <- makeFilterWrapper(learner = "regr.lm", fw.method = "chi.squared")
ps <- makeParamSet(makeDiscreteParam("fw.perc", values = c(0.2, 0.5, 0.05)))
rdesc <- makeResampleDesc("CV", iters  = 3)
res <- tuneParams(lrn, bh.task, rdesc, par.set = ps, control = makeTuneControlGrid())
res
```

The `$x`, `$y`, and `$opt.path` elements work as we have seen them before. We can pass the tuned parameter to generate a new wrapped learner for further use.


```{r, message = FALSE}
lrn <- makeFilterWrapper("regr.lm", fw.method = "chi.squared", fw.perc = res$x$fw.perc)
mod <- mlr::train(lrn, bh.task)
mod
getFilteredFeatures(mod)
```

We show another example using the `sonar.task` classification problem and make use of the multi-criteria tuning functions we introduced in the last section.

```{r, message = FALSE}
lrn <- makeFilterWrapper("classif.lda", fw.method = "chi.squared")
ps <- makeParamSet(makeNumericParam("fw.threshold", lower = 0.1, upper = 0.9))
rdesc <- makeResampleDesc("CV", iters = 10)
res <- tuneParamsMultiCrit(lrn, sonar.task, par.set = ps, resampling = rdesc, measures = list(fpr, fnr), control = makeTuneMultiCritControlRandom(maxit = 50), show.info = FALSE)
res
plotTuneMultiCritResult(res)
```

#### Wrapper methods

In the previous subsection, feature selection was accomplished independently from the performance of a specific learning algorithm. We achieved optimal feature selection through maximizing or minimizing a criterion function. We now turn out attention to *wrapper* models where the effectiveness of the performance feature selection model is directly related to the performance of the learning algorithm, usually in terms of its predictive accuracy. Therefore, in the wrapper method, a learner is trained repeatedly on a different subset of features and the subset which leads to the learner with the best performance is chosen. To do this, we need to specify which strategy we want to use to identify the various subsets to be used. Of course we also need to choose a learning algorithm and a resampling strategy by which we aim to assess performance. 

A note on the search strategy for feature selection. The mlr package offers 4 methods:

1) `FeatSelControlExhaustive` : exhaustive search; all feature sets up to a certain number of `max.features` is searched.
2) `FeatSelControlRandom`: random search; feature vectors randomly drawn up to a certain number of `max.features`. 
3) `FeatSelControlSequential`: deterministic forward or backward search. Forward (backward) search starts with smaller (bigger) feature subsets and adds (removes) sequentially. 
4) `FeatSelControlGA`: genetic algorithm. An adaptive search technique based on the analogy with biology in which a set of possible solutions evolves via natural selection.

In the following example, we use the *Wisconsin Prognostic Breast Cancer* data set using a random search strategy.

```{r}
ctrl <- makeFeatSelControlRandom(maxit = 20L)
ctrl
rdesc <- makeResampleDesc("Holdout")
sfeats <- selectFeatures(learner = "surv.coxph", task = wpbc.task, resampling = rdesc, control = ctrl, show.info = FALSE)
sfeats
```

The selected features and corresponding performance are included in the object returned by `selectFeatures`.

```{r}
sfeats$x
sfeats$y
```

We use the `bh.task` in a second example with forward sequential search. Features are added to the model until the performance improvement is smaller than some value (set by the `alpha` parameter). 

```{r}
ctrl <- makeFeatSelControlSequential(method = "sfs", alpha = 0.02)

rdesc <- makeResampleDesc("CV", iters = 10)
sfeats <- selectFeatures(learner = "regr.lm", task = bh.task, resampling = rdesc, control = ctrl, show.info = FALSE)
sfeats
```

Details of the sequential feature selection process can be retrieved using `analyzeFeatSelResult`:

```{r}
analyzeFeatSelResult(sfeats)
```

#### Fuse a learner with a wrapper method

Similarly to what we did above with fusing a learner with the filter, we can do so here with the wrapper. We fuse the feature selection and resampling strategies with a learner. The learner is trained on the selected feature subset.

```{r}
rdesc <- makeResampleDesc("CV", iters = 3)
lrn <- makeFeatSelWrapper(learner = "surv.coxph", resampling = rdesc, control = makeFeatSelControlRandom(maxit = 10), show.info = FALSE)
mod <- train(lrn, wpbc.task)
mod
sfeats <- getFeatSelResult(mod)
sfeats
sfeats$x
```

The 5-fold cross-validated performance of the learner we specified above can be computed through `resample`.

```{r}
out.rdesc <- makeResampleDesc("CV", iters = 5)

r <- resample(learner = lrn, task = wpbc.task, resampling = out.rdesc, models = TRUE, show.info = FALSE)

r$aggr
```

With `models = TRUE` in `resample`, we can access the feature selection for each model by applying `getFeatSelResult`.

```{r}
lapply(r$models, getFeatSelResult)
```





