---
title: "Working with the mlr package"
output: html_document
---

To become familiar with the mlr package and its widespread functionality, I created an Rmarkdown document with basic and advanced features from the mlr tutorial on mostly regression and classification problems with data available in R. 

```{r, echo=FALSE, message=FALSE}
library(plyr)
library(dplyr)
library(caret)
library(randomForest)
library(gbm)
library(mlbench)
library(rattle)
library(parallelMap)
library(RWeka)
library(kernlab)
library(irace)
library(mlr)
```
## Getting started

The first step is to create a **task**; this can be a classification, regression, survival, cluster, cost-sensitive classification or multilabel task. For example, let us start with the `iris` data set from which we wish to predict the `Species` based on features comprised of width and length measurements of sepals and petals. 

```{r}
task <- makeClassifTask(data = iris, target = "Species")
```

There exist other options that can be set when calling the task function like, for instance, weights and blocking. The latter is for observations that are required to be considered together such that when resampling or cross-validating, grouped observations are included either all in the train or all in the test set.

Next, we choose a learner. Here, we choose to learn based on a classification tree fitted through the `rpart` function. 

```{r}
lrn <- makeLearner("classif.rpart")
```

We can get a description of all possible parameter settings for a learner using `?getParamSet(lrn)`:

```{r, results='markup'}
getParamSet(lrn)
```

Further, we can create a description object for a resampling startegy using `makeResampleDesc`. For example, we may wish to carry out a 3-fold cross-validation of `rpart` on `iris`.

```{r}
cv3f <- makeResampleDesc("CV", iters = 3, stratify = TRUE )
```

Finally, we can fit the model specified by `lrn` on the `task` and calculate predictions and performance measures for all training and all test sets specified by the above resampling description.

```{r, message=FALSE}
set.seed(123)
r <- resample(lrn, task, cv3f)
r$aggr 
```

The mean misclassification error is given by `r r$aggr`. The default measure for a classification task is the misclassification error, `mmce` but it is possible to choose a different measure, say *accuracy*, `acc` which is given by  1 - `mmce`. We will see later that we can also define our own measures. 

```{r, message=FALSE}
set.seed(123)
r <- resample(lrn, task, cv3f, measures = acc)
r$aggr 
```



## Basics

### Tasks

As previously mentioned, depending on the type of problem at hand, one can define an appropriate task. We have seen an instance of a classification problem above, let us now look at a supervised regression problem using the `BostonHousing` data. By printing task, we get some information on the task object we have just created and the associated data.

```{r}
data(BostonHousing, package = "mlbench")
task <- makeRegrTask(data = BostonHousing, target = "medv")
print(task)
```

#### Modifying a task

Once the task is created, it can be modified in several ways. For example, the function `subsetTask` allows the selection of certain observations and/or features. 

```{r}
names(BostonHousing)
modifiedtask <- subsetTask(task, subset = 1:400, features = c("crim", "age", "dis", "lstat"))
str(getTaskData(modifiedtask))
```

Using `getTaskData` we can see that the modified task now includes 400 observations and 5 variables (the 4 included in the features character vector above and the target variable which will always be included). Some other useful functions are the following:

  1. `removeConstantFeatures(<task name>)`:
  
  constant features may arise dur to an inherent feature in the data collected or as a result of choosing a subset of observations.
  
  2. `dropFeatures(task, c("rm", "nox"))`:
  
  remove selected features from the task.
  
  3. `normalizeFeatures(task, method = "standardize")`:
  
  normalize numerical features by different methods (nonnumerical features are left untouched). The normalizing method can be one of `center` (subtract mean), `scale` (divide by standard deviation), `standardize` (center and scale), `range` (scale to a given range - default is [0, 1]). The optional argument `exclude` may be used to supply a character vector of columns to be excluded from the normalization. Type `?normalizeFeatures` for more info.

### Learners

Many of the popular learning algorithms are already implemented in `mlr`. The `makeLearner` function requires the user to specify the learning method. Additionally, it is possible to modify defaults on the prediction type (i.e. for classification, we may choose `predict.type = "prob"` for probabilities), or set hyperparameters using a `list` passed to the `par.vals` argument.

```{r}
class.lrn <- makeLearner("classif.randomForest", predict.type = "prob", fix.factors.prediction = TRUE)
regr.lrn <- makeLearner("regr.gbm", par.vals = list(n.trees = 500, interaction.depth = 4))
#cluster.lrn <- makeLearner("cluster.SimpleKMeans", N = 5)
```

Note that the `fix.factors.prediction = TRUE` argument is useful in situations where a factor level is present in the training data set but not the test data set; it adds a factor level for missing data in the test set thus avoiding problems. 

The Learner object is a list and information can be extracted using the `$`, e.g. `regr.lrn$par.vals`. This information can also be accessed using other functions in `mlr`, for instance, `getHyperPars(lrn)` retrieves the current hyperparameter settings of the learner `lrn`. We also used above `getParamSet(lrn)` to get a description of all possible parameter settings for `lrn`. 

#### Modifying a learner

Just like it was possible to modify an existing task, we can also do so for a learner. Modifications include changing the `id` (this is either user specified or, if omitted, it is automatically set to the algorithm name), the prediction type, hyperparameter values, and more.

```{r}
class.lrn <- setPredictType(class.lrn, "response")
regr.lrn <- setHyperPars(regr.lrn, n.trees = 400)
regr.lrn <- removeHyperPars(regr.lrn, c("n.trees", "interaction.depth"))
```

Note `removeHyperPars` sets the hyperparameters back to their default values.

### Train

Once we create the task from the data set and identify the learning algorithm, the next step is to train the learner using the `train` command.

```{r}
task <- makeClassifTask(data = iris, target = "Species")
lrn <- makeLearner("classif.lda")
mod <- mlr::train(lrn, task)
mod
```

The function `train` returns a list. The fitted model can be extracted using the `getLearnerModel(mod)` command. It is possible to choose a subset of observations to be used to train the model; this is achieved via the `subset` argument in `train`. Note that this is usually not needed since resampling strategies are supported. In the following example, we use the `BostonHousing` data to fit a linear model to the regression task. Note that `getTaskSize` returns the number of observations in a task.

```{r}
data(BostonHousing, package = "mlbench")
task <- makeRegrTask(data = BostonHousing, target = "medv")
lrn <- makeLearner("regr.lm")
train.set <- sample(getTaskSize(task), size = getTaskSize(task)/3)
mod <- mlr::train(lrn, task, subset = train.set)
getLearnerModel(mod)
```

Finally, a note on weights passed as an argument in the `train` function. As an example for an application of weights ^[note that mlr offers alternatives with more functionality for imbalanced classification problems] used in `train`, consider the `BreastCancer` data for which the target variable `Class` identifies 241 malignant and 458 benign cases. To deal with the imbalanced classes, we can incorporate weights in an attempt to allow the two classes to be equally represented in training the classifier. Here we use the predefined task `bc.task` in mlr. The `getTaskTargets` function gets the target data from a task (in this example, this is equivalent to the vector `BreastCancer$Class`). Note that if weights are defined in task as well then those would be overwritten by the weights in `train`. 

```{r}
target <- getTaskTargets(bc.task)
tab <- as.numeric(table(target))
#obtain inverse class frequencies for the weights
w <- 1/tab[target] 
mod <-mlr:: train("classif.rpart", task = bc.task, weights = w)
fancyRpartPlot(getLearnerModel(mod), sub = "")
```

Weights can also be useful as a means to grant more importance to recently collected data versus older data or to reduce the influence of outliers.

### Predict

To predict target values, we use the `predict` function which takes as input the object returned by `train` and data for which we want predictions. The data can either come from the task (using the `task` argument) or it can be a data frame (passed using the `newdata` argument). Similarly to the `train` function, the `subset` argument may be used to pass different portions of the data in task. 

```{r}
n <- getTaskSize(bh.task)
train.set <- seq(1, n, by = 2)
test.set <- seq(2, n, by = 2)
lrn <- makeLearner("regr.gbm", n.trees = 100, interaction.depth = 4)
mod <- mlr::train(lrn, bh.task, subset = train.set)
preds <- predict(mod, task = bh.task, subset = test.set)
preds
```

The function `predict` returns a list; The `$data` element of that list contains the true values of the target variable (in case of supervised learning) and the predictions. A direct way to obtain the true and predicted values of the target variable is through the `getPredictionTruth(preds)` and `getPredictionResponse(preds)` commands where `preds` is the list returned by the `predict` function.

```{r}
head(getPredictionTruth(preds), 10)
head(getPredictionResponse(preds), 10)
```

For classification problems, class labels are predicted. We can obtain a confusion matrix through the command `getConfMatrix`. To get predicted posterior probabilities, we need to create the learner with `predict.type = "prob"`.

```{r}
lrn <- makeLearner("classif.rpart", predict.type = "prob")
mod <- mlr::train(lrn, iris.task)
preds <- predict(mod, newdata = iris)
head(as.data.frame(preds))
head(getPredictionProbabilities(preds))
```

We can also adjust the threshold value that is used to map the predicted posterior probabilities to the class labels. The default value for binary classification is 0.5; however, it may be necessary to increase/decrease this value in various situations such as in the case of classifying cancer rates where one would be concerned with false negatives (prediction of no cancer when the truth is yes cancer). By changing the threshold we change the sensitivity of the model. An example for adjusting the threshold in a binary classification setting is shown below. 

```{r}
data(Sonar)
table(Sonar$Class)
getTaskDescription(sonar.task)$positive
lrn <- makeLearner("classif.rpart", predict.type = "prob")
mod <- mlr::train(lrn, task = sonar.task)
# predict with default threshold
preds <- predict(mod, sonar.task)
preds$threshold
# set threshold value for +ve class
preds2 <- setThreshold(preds, 0.9)
# confusion matrices
getConfMatrix(preds)
getConfMatrix(preds2)
```

For multiclass classification problems, the threshold is given by a named vector specifying the values by which each probability will be divided. 

### Performance

There are many available performance measures implemented in mlr but one can also create their own performance measures. For a particular prediction object, say `preds` calling `performance(preds)` will give the calculated performance measure. It is also possible to calculate the time needed to train the learner (passing `timetrain` as an argument - see below), the time needed to compute the prediction (`timepredict`) or both (`timeboth`). 

To obtain a list of available measures suitable for a particular problem type or `task`, use the `listMeasures` argument. The function `getDefaultMeasure` shows the defaults for a particular learner or task.

```{r}
listMeasures("classif", properties = "classif.multi")
getDefaultMeasure(bh.task)
```

The following piece of `R` code shows how to obtain the performance measure from the prediction object.

```{r}
n <- getTaskSize(bh.task)
train.set <- seq(1, n, by = 2)
test.set <- seq(1, n, by = 2)
lrn <- makeLearner("regr.gbm", n.trees = 1000)
mod <- mlr::train(lrn, task = bh.task, subset = train.set)
preds <- predict(mod, task = bh.task, subset = test.set)
performance(preds)
```

To change the performance measure, we can do so via the `measures` argument. It is possible to calculate several performance measures by passing them as a list.

```{r}
performance(preds, measures = medse)
performance(preds, measures = list(mse, medse, mae))
```

It is a necessary requirement to pass the model or the task in order to calculate some performance measures. For instance, for `timetrain` calculations, the model also needs to be passed as ar argument.

```{r}
performance(preds, measures = timetrain, model = mod)
```

For clustering problems, the task is required.

```{r}
lrn <- makeLearner("cluster.kmeans", centers = 3)
mod <- mlr::train(lrn, task = mtcars.task)
preds <- predict(mod, task = mtcars.task)
performance(preds, measures = dunn, task = mtcars.task)
```


As previously mentioned, the threshold for classification problems alters the sensitivity of the model and therefore affects performance. The command `generateThreshVsPerfData` used on the prediction object along with a performance measure (or a list of them) generates data on the learner performance versus the threshold. T

```{r}
lrn <- makeLearner("classif.lda", predict.type = "prob")
n <- getTaskSize(sonar.task)
train.set <- seq(1, n, by = 2)
test.set <- seq (2, n, by = 2)
mod <- mlr::train(lrn, task = sonar.task, subset = train.set)
preds <- predict(mod, task = sonar.task, subset = test.set)
performance(preds, measures = list(fpr, fnr, mmce))
d <- generateThreshVsPerfData(preds, measures = list(fpr, fnr, mmce))
plotThreshVsPerf(d)
```

### Resampling

Resampling strategies are often used to assess the performance of a learning algorithm by splitting the data into multiple training and test sets. Each training set is used to train a learner and each test set is reserved for predictions. We get the mean performance measure obtained by agreggating all individual performances. The `makeResampleDesc` function is used to choose the resampling strategy; the method argument can be set to one of cross-validation, leave-one-out cross-validation, repeated cross-validation, out-of-bag bootstrap, subsampling (a.k.a. *Monte-Carlo* cross-validation), and holdout. Additional arguments can be passed to the function depending on the chosen method. Once the resampling description is specified, we use the function `resample` to fit a model specified by a learner on a task, which calculates predictions and performance measures for all training and test sets as specified by the resampling description.

```{r}
rdesc <- makeResampleDesc("CV", iters = 3, stratify = TRUE)
task <- makeClassifTask(data = iris, target = "Species")
lrn <- makeLearner("classif.rpart")
r <- resample(lrn, task, rdesc)
r
r$measures.test
r$aggr
r$measures.train
```

The `resample` function returns a list whose elements we can access using the `$` notation. In the above exmaple `r$measures.test` gives the performance on the 3 individual test sets (as specified in the resampling description) and `r$measures.train` returns missing values since no predictions on the training sets were made. If we wish to have predictions on the training sets we can set `predict = "both"` or `predict = "train"` as an argument in `makeResampleDesc`. 

Next, we can access `r$pred$data` which gives a data frame of the predictions and true (in supervised learning) values of the target variable. Note that it is possible to pass multiple measures as a list in `resample` (including `timetrain`). 

```{r}
head(r$pred$data)
```

*Stratified resampling* ensures that the same proportion of the classes falls in all partitions of the data such that, in a classification setting, each training/test set trains a model with no class being under-represented. This is especially important in small data sets as well as imbalanced classification problems. The `stratify = TRUE` argument is passed when making the resampling description (as in the `R` code above). The `stratify.cols = <col. name>` argument is used to stratify factor variable inputs to ensure that all subgroups are represented in the data partitions.

#### Accessing individual learners

By default, `resample` does not return the individual learners but can do so by passing the argument `models = TRUE` when calling `resample`. More useful still, is to extract certain information from each model like, for instance, the variable importance for chosen models. This is achieved via the `extract` argument.

```{r}
rdesc <- makeResampleDesc("CV", iters = 3)
lrn <- makeLearner("regr.rpart")
r <- resample(lrn, task = bh.task, resampling = rdesc, extract = function(x)
  x$learner.model$variable.importance, models = FALSE)
r$extract
```

Now, `r$extract` is going to give information on the given function from which we asked to obtain the variable importance. With `models = FALSE` we do not obtain other information on the model so attempting to call `getLearnerModel(r$models[[1]])` would return an error. 

#### Resample instance

The command `makeResampleInstance` takes as arguments an object of class ResampleDesc and a task [or the size of the data set i.e. nrow(<data>)]. This creates a ResampleInstance object which mainly stores the indices of the training and test sets used in each iteration. This feature may be useful in situations where we want to perform paired experiments like testing the performance of several learners on exactly the same data. 

```{r}
rdesc <- makeResampleDesc("CV", iters = 3, stratify = TRUE)
rin <- makeResampleInstance(rdesc, task = iris.task)
rin$train.inds[[2]]
```

In `makeResampleInstance` the indices are drawn randomly; the function `makeFixedHoldoutInstance` allows the training and test sets to be specified manually. 

```{r}
rin <- makeFixedHoldoutInstance(train.inds = 1:100, test.inds = 101:150, size = 150)
rin
```

In resampling startegies, we get a performance measure which is aggregated over all the measures calculated for each iteration. By default, the aggregated score is the mean error on the test set. We can change the aggregation method for a measure via `setAggregation(measure, aggr)`. For the different options for `measure` and `aggr` type `?measures` and `?aggregations`, respectively. 

```{r}
m1 <- mmce
m2 <- setAggregation(tpr, test.median) # tpr = true positive rate
rdesc <- makeResampleDesc("CV", iters = 3)
r <- resample("classif.rpart", task = sonar.task, resampling = rdesc, measures = list(m1,m2))
r$aggr
```

To get predictions on both training and test sets, we need to set `predict = "both"` in `makeResampleDesc` and set the aggregation method to `train.mean` if we wish to calculate the mean. An example is shown below.

```{r}
train.mmce <- setAggregation(mmce, train.mean)
rdesc <- makeResampleDesc("CV", iters = 3, predict = "both")
r <- resample("classif.rpart", task = sonar.task, resampling = rdesc, measures = list(mmce, train.mmce))
r$aggr
```

There exist functions which act as convenience wrappers for the various existing resampling strategies. They don't offer as much flexibility as `resample` but can be quick and useful if trying out a number of learners initially. An example with `crossval` is shown below.

```{r}
cviris <- crossval("classif.lda", task = iris.task, iters = 3, measures = list(mmce, ber))
cviris
```


### Benchmark experiments

Using `benchmark`, we can compare different learning algorithms across one or more tasks w.r.t. a given resampling strategy. The function has the advantage of conducting *paired* experiments thus comparing the same training/test sets for the different learners. In the following `R` code, we specify a single task (here, we use the built in `sonar.task`) and apply a LDA and a classification tree learner. The resampling strategy is chosen as `Holdout` hence the performance is calculated on a single randomly sampled test set.

```{r}
lrns <- list(makeLearner("classif.lda"), makeLearner("classif.rpart"))
rdesc <- makeResampleDesc("Holdout")
bmr <- benchmark(learners = lrns, tasks = sonar.task, resamplings = rdesc)
bmr
```

The function `benchmark` returns an object of class BenchmarkResult which contains a list of lists of ResampleResult objects ordered by task and followed by learner. The mlr `getBMR<...>` commands
allows access to the benchmark results.

#### Learner performances

```{r}
getBMRPerformances(bmr, as.df = TRUE)
getBMRAggrPerformances(bmr, as.df = TRUE)
```

The two results (top: individual performance in resampling runs, bottom: aggregated performance values) coincide since `Holdout` was used as the resampling strategy. The optional argument `as.df = TRUE` returns the results in the form of a data frame which is often more convenient. 

#### Predictions

By default, `keep.pred = TRUE` in `benchmark` which allows the user to access the predictions with  `getBMRPredictions`. If `keep.pred = FALSE`, the following command will result in an error.

```{r}
head(getBMRPredictions(bmr, as.df = TRUE))
```

The learner and task ID is automatically set to the name of the algorithm and task if not explicitly specified. Using the ID in `getBMRPredictions`, it is possible to access results for certain learners or tasks. For instance below, we obtain the predictions from the classification tree learner. 

```{r}
head(getBMRPredictions(bmr, learner.ids = "classif.rpart", as.df = TRUE))
```

If unsure of the ID for the learner, task, and performance measure, they can be accessed using `getBMRLearnerIds`,  `getBMRTaskIds`, and `getBMRMeasureIds`, respectively.

#### Models

Similarly, to *Predictions* above, the argument `models = FALSE` must be set in `benchmark` if the user does not want to keep the fitted models for all learners and tasks. If set to true (default), the models may be accessed through `getBMRModels`.

```{r}
getBMRModels(bmr, learner.ids = "classif.lda")
```

More `getBMR<...>` functions exist to extract information on learners, measures, and more.

#### Additional benchmark experiments and merging results

Once a benchmark experiment has been conducted on a task, we may need to add more learners (or, alternatively, we may wish to extend existing learners to other tasks). We can perform another benchmark experiment and then merge the results (through either `mergeBenchmarkResultLearner` or `mergeBenchmarkResultTask`). As an example, we perform another benchmark experiment on `sonar.task` now with random forest and quadratic discriminant analysis learning algorithms. We then fuse this with the  `bmr` object we obtained above to get a single `BenchmarkResult` object.

```{r}
lrns2 <- list(makeLearner("classif.randomForest"), makeLearner("classif.qda"))
bmr2 <- benchmark(learners = lrns2, tasks = sonar.task, resamplings = rdesc, show.info = FALSE)
bmr2
bmrsingle <- mergeBenchmarkResultLearner(bmr, bmr2)
bmrsingle
```

However, note that the resampling description was passed to `benchmark` twice, once to obtain the `bmr` and then the `bmr2` objects. The training/test pairs therefore were most likely different in the first benchmark call than the second. For more accurate merging of benchmark results, we can opt to work with `ResampleInstance` from the start, or extract the `ResampleInstance` from the resample description in the first benchmark call and pass it as argument in later benchmark calls. An example is given below.

```{r}
rin <- getBMRPredictions(bmr)[[1]][[1]]$instance
rin
bmr3 <- benchmark(learners = lrns2, tasks = sonar.task, resamplings = rin, show.info = FALSE)
mergeBenchmarkResultLearner(bmr, bmr3)
```

#### Benchmark analysis & visualization

Once benchmark experiments are conducted for the various learners and tasks, we may wish to rank and assess the performance of various algorithms, perform hypotheses tests or visualize the results. The mlr package offers various functions to do so and we explore some of these below using a longer benchmark example than the one above.

```{r}
# list of 3 learners
lrns <-list(makeLearner("classif.lda", id = "lda"), makeLearner("classif.rpart", id = "rpart"), makeLearner("classif.randomForest", id = "randomForest"))
# convertMLBenchObjToTask does exactly what it says... here we create 2 tasks
ring.task <- convertMLBenchObjToTask("mlbench.ringnorm", n = 600)
wave.task <- convertMLBenchObjToTask("mlbench.waveform", n = 600)
tasks <- list(iris.task, sonar.task, pid.task, ring.task, wave.task)
# 10-fold cross-validation 
rdesc <- makeResampleDesc("CV", iters = 10)
meas <- list(mmce, ber, timetrain)
bmr <- benchmark(learners = lrns, tasks = tasks, resamplings = rdesc, measures = meas, show.info = FALSE)
bmr
```

The individual performances on each iteration for each learner, task, and measure can be accessed via `getBMRPerformances` as shown below:

```{r}
perf <- getBMRPerformances(bmr, as.df = TRUE)
head(perf)
```

Performance tables like the one shown above get increasingly harder to read and comprehend with more experiments. A more convenient way to view the results is by plotting and visualization.

#### Plots

The function `plotBMRBoxplots` takes as input a benchmark object and displays a selected performance measure for all tasks and for all learners as a box or violin plot using the `ggplot2` graphics. 

```{r}
plotBMRBoxplots(bmr, measure = mmce) + facet_wrap(~ task.id, nrow = 2)
```

```{r}
plotBMRBoxplots(bmr, measure = ber, style = "violin") + aes(color = learner.id) +
  facet_wrap(~ task.id, nrow = 2)
```

The aggregated measure score on the test set (e.g. `mmce.test.mean`) is retrieved from the `benchmark` output for each learner and task and displayed through `plotBMRSummary`. By default the first measure is used. Note that the argument `jitter = 0.05` is a vertical distance added between points to prevent overplotting.

```{r}
plotBMRSummary(bmr)
```

Clearly, functionality showing the relative performance is of interest and what we are usually after with the benchmark experiments. The function `convertBMRToRankMatrix` calculates the rank based on a selected learner aggregated performance measure. 

```{r}
m <- convertBMRToRankMatrix(bmr, measure = mmce)
m
```

Alternatively, we can visualize the ranking results as a bar chart using `plotBMRRanksAsBarChart`. The ranks are displayed from best to worst on the horizontal axis and the tasks are shown on the vertical axis.

```{r}
plotBMRRanksAsBarChart(bmr, pos = "tile")
```

#### Hypothesis tests

Hypothesis tests can be used to conclude whether there is a significant difference between the performance of the various learners. While parametric hypothesis tests may have more power over nonparametric tests, they make assumptions about the underlying distributions from which the sample was drawn from which often means that, in order for the results to be at all reliable, we would need many data sets to show significance differences at reasonable significance levels. The mlr package provides the **Overall Friedman test** and the **Friedman-Nemenyi post hoc test**.

The Friedman test is the nonparametric alternative to a one-way ANOVA with repeated measures. We use it to compare three or more learners where the data used is the same in each learning algorithm. Unlike the ANOVA which requires the sample is drawn from a normal distribution and equal variances of the residuals, the Friedman test is free from such restrictions (but, as mentioned above, less powerful). The hypotheses for the comparison are $H_0$: The distributions (whatever they are) are the same across repeated measures and $H_1$: The distributions across repeated measures are different.

```{r}
friedmanTestBMR(bmr)
```

Next, if the Friedman test results show a significant $p$-value (depending on the significance level you set i.e. $p$ < $\alpha$ for significance), then this would mean that we can reject the null that all learners perform the same but at this point we don't know which ones are superior. Therefore, our next step will be to try and find out which pairs of our groups are significantly different then each other with a post hoc analysis. We carry this out with `friedmanPostHocTestBMR`. The following `R` code demonstrates this with a choice of a significance level of 0.1.

```{r}
friedmanPostHocTestBMR(bmr, p.value = 0.1)
```

The results show that a significance level of 0.1, we can reject the null that there exists no performance difference between `rpart` and `randomForest`. 

#### Custom plots

Examples of custom plots using the objects returned by `getBMRPerformances` and `getBMRAggrPerformances`. 

**Density plots**

```{r}
perf <- getBMRPerformances(bmr, as.df = TRUE)
qplot(mmce, colour = learner.id, facets = . ~ task.id,
data = perf[perf$task.id %in% c("iris-example", "Sonar-example"), ],
geom = "density")
```

In the following `R` code, we reshape the `perf` data frame by keeping the variables `task.id`, `learner.id`, and `iter` and collect all measures into a single column (`variable`) and their corresponding values in a second column (`value`).

```{r}
head(perf)
perfdf <- reshape2::melt(perf, id.vars = c("task.id", "learner.id", "iter"))
head(perfdf)
```

We plot boxplots for `mmce` and `timetrain` for each task.

```{r}
perfdf <- perfdf[perfdf$variable != "ber", ]
qplot(variable, value, data = perfdf, colour = learner.id, geom = "boxplot",
      xlab = "measure", ylab = "performance") +
  facet_wrap(~ task.id, nrow = 2)
```

Further insight may be gained on learner performance by comparing the performance in each fold for a particular task; one learner could be performing exceptionally well in a single iteration while another may be performing exceptionally bad. We do this on the sonar data below by collecting the misclassification errors computed by the three learners we used. 

```{r}
perf <- getBMRPerformances(bmr, task.ids = "Sonar-example", as.df = TRUE)
df <- reshape2::melt(perf, id.vars = c("task.id", "learner.id", "iter"))
df <- df[df$variable == "mmce",]
df <- reshape2::dcast(df, task.id + iter ~ variable + learner.id)
head(df)
# scatterplot matrix
GGally::ggpairs(df, 3:5)
```


### Parallelization

A number of instances are parallelizable with mlr. Parallelization is activated using `parallelMap::parallelStart`, the first loop mlr encounters (which is parallel executable) will be automatically parallelized. 

```{r}
# parallelStartSocket(2) 
parallelStartMulticore(cpus = 2) # better for macosx?
rdesc <- makeResampleDesc("CV", iters = 3)
r <- resample("classif.lda", task = iris.task, rdesc)
parallelStop()
```

The `parallelStart` functions have an optional argument `levels`; by setting it, we can control which level gets parallelized. For instance, we may be running a few benchmark experiments which use an elaborate resampling description. In such a case, it would be best to have the resampling parallelized and not the benchmark so we could set `level = "mlr.resample"` in the `parallelStart` function. Not all levels are supported, the current version (2.8) supports `mlr.benchmark`, `mlr.resample`, `mlr.selectFeatures`, and `mlr.tuneParams`.

For custom learners, they need to be exported to the slave before calling the `parallelStart` functions:

`parallelExport("trainLearner.regr.<myregrlearner>", "predictLearner.regr.<myregrlearner>")`

#### Visualizations

We have already seen various `generate` functions within mlr which generate data that can then be used for plotting and visualization using `plotting` functions. Let us revisit the binary classification task with the sonar data set and plot the classifier performance against the boundary decision threshold.

```{r}
lrn <- makeLearner("classif.lda", predict.type = "prob")
n <- getTaskSize(sonar.task)
mod <- mlr::train(learner = lrn, task = sonar.task, subset = seq(1, n, by = 2))
pred <- predict(mod, task = sonar.task, subset = seq(2, n, by = 2))
d <- generateThreshVsPerfData(pred, measures = list(fpr, fnr, mmce))
head(d$data)
plotThreshVsPerf(d) #GGVIS is also possible but currently experimental
```

Alternatively, the plot can be manually created using the data generated from the `generate` function. Using ggplot we show this below.

```{r}
dnew <- reshape2::melt(data = d$data, id.vars= c("threshold"))
ggplot(data = dnew, aes(threshold, value)) + geom_line() + facet_grid(. ~ variable,
                                                                      scales = "free_y")
```

In this second example, we `generate` partial data on what we are interested in visualizing from the fitted model using `generatePartialPredictionData` and then create plot using `plotPartialPrediction`.

```{r}
sonar <- getTaskData(sonar.task)
pd <- generatePartialPredictionData(mod, input = sonar, features = "V11")
plt <- plotPartialPrediction(pd)
head(plt$data)
plt
```


## Advanced

### Wrappers

#### Introductory example

It is possible to add pre-processing, missing values imputation, tuning, feature selection and other functionality to a learner through the use of wrappers. Each time a wrapper is used around an mlr learner, a new learner is returned and this procedure can be repeated many times.

As a first example we use a bagging wrapper to create a random forest which supports weights and compare it to the unwrapped learner (`base.lrn` below). 

```{r}
data(iris)
task <- makeClassifTask(data = iris, target = "Species", weights = as.integer(iris$Species))
base.lrn <- makeLearner("classif.rpart")
```

The `makeBaggingWrapper` function takes as inputs the learner we want wrapped, the number of iterations which is the number of fitted models in bagging (for `rpart` learner, this would be `ntree` = N base learners), an option for sampling with/without replacement, proportion of randomly selected features (this would be the equivalent of `mtry`, here we set it at .5)

```{r}
wrapped.lrn <- makeBaggingWrapper(learner = base.lrn, bw.iters = 100, bw.feats = 0.5 )
print(wrapped.lrn)
```

Next, we run a `benchmark` experiment with the list of learners comprised of the base and wrapped learners. Note that the default resampling strategy is 10-fold cross-validation.

```{r}
bmr <- benchmark(learners = list(base.lrn, wrapped.lrn), task = task)
bmr
```

We may choose to carry out hyperparameter tuning to hopefully improve on the performance of the new learner. The function `makeTuneWrapper` fuses a learner with a search strategy to select its hyperparameters. This is set in effect when `train` is called (see below). First the algorithm for the hyperparameter optimization must be chosen - this is done via the `makeTuneControl` functions. A grid algorithm is traditional but it may be computationally expensive in high-dimensional spaces. In this example we only explore a small parameter space (we tune `minsplit` and `bw.feats`) with *random search*. The `minsplit` parameter is the minimum number of observations that must exist in a node in order for a split to be attempted and `bw.feats` was discussed above. In the random search, we choose to have sampled settings randomly selected 10 times (`maxit = 10`)

```{r}
ctrl <- makeTuneControlRandom(maxit = 10)
rdesc <- makeResampleDesc("CV", iters = 3)
par.set <- makeParamSet(makeIntegerParam("minsplit", lower = 1, upper = 10),
                        makeNumericParam("bw.feats", lower = 0.25, upper = 1))

tuned.lrn <- makeTuneWrapper(learner = wrapped.lrn, resampling = rdesc,
                             measures = mmce, par.set = par.set, control = ctrl)
print(tuned.lrn)
```

Now we have a learner object like before but this learner internally used `tuneParams` such as when called with `train`, the search strategy and resampling are invoked to choose an optimal set of parameters. The output from `train` is a tuned, bagged learner. 

```{r}
lrn <- mlr::train(learner = tuned.lrn, task = task)
print(lrn)
getTuneResult(lrn)
```


#### Data preprocessing

The mlr package offers many options for data preprocessing. Some of them are directly applied to a task to modify it. These include `capLargeValues`, `createDummyFeatures` (for factor variables), `dropFeatures`, `joinClassLevels`, `mergeSmallFactorLevels`, `normalizeFeatures`, `removeConstantFeatures`, and `subsetTask` which are all pretty self explanatory from their name. 

With the mlr wrapper functionality, the preprocessing steps are done at the time the learner is trained or predictions are made. This is important as it avoids the mistake of integrating processing steps which are data-dependent on the whole data set while the learner is trained only on training/test sets. A more honest performance of the learner is obtained if all preprocessing steps are included in the resampling. This is automatically done when fusing a learner with preprocessing. 

`makePreprocWrapperCaret` permits access to all preprocessing options offered by `caret`'s `preProcess` function while`makePreprocWrapper` is used when writing custom preprocessing methods by defining the actions to be taken before training and before prediction. With these two functions, the preprocessing steps then belong to the learner. This is in contrast to the functions defined above (e.g `normalizeFeatures`) which alters the task. So, here, the task remains unchanged, preprocessing is done for every pair of training/test sets in resampling and not globally on the whole data set, and any parameters pertinent to preprocessing can be tuned with the learner's parameters. 

Firstly, the usage of`makePreprocWrapperCaret` is similar to `caret`'s `preProcess` in the sense that it takes almost all of the formal arguments though their names are prefixed by `ppc.`, e.g. `knnImpute` becomes `ppc.knnImpute`. Secondly, there is no `method` argument; instead, the preprocessing options are passed as individual logical arguments, i.e. `pcc.knnImpute = TRUE`. 

For knn imputation and pca in caret:

`preProcess(x, method = c("knnImpute", "pca"), pcaComp = 10)`

With the mlr wrapper:

`makePreprocWrapperCaret(learner, ppc.knnImpute = TRUE, ppc.pca = TRUE, ppc.pcaComp = 10)`

We show an example where we apply PCA on the sonar data (this poses a binary classification problem with 208 obs and 60 features) for dimensionality reduction. The `makePreprocWrapperCaret` function below is used to fuse quadratic discriminant analysis with PCA preprocessing where we set the threshold for PCA to retain 90 \% (cumulative percent) of variance. Note that the data is automatically standardized prior to applying PCA.

```{r}
lrn <- makePreprocWrapperCaret(learner = "classif.qda", ppc.pca = TRUE, ppc.thresh = 0.9)
lrn
```

Now, that we have a wrapped learner, calling `train` with the task will train with the principal components returned after PCA has been applied.

```{r}
mod <- mlr::train(lrn, sonar.task)
mod$learner.model
# or, for more info:
getLearnerModel(model = mod, more.unwrap = TRUE)
```

Finally, we carry out a benchmark experiment to explore whether preprocessing with PCA has improved the performance of qda. We choose `stratify = TRUE` in resampling due to each class being represented by a small number of observations.

```{r}
rin <- makeResampleInstance("CV", iters = 3, stratify = TRUE, task = sonar.task)
bmr <- benchmark(learners = list(makeLearner("classif.qda"), lrn), tasks = sonar.task, resamplings = rin, show.info = FALSE)
bmr
```

So far so good; it seems that PCA preprocessing is beneficial for the qda. However, the `thresh` value was chosen somewhat arbitrarily and resulted in 22 principal components. Perhaps with further tuning of the parameter, we can improve the performance of qda. Preprocessing and learner parameters can be tuned jointly. Calling `getParamSet` on the wrapped learner gives us all options.

```{r}
getParamSet(lrn)
```

In what follows, we tune the number of principal components (instead of `ppc.thresh`) and we try two different ways to estimate the posterior probabilities in qda: *plug-in estimates* and *unbiased estimates*. This is controled via the `predict.method` parameter. The hyperparameter tuning is done via a *grid search* this time with a resolution of 10 (consider finer resolutions in real problems).

```{r}
ps <- makeParamSet(
  makeIntegerParam("ppc.pcaComp", lower = 1, upper = getTaskNFeats(sonar.task)),
  makeDiscreteParam("predict.method", values = c("plug-in", "debiased"))
)

ctrl <- makeTuneControlGrid(resolution = 10)
res <- tuneParams(lrn, sonar.task, rin, par.set = ps, control = ctrl, show.info = FALSE)
res
```

The following example shows how to create a custom preprocessing wrapper using `makePreprocWrapper` which adds a scaling option to a learner by coupling it with the function `scale` (note this is actually possible through `makePreprocWrapperCaret`). Since wrappers are implemented using a *train* and *predict* method, we specify custom train and predict functions. The *train* function has to return a list with the preprocessed data set (`$data`) and an element which stores the information required to preprocess the data before prediction (`$control`). In our example, `$control` stores the scaling parameters which are to be used in the prediction.

```{r}
trainfun <- function(data, target, args = list(center, scale)){
  cns <- colnames(data)
  # identify num data - exclude target
  nums <- setdiff(cns[sapply(data, is.numeric)], target) 
  # extract numerical feats
  x <- as.matrix(data[, nums, drop = FALSE])
  x <- scale(x, center = args$center, scale = args$scale)
  # store the scaling parameters in control, needed to preprocess the data before preds.
  control <- args
  if (is.logical(control$center) && control$center)
    control$center = attr(x, "scaled:center")
  if (is.logical(control$scale) && control$scale)
    control$scale = attr(x, "scaled:scale")
  # recombine the data
  data <- data[, setdiff(cns, nums), drop = FALSE]
  data <- cbind(data, as.data.frame(x))
  return(list(data = data, control = control))
}
```

Next, the *predict* function takes in the data (without target variable), the name of the target variable, the`args` that were passed to `trainfun` and the `control` object returned by `trainfun`. 

```{r}
predictfun <- function(data, target, args, control){
  cns <- colnames(data)
  nums <- cns[sapply(data, is.numeric)]
  x <- as.matrix(data[, nums, drop = FALSE])
  x <- scale(x, center = control$center, scale = control$scale)
  data <- data[, setdiff(cns, nums), drop = FALSE]
  data <- cbind(data, as.data.frame(x))
  return(data)
}
```

Now we are going to use the functions we specified in a preprocessing wrapper. We use a regression neural network which does not have a scaling option and couple it with our own center + scale. 

```{r}
lrn <- makeLearner("regr.nnet", trace = FALSE, decay = 1e-02)
lrn <- makePreprocWrapper(lrn, train = trainfun, predict = predictfun, 
                          par.vals = list(center = TRUE, scale = TRUE))
lrn
```


We now compare the performance of a `nnet` with and without scaling using the cross-validated mean squared error as a measure.

```{r}
rdesc <- makeResampleDesc("CV", iters = 3)
r <- resample(lrn, bh.task, rdesc, measures = mse, show.info = FALSE)
r
```

And without scaling.

```{r}
lrn <- makeLearner("regr.nnet", trace = FALSE, decay = 1e-02)
r <- resample(lrn, bh.task, rdesc, measures = mse, show.info = FALSE)
r
```

#### Joint tuning of preprocessing and learner parameters

Usually we have some idea of what preprocessing we want to apply but it's not clear what options work best for each algorithm. It is possible to tune the preprocesssing and learner parameters as we have previously done.

```{r}
lrn <- makeLearner("regr.nnet", trace = FALSE)
lrn <- makePreprocWrapper(lrn, train = trainfun, predict = predictfun, 
                          par.set = makeParamSet(
                            makeLogicalLearnerParam("center"),
                            makeLogicalLearnerParam("scale")
                          ),
                          par.vals = list(center = TRUE, scale = TRUE)
                          )
lrn
getParamSet(lrn)
```

We now tune via a grid search the decay parameter for the learner as well as the center and scale parameters for the preprocessing.

```{r}
rdesc <- makeResampleDesc("Holdout")
ps <- makeParamSet(
  makeLogicalParam("center"),
  makeLogicalParam("scale"),
  makeDiscreteParam("decay", c(0, 0.05, 0.1))
)
ctrl <- makeTuneControlGrid()
res <- tuneParams(lrn, bh.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE)
res
as.data.frame(res$opt.path)
```

### Imputation of missing values

In the mlr package, methods of imputation of missing values include imputation by a constant (mean, median, mode or some other constant), random numbers (some distribution), or based on predictions by a supervised learner. The possibility of custom imputation methods is also available. Note that some of the learning algorithms in mlr which have the `missings` property integrated, can deal with missing values in a sensible way (i.e. not simply deleting observations). Typing `listLearners("regr", properties = "missings")[c("class", "package")]` will give info on those packages if they are installed.

We first look at a simple example making use of the function `impute` on the `airquality` data set. To further demonstrate the functionality of `impute`, we add some missing values in the `Wind` column and then coerce into a factor using `cut`.

```{r}
data("airquality")
airq <- airquality
ind <- sample(nrow(airquality), 10)
airq$Wind[ind] <- NA
airq$Wind <- cut(airq$Wind, c(0, 8, 16, 24))
summary(airq)
sapply(airq, class)
```

The `Ozone` and `Solar.R` variables are of class integer and the `Wind` variable is of class factor. We choose to impute the integer features by the mean and the factor feature by the mode. We also choose to create dummy variables for all integer features indicating which observations were missing. 

```{r}
imp <- impute(airq, classes = list(integer = imputeMean(), factor = imputeMode()), dummy.classes = "integer")
head(imp$data)
```

The function `impute` returns an object with the elements `$data` and `$desc`; the latter stores the information for the imputation. Note that "Imputed" refers to the class of features for which an imputation method was specified (here, 5 integers + 1 factor) and not the features which contain NA values. Here, the target variable was not specified. Next, let us look at a learning task example where the target variable is involved. In the `airquality` data set, we wish to predict the ozone pollution based on meteorological features. Our dataset therefore will have 4 columns: `Ozone`, `Solar.R`, `Wind`, and `Temp`.

```{r}
# prepare data set
# remove Day and Month 
airq <- dplyr::select(airq, -c(Day, Month))
# choose first 100 obs to make up the training set
airq.train <- airq[1:100, ]
airq.test <- airq[-c(1:100), ]
```

In this example, we do the following:

1) impute missing values in `Solar.R` with random numbers drawn from an empirical distribution;

2) Use function `imputeLearner`, which allows the user to use all supervised learning algorithms integrated in mlr, to impute missing values in the factor variable `Wind`. With `imputeLearner`, we need to create a learner whose type must match the variable to be imputed. So, here we go for `classif.<learning_algorithm>` since our feature variable is a factor. Then, all columns apart from the one imputed and the target variable are used as features in the learning algorithm chosen. Note that there are algorithms that can deal with missing values so in our example, to impute `NA`'s in `Wind` using say, a classification tree, only `Temp` and `Solar.R` will be used as features in the tree. The `rpart` algorithm will be able to handle the missing values in `Solar.R`. 

```{r}
imp <- impute(data = airq.train, target = "Ozone", cols = list(Solar.R = imputeHist(), Wind = imputeLearner(makeLearner("classif.rpart"))), dummy.cols = c("Solar.R", "Wind"))
imp$desc
summary(imp$data)
```

To impute the test data the same way as the train data, we can simply use the `imp$desc` object with the function `reimpute`.

```{r}
airq.test.imp <- reimpute(airq.test, desc = imp$desc)
head(airq.test.imp)
```

#### Fuse a learner with imputation

When creating a learner with `makeImputeWrapper`, before training the resulting learner, impute is applied to the training set and, subsequently, before prediction, `reimpute` is called on the test set using the `$desc` object from the training stage. In what follows, we ask for the same imputation as above and we choose a linear regression model as the learner.

```{r}
lrn <- makeImputeWrapper(learner = "regr.lm", cols = list(Solar.R = imputeHist(), Wind = imputeLearner(makeLearner("classif.rpart"))), dummy.cols = c("Solar.R", "Wind")
                         )
lrn
```

To create a task, we need to delete any observations from the target variable (`Ozone`) which are missing. 

```{r}
airq <- subset(airq, subset = !is.na(airq$Ozone))
task <- makeRegrTask(data = airq, target = "Ozone")
rdesc <- makeResampleDesc("CV", iters = 3)
r <- resample(lrn, task, rdesc, show.info = FALSE, models = TRUE)
r$aggr
```

```{r}
lapply(r$models, getLearnerModel, more.unwrap = TRUE)
```

Note that `makePreprocWrapperCaret` is also available for fusion with imputation and a learner but it is somewhat limited. 

### Generic Bagging

With `makeBaggingWrapper`, it is possible to bag an mlr learner to use bagging as a technique to gain more stability. The `makeBaggingWrapper` function takes arguments which decide on the subsets to be chosen for each iteration of the bagging process. So, just like in `randomForest`, we need to train a learner on a subset of data.

- `bw.iters`: the number of subsets (samples) we want to train our learner
- `bw.replace`: logical. sample with replacement (bootstrapping) or without
- `bw.size`: percentage size of sampled bags
- `bw.feats`: percentage size of randomly selected features in bags

In the example below, we compare the performance of a pruned tree (using `RWeka`'s `PART`) with and without bagging on the `Sonar` data.

```{r}
lrn <- makeLearner("classif.rpart")
bag.lrn <- makeBaggingWrapper(lrn, bw.iters = 50, bw.replace = TRUE, bw.size = 0.8, bw.feats = 3/4)
rdesc <- makeResampleDesc("CV", iters = 5)
r <- resample(learner = lrn, task = sonar.task, resampling = rdesc, show.info = FALSE)
r$aggr
rdesc <- makeResampleDesc("CV", iters = 5)
r2 <- resample(learner = bag.lrn, task = sonar.task, resampling = rdesc, show.info = FALSE)
r2$aggr
```

#### Changing the type of prediction

Using `setPredictType`, we can change the type of prediction. For classification problems we can either have labels (these are determined by majority voting over the preductions of the individual models) or posterior class probabilities. Note that the predict type for the base learner always has to be "response". 

```{r}
bag.lrn <- setPredictType(bag.lrn, predict.type = "prob")
```

In the case of a regression problem, the options for prediction type are numeric, response or standard errors.

```{r}
n <- getTaskSize(bh.task)
train.inds <- seq(1, n, 3)
test.inds <- setdiff(1:n, train.inds)
lrn <- makeLearner("regr.rpart")
bag.lrn <- makeBaggingWrapper(learner = lrn) # using defaults for the rest
bag.lrn <- setPredictType(bag.lrn, predict.type = "se")
mod <- mlr::train(bag.lrn, bh.task, subset = train.inds)
head(getLearnerModel(mod), 2)
```

Next we predict the response and calculate the standard deviation for each prediction:

```{r}
pred <- predict(mod, task = bh.task, subset = test.inds)
head(as.data.frame(pred))
```

We can then plot the percentage of lower status of the population against the predicted response (predicted `medv`) with its calculated standard errors using `ggplot2`. 

```{r}
data <- cbind(as.data.frame(pred), getTaskData(bh.task, subset = test.inds))
g <- ggplot(data, aes(x = lstat, y = response, ymin = response - se, ymax = response + se), col = age)
g + geom_point() + geom_linerange(alpha = 0.5)
```

### Tuning

We can set selected hyperparameters in machine learning algorithms by passing them on to `makeLearner`. By tuning the hyperparameters, we can automatically identify values that lead to the best performance. In order to do the tuning, we need to specify the *search space*, the *optimization algorithm*, and the *evaluation method* (i.e. a resampling strategy + a performance measure).

#### Grid search

1. Create the `ParamSet` object which describes the parameter space we want to search.

```{r}
ps <- makeParamSet(
  makeDiscreteParam("C", values = 2^(-2:2)),
  makeDiscreteParam("sigma", values = 2^(-2:2))
)
```

2. Create the `TuneControl` object which describes the optimization strategy to be used and its settings.

```{r}
ctrl <- makeTuneControlGrid()
```

3. Create the resampling description.

```{r}
rdesc <- makeResampleDesc("CV", iters = 3L)
```

4. Tuning the parameters with `tuneParams`.

```{r}
result <- tuneParams("classif.ksvm", task = iris.task, resampling = rdesc, par.set = ps, control = ctrl)
result
```

The `show.info` argument can be set via `configureMlr`; the command `getMlrOptions()` shows the current settings. In the above example, `tuneParams` 
performs cross-validation for every element of the cross product (so in this case, we have 5 x 5 different combinations). The optimal parameters (which give the best mean performance) are selected in the end. In the example above, since no measure was selected, the default for classification was used (`mmce`). Alternatively, we can set a different measure; this is shown below.

```{r}
result <- tuneParams("classif.ksvm", task = iris.task, resampling = rdesc, par.set = ps, control = ctrl, measures = list(acc, setAggregation(acc, test.sd)), show.info = FALSE)
result
```

The object returned by `tuneParams` is a list which includes the best parameter settings (accessed via `$x`) and their estimated performance (accessed via `$y`). Through `$opt.path`, we can obtain the performance of all points evaluated.

```{r}
opt.grid <- as.data.frame(result$opt.path)
opt.grid
```

Further, we can visualize a selected performance measure using `geom_tile()` from `ggplot2`. In the code below, we ask for the color of the tiles to represent the achieved accuracy while the labels on the tiles represent the standard deviation for each of the settings attempted.

```{r}
g <- ggplot(opt.grid, aes(x = C, y = sigma, fill = acc.test.mean, label = round(acc.test.sd, 3)))
g + geom_tile() + geom_text(color = "white")
```

#### Using the optimal parameters

Once we have the optimal hyperameter set, we generate a learner and pass the settings as an argument in `setHyperPars`.

```{r}
lrn <- setHyperPars(makeLearner("classif.ksvm"), par.vals = result$x)
mod <- mlr::train(learner = lrn, task = iris.task)
predict(mod, task = iris.task)
```

Above, we used `mkeDiscreteParam` to discretize manually the parameters we wanted to set (here, `C` and `sigma`). We can also use their true numeric value and set `resolution = TRUE` in the control strategy to automatically discretize them. When setting the numerical parameter range, we can also pass a transformation function (`trafo`).

```{r}
ps <- makeParamSet(
  makeNumericParam("C", lower = 12, upper = 12, trafo = function(x) 2^x),
  makeNumericParam("sigma", lower = -12, upper = 12)
)
ctrl <- makeTuneControlGrid(resolution = 3L)
rdesc <- makeResampleDesc("CV", iters = 2L)
res <- tuneParams("classif.ksvm", task = iris.task, resampling = rdesc, par.set = ps, control = ctrl)
res 
```

Note that calling `res$opt.path` as before returns the parameter values on the original scale before the transformation applied as set in the `trafo` option. The transformed parameters can be retrieved using `trafoOptPath`, as shown below.

```{r}
# original scale
as.data.frame(res$opt.path)
# transformed
as.data.frame(trafoOptPath(res$opt.path))
```

The mlr package supports additional tuning algorithms to the traditional grid. In previous parts of this tutorial, we used `makeTuneControlRandom` which uses random search. An iterated F-racing algorithm is also included; the algorithm starts by considering a set of candidate parameters and completely discards any for which any statistical evidence arises against them. We show an example below with the `iris.task`. In this example, we experiment with different kernels in the svm algorithm. However, we wish to tune certain parameters with particular kernels; we can set this using the `requires` argument which states requirements on other parameters. For instance, below we make `sigma` and `degree` dependent on `rbfdot` and `polydot` kernels, respectively.

```{r}
ps <- makeParamSet(
  makeNumericParam("C", lower = -12, upper = 12, trafo = function(x) 2^x),
  makeDiscreteParam("kernel", values = c("vanilladot", "polydot", "rbfdot")),
  makeNumericParam("sigma", lower = -12, upper = 12, trafo = function(x) 2^x, 
                   requires = quote(kernel == "rbfdot")),
  makeIntegerParam("degree", lower = 2L, upper = 5L, 
                   requires = quote(kernel == "polydot"))
)
ctrl <- makeTuneControlIrace(maxExperiments = 200L)
rdesc <- makeResampleDesc("Holdout")
res <- tuneParams(learner = "classif.ksvm", task = iris.task, resampling = rdesc, par.set = ps, control = ctrl, show.info = FALSE)
head(as.data.frame(res$opt.pat))
res
head(as.data.frame(res$opt.path))
```

#### One step further with ModelMultiplexer

We can tune over different models at the same time and set parameters for each model using `makeModelMultiplexerParameterSet`. Once more we look at the `iris.task` classification problem set and tune SVM and randomForest algorithms at the same time. 

```{r, message=FALSE}
base.learners <- list(
  makeLearner("classif.ksvm"),
  makeLearner("classif.randomForest")
)
lrn <- makeModelMultiplexer(base.learners)
ps <- makeModelMultiplexerParamSet(lrn,
       makeNumericParam("sigma", lower = -12, upper = 12, trafo = function(x) 2^x),
       makeIntegerParam("ntree", lower = 1L, upper = 500L)
       )
rdesc <- makeResampleDesc("CV", iters = 2L)
ctrl <- makeTuneControlIrace(maxExperiments = 200L)
res <- tuneParams(lrn, iris.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE)
res
```

#### Control structures for multi-criteria tuning

It is possible that we may want to optimize multiple performance measures simultaneously when tuning parameters. The mlr package offers multi-criteria tuning algorithms as control structures which are then passed onto `tuneParamsMultiCrit`. As an example, we tune SVM hyperparameters on the `sonar.task` classification task where we aim to optimize both `fpr` and `fnr`. 

```{r, message=FALSE}
ps <- makeParamSet(
  makeNumericParam("C", lower = -12, upper = 12, trafo = function(x) 2^x),
  makeNumericParam("sigma", lower = -12, upper = 12, trafo = function(x) 2^x)
)

ctrl <- makeTuneMultiCritControlRandom(maxit = 30L)
rdesc <- makeResampleDesc("Holdout")
res <- tuneParamsMultiCrit("classif.ksvm", sonar.task, rdesc, measures = list(fpr, fnr), par.set = ps, control = ctrl, show.info = FALSE)
res
```

In the case of nontrivial multi-objective optimization problems, the objective functions are said to be conflicting. This could potentially result in the existence of a large number of optimal solutions. Here, as hyperparameters are tuned, the `fpr` and `fnr` measures are calculated and recorded. A *Pareto improvement* is said to occur if, the tuning of a certain parameter set, improves the lot of at least one of the measures. A solution is concerned *Pareto efficient* if no further Pareto improvements can be made. The Pareto efficient solutions make the **Pareto front**. The number of points on the front is the result shown when calling the object of the `tuneParamsMultiCrit`. Finally, the optimal solutions can be accessed using `res$x` (for the parameter sets) and `res$y` (for the estimated performance measures). All solutions cab be visualized using `plotTuneMultiCritResult` with the solutions on the Pareto front shown as bigger points (note that some - if more than one - may completely overlap). 

```{r}
plotTuneMultiCritResult(res)
```

Note that tuning works similarly in tasks other than classification and see also later section on *nested resampling* for unbiased performance estimation.

### Feature Selection

The mlr package supports two different methods for feature selection which are discussed here in this section.

#### Filter methods

Filter methods assign the features a value of **importance** which allows a ranking of the features which allows to subsequent subsetting of features to be used in a learning algorithm. A number of methods are available for calculating feature importance for classification, regression, and survival tasks (current support). The method is passed as an argument in `generateFilterValuesData`; the default is `rf.importance`.

```{r}
fv <- generateFilterValuesData(task = iris.task, method = "information.gain")
fv
```

A vector of methods can also be supplied to obtain feature importance values for each specified method.

```{r}
fv2 <- generateFilterValuesData(task = iris.task, method = c( "information.gain", "chi.squared"))
fv2
```

And these can be visualized by passing the resulting object into `plotFilterValues`.

```{r}
plotFilterValues(fv2)
```

In this example, according to both measures, `Petal.Width` and `Petal.Length` contain the most information about the target variable `Species`. 

Using this information, we can create a new task that includes a subset of the features specified by the user. To select the subset, we pass arguments to `filterFeatures`. We can choose from: `abs = K` where `K` is equal to some number of features to keep, `perc = r` where 0 < `r` < 1 is a percentage of the most important features to keep, and `threshold = m` where `m` denotes a threshold importance value (we keep features with values above `m`). In `filterFeatures`, we can either specify the method of calculating feature importance or, we can pass the object returned from `generateFilterValuesData` through the `fval` argument. 

```{r}
filtered.task <- filterFeatures(iris.task, method = "information.gain", abs = 2)

filtered.task <- filterFeatures(iris.task, fval = fv, perc = 0.25)

filtered.task <- filterFeatures(iris.task, fval = fv, threshold = 0.5)
filtered.task
```

#### Fuse a learner with a filter method

In an experiment with objectives to train a learner, we employ a resampling startegy to identify a validation method. Obviously feature selection is an important step contributing to the overall learner performance and we often want to carry out feature selection prior to each iteration identified in our resampling strategy. In what follows, we return the `iris.task` to train a fast k nearest neighbor classifier. The resampling strategy chosen is 10-fold cross-validation: in each iteration feature selection is applied via the filter method using `information.gain` as the measure and always keeping the 2 most important features.

```{r, message=FALSE}
lrn <- makeFilterWrapper(learner = "classif.fnn", fw.method = "information.gain", fw.abs = 2)
rdesc <- makeResampleDesc("CV", iters = 10)
r <- resample(lrn, iris.task, rdesc, show.info = FALSE, models = TRUE)
r$aggr
```

By calling `models = TRUE`, we store information on the models which allows us to access information like the two selected features for each model trained. We use `getFilteredFeatures` on each of the model fitted.

```{r}
sfeats <- sapply(r$models, getFilteredFeatures)
table(sfeats)
```

In this data set, the importance of `Petal.Length` and `Petal.Width` on `Species` seems to be very stable as they are consistently selected as the top 2 features. 

#### Tuning the size of the feature subset

Just as we tuned the hyperparameters of learning algorithms in the previous section, we can also tune the size of the feature subset. We pass on a control strategy and a range of values corresponding to the number/percentage of features or threshold into `tuneParams`.

```{r}
lrn <- makeFilterWrapper(learner = "regr.lm", fw.method = "chi.squared")
ps <- makeParamSet(makeDiscreteParam("fw.perc", values = c(0.2, 0.5, 0.05)))
rdesc <- makeResampleDesc("CV", iters  = 3)
res <- tuneParams(lrn, bh.task, rdesc, par.set = ps, control = makeTuneControlGrid())
res
```

The `$x`, `$y`, and `$opt.path` elements work as we have seen them before. We can pass the tuned parameter to generate a new wrapped learner for further use.


```{r, message = FALSE}
lrn <- makeFilterWrapper("regr.lm", fw.method = "chi.squared", fw.perc = res$x$fw.perc)
mod <- mlr::train(lrn, bh.task)
mod
getFilteredFeatures(mod)
```

We show another example using the `sonar.task` classification problem and make use of the multi-criteria tuning functions we introduced in the last section.

```{r, message = FALSE}
lrn <- makeFilterWrapper("classif.lda", fw.method = "chi.squared")
ps <- makeParamSet(makeNumericParam("fw.threshold", lower = 0.1, upper = 0.9))
rdesc <- makeResampleDesc("CV", iters = 10)
res <- tuneParamsMultiCrit(lrn, sonar.task, par.set = ps, resampling = rdesc, measures = list(fpr, fnr), control = makeTuneMultiCritControlRandom(maxit = 50), show.info = FALSE)
res
plotTuneMultiCritResult(res)
```

#### Wrapper methods

In the previous subsection, feature selection was accomplished independently from the performance of a specific learning algorithm. We achieved optimal feature selection through maximizing or minimizing a criterion function. We now turn out attention to *wrapper* models where the effectiveness of the performance feature selection model is directly related to the performance of the learning algorithm, usually in terms of its predictive accuracy. Therefore, in the wrapper method, a learner is trained repeatedly on a different subset of features and the subset which leads to the learner with the best performance is chosen. To do this, we need to specify which strategy we want to use to identify the various subsets to be used. Of course we also need to choose a learning algorithm and a resampling strategy by which we aim to assess performance. 

A note on the search strategy for feature selection. The mlr package offers 4 methods:

1) `FeatSelControlExhaustive` : exhaustive search; all feature sets up to a certain number of `max.features` is searched.
2) `FeatSelControlRandom`: random search; feature vectors randomly drawn up to a certain number of `max.features`. 
3) `FeatSelControlSequential`: deterministic forward or backward search. Forward (backward) search starts with smaller (bigger) feature subsets and adds (removes) sequentially. 
4) `FeatSelControlGA`: genetic algorithm. An adaptive search technique based on the analogy with biology in which a set of possible solutions evolves via natural selection.

In the following example, we use the *Wisconsin Prognostic Breast Cancer* data set using a random search strategy.

```{r}
ctrl <- makeFeatSelControlRandom(maxit = 20L)
ctrl
rdesc <- makeResampleDesc("Holdout")
sfeats <- selectFeatures(learner = "surv.coxph", task = wpbc.task, resampling = rdesc, control = ctrl, show.info = FALSE)
sfeats
```

The selected features and corresponding performance are included in the object returned by `selectFeatures`.

```{r}
sfeats$x
sfeats$y
```

We use the `bh.task` in a second example with forward sequential search. Features are added to the model until the performance improvement is smaller than some value (set by the `alpha` parameter). 

```{r}
ctrl <- makeFeatSelControlSequential(method = "sfs", alpha = 0.02)

rdesc <- makeResampleDesc("CV", iters = 10)
sfeats <- selectFeatures(learner = "regr.lm", task = bh.task, resampling = rdesc, control = ctrl, show.info = FALSE)
sfeats
```

Details of the sequential feature selection process can be retrieved using `analyzeFeatSelResult`:

```{r}
analyzeFeatSelResult(sfeats)
```

#### Fuse a learner with a wrapper method

Similarly to what we did above with fusing a learner with the filter, we can do so here with the wrapper. We fuse the feature selection and resampling strategies with a learner. The learner is trained on the selected feature subset.

```{r}
rdesc <- makeResampleDesc("CV", iters = 3)
lrn <- makeFeatSelWrapper(learner = "surv.coxph", resampling = rdesc, control = makeFeatSelControlRandom(maxit = 10), show.info = FALSE)
mod <- train(lrn, wpbc.task)
mod
sfeats <- getFeatSelResult(mod)
sfeats
sfeats$x
```

The 5-fold cross-validated performance of the learner we specified above can be computed through `resample`.

```{r}
out.rdesc <- makeResampleDesc("CV", iters = 5)

r <- resample(learner = lrn, task = wpbc.task, resampling = out.rdesc, models = TRUE, show.info = FALSE)

r$aggr
```

With `models = TRUE` in `resample`, we can access the feature selection for each model by applying `getFeatSelResult`.

```{r}
lapply(r$models, getFeatSelResult)
```


### Nested resampling

When we introduced *tuning*, we carry out the optimization over the same data. As a result, the estimated performance could be optimistically biased. To obtain an unbiased performance estimatation, we use **nested resampling**. In nested resampling, data preprocessing and feature selection is included in each training/test data resulting in an honest performance estimate. As an example consider a case where we want to train a learner with hyperparameter tuning. Suppose we choose 3-fold cross-validation as our resampling strategy; this creates three pairs of training/test sets which consist of the *outer resampling loop*. For *each* pair, we perform tuning with a new resampling strategy (say, 4-fold cross-validation), use the tuned parameters on the training set of that pair to fit a model and evaluate the model's performance on the test set. This gives one set of hyperparameters for each of the 3 pairs in the outer resampling.

The mlr package offers a way to do this nested resampling without actually programming any loops:

1) Generate a wrapped learner using `makeTuneWrapper` or `makeFeatSelWrapper` and specify the **inner** resampling strategy using the `resampling` argument.

2) Call function `resample` and pass the **outer** resampling startegy to its `resampling` argument.

Different inner and outer resampling strategies can be used. It is common to have the prediction and performance evaluation on a fixed outer set hence the outer strategy can be set using `makeFixedHoldoutInstance` to generate the outer resampling. For the inner resampling strategy, using `ResampleDesc` is preferable to `ResampleInstance`. The default setting is for the inner resampling strategy to be instantiated just once so tuning/feature selection are compared to the same inner training/test sets. However, if needed, this can be turned off by setting `same.resampling.instance = FALSE` in the `make` tuning/feature selection functions.

Parallelizing nested resampling is usually a good idea as it can be computationally expensive. However, for the purposes of this tutorial and to keep things fast, we keep the iterations in the loops, low.

#### Tuning

We carry out nested resampling with tuning first. The objective is to train a learner using the `ksvm` learner class on a classification task (yes, the `iris` data of course!) and tune the hyperparameters `C` and `sigma`. To set this up, we need a search space and a search strategy, as before (see earlier section on *tuning*). 

```{r}
ps <- makeParamSet(
  makeDiscreteParam("C", values = 2^(-2:2)),
  makeDiscreteParam("sigma", values = 2^(-2:2))
)
ctrl <- makeTuneControlGrid() #using defaults
```

We choose *subsampling* as the inner strategy with 2 iterations (and the default split date of 2/3). In subsampling, the size of the training and test sets is independent of the number of folds (vs *k*-fold CV) but since the split is random, some obervations may never be selected and others may be selected many times. Further, the resampling here is done without replacement so each subsample of size *b* is a sample from the true, unknown distribution of the original sample. This is in contrast with bootstrap resampling where each sample (this is with replacement) has the same size as the original sample but is now from the empirical distribution associated with the original sample.


```{r}
inner <- makeResampleDesc("Subsample", iters = 2)
lrn <- makeTuneWrapper("classif.ksvm", resampling = inner, par.set = ps, control = ctrl, show.info = FALSE)
```

For the outer resampling loop, we choose 3-fold cross-validation.

```{r}
outer <- makeResampleDesc("CV", iters = 3)
r <- resample(lrn, iris.task, resampling = outer, extract = getTuneResult, show.info = FALSE)
r
#error rates on outer tests
r$measures.test
```

Recall that if we want access to the train error rates as well, we need to set `predict = "both"` in the *outer* resampling strategy.

In `resample` we can use the function `extract` to extract infromation from a fitted model during resampling. Keeping the entire model may be too expensive but this can be set using `models = TRUE`. The function `getTuneResult` allows us to access the optimal set of hyperparameters in each iteration in the `outer` loop. When calling `r$extract` we obtain the optimal hyperparameter values as well as the aggregated performance value from the *inner* iterations (here, we asked for 2 iters). This is in contrast with `r$measures.test`. 

```{r}
r$extract
```

It is possible to compare the performance measures for each pair of settings for `C` and `sigma` tried in each iteration in the outer resampling. 

```{r}
opt.paths <- getNestedTuneResultsOptPathDf(r) #returns df
head(opt.paths, 10)
```

The optimal configuration depends on the data but from this experiment, one can identify a good range of hyperparameters that give good performance. Instead of going through the results in a table one by one, it is easier to visualize the results using a tile plot representing the search grid colored by the `mmce.test.mean`.

```{r}
g <- ggplot(opt.paths, aes(x = C, y = sigma, fill = mmce.test.mean)) + geom_tile() + facet_wrap( ~ iter)
g
```

Lastly, if we want to access just the optimal hyperparameter values for each iteration, we can use the `resample` object with `getNestedTuneResultsX`.

```{r}
getNestedTuneResultsX(r)
```

#### Feature selection

Nested resampling with feature selection wrapper works very similary to the tuning example in the previous subsection. We create a learner with the feature selection wrapper and pass an inner resampling strategy. For each iteration in the resampling strategy made up of training/test sets, feature selection according to the strategy chosen is carried out. Then, each training in the outer resampling is trained with its selected features and the performance is evaluated on the correspoding test set. 

```{r}
inner <- makeResampleDesc("CV", iters = 3)
lrn <- makeFeatSelWrapper("regr.lm", resampling = inner, control = makeFeatSelControlSequential(method = "sfs"), show.info = FALSE)

outer <- makeResampleDesc("Subsample", iters = 2)
r <- resample(lrn, bh.task, resampling = outer, extract = getFeatSelResult, show.info = FALSE)
r
r$measures.test
```

Again, we can access the information we asked to be extracted using `r$extract`. We then access the features selected in the two iterations in the outer resampling. 

```{r}
r$extract
r$extract[[1]]$x
r$extract[[1]]$y
```

The optimization path for each iteration in the outer loop can be read using the function `analyzeFeatSelResult` which can be applied to `r$extract` in a loop.

```{r}
opt.paths <- lapply(r$extract, function(x) analyzeFeatSelResult(x))
opt.paths
```

#### Filter methods with tuning

As seen previously, filter methods assign an importance value to each feature; upon setting a threshold value or a fixed number/percentage ranking the most important features, a subset of variables may be selected for training a learner. The threshold or fixed number/percentage of features to keep usually needs to be tuned. To do this, we need to make a `Filter` wrapper where we choose a filtering method -  we tune the parameter associated with the subset selection in a `Tune` wrapper. So we wrap a base learner (here, we fuse linear regression on the `bh.task`) twice.

```{r}
lrn <- makeFilterWrapper(learner = "regr.lm", fw.method = "chi.squared")
ps <- makeParamSet(
  makeDiscreteParam("fw.threshold", values = seq(0, 1, 0.2))
)
ctrl <- makeTuneControlGrid()
inner <- makeResampleDesc("CV", iters = 3)
lrn <- makeTuneWrapper(learner  = lrn, resampling = inner, par.set = ps, 
                       control = ctrl, show.info = FALSE)
```

The subset selection parameter is tuned in the inner resampling loop using data from each iteration in the outer resampling. The chosen parameter is used to train a learner using each of the training set in the outer resampling. The outer resampling strategy here is chosen as 3-fold cross-validation.

```{r}
outer <- makeResampleDesc("CV", iters = 3)
r <- resample(learner = lrn, task = bh.task, resampling = outer, models = TRUE, show.info = FALSE)
r
```

The regression task `bh.task` has 506 observations and 13 features. In the example above we kept the models in `resample` and so we can access information about them contained in the object returned by `resample`. 

```{r}
r$models
```

Note that `r$models` gives us information on the number of observations used to train the learner and what hyperparameters were explored. The number of features indicated is the number included in the task, it does not represent the size of the subset of features selected. Feature selection information can be extracted using the function `getFilteredFeatures`.

```{r}
lapply(r$models, getFilteredFeatures) #all features?
lapply(r$models, function(x) getFilteredFeatures(x$learner.model$next.model))
```

[Ok, so calling `getFilteredFeatures` on `r$models` returns all available features, I suppose that has to do something with the double wrapper but couldn't find info on `next.model` anywhere. When we just used the `FilterWrapper` learner in the previous section, `getFilteredFeatures(r$models)` returned the *selected* features. Since we often do not want to keep the models since it can get expensive, we can use `extract = function(x) getFilteredFeatures(x$learner.model$next.model)` in `resample` to get the filtered features in this case].

We can access the tune results from the full model using the function `getTuneResult`. Of course note that if the models are not kept, then we can still access information on the tuning and, from that, information on the optimization path by setting `extract = getTuneResult` in `resample`. 

```{r}
res <- lapply(r$models, getTuneResult)
res
```

From `res`, we can then obtain information on the optimization paths.

```{r}
opt.paths <- lapply(res, function(x) as.data.frame(x$opt.path))
opt.paths
```


#### Benchmark experiments

We now extend the techniques used in nested resampling above to benchmark experiments where we compare different learners on one or more tasks. Tuning or feature selection occurs using different inner resampling descriptions and then benchmark is called with outer resampling for all tasks.
 
**Example 1: Two tasks, two learners, tuning**

In this first example, we use the `iris` and `sonar` task data with two learners (`ksvm` and `kknn`). We perform tuning on both of them.

```{r}
tasks <- list(iris.task, sonar.task)

# svm tune wrapper in inner resampling loop
ps <- makeParamSet(
  makeDiscreteParam("C", 2^(-1:1)),
  makeDiscreteParam("sigma", 2^(-1:1))
)
ctrl <- makeTuneControlGrid()
inner <- makeResampleDesc("Holdout")
lrn1 <- makeTuneWrapper("classif.ksvm", resampling = inner, par.set = ps, control = ctrl, show.info = FALSE)

# knn tune wrapper in inner resampling loop
ps <- makeParamSet(makeDiscreteParam("k", 3:5))
ctrl <- makeTuneControlGrid()
inner <- makeResampleDesc("Subsample", iters = 3)
lrn2 <- makeTuneWrapper("classif.knn", resampling = inner, control = ctrl, par.set = ps, show.info = FALSE )

lrns <- list(lrn1, lrn2)

# outer resampling loop
outer <- list(makeResampleDesc("Holdout"), makeResampleDesc("Bootstrap", iters = 2))
res <- benchmark(lrns, tasks, outer, measures = list(acc, ber), show.info = FALSE)
# aggregated performances from the outer resampling loop
res
```

The `resamplings` argument in `benchmark` gives resampling strategies for *each* task. If only one is applied, it is replicated to match the number of tasks, if none are applied, the default 10-fold corss-validation is used. So, in the above example, the outer resampling for the `iris` task is set to *holdout* and for the `sonar` task is set to *bootstrap* with 2 iterations. 

We can access the benchmark result using the `get` functions available in the mlr package. We can access the outer resampling individual performances using `getBMRPerformances`. 

```{r}
getBMRPerformances(res, as.df = TRUE)
```

This gives the result of each iteration specified in the outer resampling strategy for each task, trained on each learner. Since for the `iris` task we use *holdout*, the performance is tested on a single test set for each learner. With the `sonar` task, we have *bootstrap* with 2 iterations, we get 2 test sets for each learner. 

To obtain the results of the tuning, we use `getBMRTuneResults` on the object returned by `benchmark`.

```{r}
getBMRTuneResults(res, as.df = TRUE)
```

The `task.ids` and `learner.ids` arguments can be used in `getBMRTuneResults` to access the tuning results from individual learners and tasks. We can access the tuned hyperparameter settings from a nested tuning using `getNestedTuneResultsX`; this takes as an argument a `ResampleResult` so in the case of the `benchmark` object, we need to input the element from the list of results contained in the benchmark object `res$results`.

```{r}
# tuned hyperparameters
getNestedTuneResultsX(res$results[["Sonar-example"]][["classif.ksvm.tuned"]])
# get the opt.paths from each tuning step from the outer resampling
getNestedTuneResultsOptPathDf(res$results[["Sonar-example"]][["classif.ksvm.tuned"]])
```

**Example 2: One task, two learners, feature selection**

In this example, we wrap a linear regression learner with feature selection wrapper achieved by a sequential forward search. The features are selected using a two-iteration, subsample strategy (inner resampling). The second learner is a base `rpart` regression algorithm (no feature selection).

```{r}
ctrl <- makeFeatSelControlSequential(method = "sfs")
inner <- makeResampleDesc("Subsample", iters = 2)
lrn <- makeFeatSelWrapper("regr.lm", resampling = inner, control = ctrl, show.info = FALSE)

lrns <- list(makeLearner("regr.rpart"), lrn)

outer <- makeResampleDesc("Subsample", iters = 2)
res <- benchmark(learners = lrns, tasks = bh.task, resamplings = outer, show.info = FALSE)
res
```

The selected features can be accessed using `getBMRFeatSelResults`:

```{r}
getBMRFeatSelResults(res)
```

Again, we can access specific learners:

```{r}
feats <- getBMRFeatSelResults(res, learner.ids = "regr.lm.featsel")
feats <- feats[["BostonHousing-example"]][["regr.lm.featsel"]]
```

The list `feats` now has information on the outer resampling iterations where for each element in the list, `$x` and `$y` access the features and resampled performance of the test set.

```{r}
feats[[1]]$x
feats[[1]]$y
```

The optimization paths indicating which features were selected during the feature selection search along with the corresponding test performances can be extracted using `$opt.path`. 

```{r}
opt.paths <- lapply(feats, function(x) as.data.frame(x$opt.path))
head(opt.paths[[1]])
```

The function `analyzeFeatSelResult` gives a clearer overview of what feature has been added in sequence with the improvement in performance. 

```{r}
analyzeFeatSelResult(feats[[1]])
```


**Example 3: One task, two learners, feature filtering with tuning**

This is an example where we use feature filtering and tune a parameter in the filtering method. 

```{r}
# feature filtering with tuning in inner loop
lrn <- makeFilterWrapper(learner = "regr.lm", fw.method = "chi.squared")
ps <- makeParamSet(makeDiscreteParam("fw.abs", values = seq_len(getTaskNFeats(bh.task))) )
ctrl <- makeTuneControlGrid()
inner <- makeResampleDesc("CV", iters = 2)
lrn <- makeTuneWrapper(lrn, resampling = inner, par.set = ps, control = ctrl, show.info = FALSE)

# outer loop
lrns <- list(makeLearner("regr.rpart"), lrn)
outer <- makeResampleDesc("Subsample", iters = 3)
res <- benchmark(lrns, tasks = bh.task, resamplings = outer, show.info = FALSE)
res
```

### Cost-sensitive classification

Classifiers are biased to predict well the *majority* class. In regular classification problems, all misclassification errors are taken to be equal. If we take the example of a medical test used to identify patients with tumor (positive class) and patients without tumor (negative class), there should be a higher cost associated with a *false negative* (FN) than a *false positive* (FP). In the former case, we miss the tumor and the patient potentially dies as a result of not undergoing treatment while in the latter case, the patient is, in addition of giving him/her quite a scare, sent to carry out additional tests. The FN error therefore should be associated a higher cost than the FP error. 

The confusion matrix for a binary classification problem with classes `+` and `-` is given below:

Actual/Predicted | +    |    -
--------------- | ------|------
+               |  TP   |   FN
-               |  FP   |   TN

where the usual metrics are:

where the usual metrics are:

sensitivity/recall = $\frac{TP}{TP+FN}$, specificity = $\frac{TN}{FP+TN}$, and precision = $\frac{TP}{TP+FP}$. 

When calculating *classification costs*, we associate a cost with misclassification, i.e. $C(i | j)$ is the cost of misclassifying class $j$ as class $i$. As indicated above, in the medical example, a FN is associated with higher cost than a FP:

$C(-|+) > C(+|-)$.

The associated cost matrix therefore is given as:


Actual/Predicted | +    |    -
--------------- | ------|------
+               |  $C(+|+)$   |   $C(-|+)$
-               |  $C(+|-)$   |   $C(-|-)$

The cost matrix can then be used for cost-sensitive evaluation of classifiers and cost-sensitive classification where the objective is to minimize expected costs. The cost associated with predicting the correct class is of course lowest; usually the diagonal is calculated to be zero or the matrix is rescaled such as the diagonal is made up of zeros. Some algorithms can utilize a cost matrix directly; an example is `rpart`. Alternatively, we can use the cost matrix to calculate a threshold when predicting class labels or carry out rebalancing wherein less costly classes are given higher importance in training. The rebalancing is achieved either through *weights* (if the learner supports them) or through *oversampling/undersampling* We start with examples in **binary classification** and then we move to **multi-class**. 

#### Binary classification examples

In mlr, we use the ordinary `ClassifTask` for classification as in previous parts of this tutorial. The `CostSensTask` is used when the costs are *example-dependent*; in our case the costs are *class-dependent*. In the example below, we use the `GermanCredit` data to create a classification task and then remove constant features using the appropriate function.

```{r}
data(GermanCredit, package = "caret")
credit.task <- makeClassifTask(data = GermanCredit, target = "Class")
credit.task <- removeConstantFeatures(credit.task)
credit.task
```

By default, bad is the positive class, as indicated when printing `credit.task`. We then create the cost matrix (note that this needs to be calculated based on application).

```{r}
costs <- matrix(c(0,1,5,0), 2)
colnames(costs) <- rownames(costs) <- getTaskClassLevels(credit.task)
```

We first create a logistic regression learner to fit the data and predict posterior probabilities. The classes are then predicted from those probabilities with a default threshold of 0.5. Note that `multinom` fits multinomial log-linear models via neural networks.

```{r}
lrn <- makeLearner("classif.multinom", predict.type = "prob", trace = FALSE)
mod <- mlr::train(lrn, credit.task)
pred <- predict(mod, task = credit.task)
pred
```

According to the cost matrix, we should predict class `Good` only if we are very sure that the class label is in fact `Good`. We need to increase the threshold for class `Good` and decrease the threshold for class `Bad`. 

**Theoretical thresholding**

The cost matrix can be used to calculate a theoretical thresholding for the *positive class* [Elkan (2001)](http://web.cs.iastate.edu/~honavar/elkan.pdf), as follows:

$t^* = \frac{C(+|-)-C(-|-)}{C(+|-)-C(+|+)+C(-|+)-C(-|-)}$

With the diagonal being zeros, the formula simplifies quite a bit.

```{r}
th <- costs[2,1]/(costs[2,1]+costs[1,2])
th
```

We can change the threshold before training in `makeLearner` using the argument `predict.threshold = th` where `th` is the desired threshold or, after predidction by calling `setThreshold` on the object returned by `predict`.

```{r}
pred.th <- setThreshold(pred, th)
pred.th
```

Next, we wish to calculate the cost associated with our predictions averaged over the entire data set. Recall that the objective is to choose a model which minimize the costs. To calculate the averaged costs, we need to create a new performance measure using `makeCostMeasure`. 

```{r}
credit.costs <- makeCostMeasure(id = "credit.costs", name = "Credit costs", costs = costs, task = credit.task, best = 0, worst = 5)
credit.costs
```

We now calculate the performance based on our new `credit.costs` measire and the misclassification error rate on both prediction objects `pred` and `pred.th`:

```{r}
performance(pred, measures = list(credit.costs, mmce))
performance(pred.th, measures = list(credit.costs, mmce))
```

No resampling was carried out and therefore the same data (all of it) was used for training and prediction leading to potentially overly optimistic performance values. Below, we fit again a logistic regression model using 3-fold cross-validation to obtain less biased predictions. Note that we create a `ResampleInstance` to specify training/test sets so that we get comparable performance values when we try different cost-sensitive methods on the `credit.task` data.

```{r}
rin <- makeResampleInstance("CV", iters = 3, task = credit.task)
lrn <- makeLearner("classif.multinom", predict.type = "prob", predict.threshold = th, trace = FALSE)
r <- resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE)
r
```

To get the cross-validated performance measures using the default threshold, we use the `performance` function as above:

```{r}
pred.th <- setThreshold(r$pred, 0.5)
performance(pred.th, measures = list(credit.costs, mmce))
```
                            
We can visualize the performance measures (costs, mmce, etc) vs threshold in [0,1] for the positive class using `plotThreshVsPerf` using data from `generateThreshVsPerfData`. The latter requires a prediction object (from `predict`, `resample`, `benchmark`).

```{r}
d <- generateThreshVsPerfData(r, measures = list(credit.costs, mmce))
plotThreshVsPerf(d, mark.th = th)
```

From the plots, we observe that the theoretical threshold (indicated by the vertical line) is a little large.

**Empirical thresholding**

Theoretical thresholding is reliable when the predicted posterior probabilities are correct; empirical thresholding is useful when the probabilities are order-correct. In empirical thresholding, the cost-optimal threshold is selected for a given learner using training data. The function `tuneThreshold` is used to determine the optimal threshold; this is used with a resampling strategy and not on the entire data set, to avoid overfitting. The function `tuneThreshold` returns the optimal threshold and performance for the specified measure (below, we choose to minimize `credit.costs`).

```{r}
lrn <- makeLearner("classif.multinom", predict.type = "prob", trace = FALSE)

r <- resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE)
r

tune.res <- tuneThreshold(pred = r$pred, measure = credit.costs)
tune.res
```


**Rebalancing**

To minimize average costs, observations associated with the less costly class should be given higher importance during training. One way of achieving this is through class weighting. The learner must be able to support observation or class weights to use this method. Alternative methods, over- and undersampling, are considered later in this section.

*Weighting*

```{r}
# check which mlr learners support weights
listLearners("classif", properties = "weights")[c("class", "package")] #obs weights
listLearners("classif", properties = "class.weights")[c("class", "package")] #class weights
```

According to [Elkan 2001](http://web.cs.iastate.edu/~honavar/elkan.pdf), the proportion of observations in the positive class is multiplied by

$\frac{1-t}{t}\frac{t_0}{1-t_0}$,

where $t$ and $t_0$ are the target and original thresholds, respectively. If $t_0=0.5$, the second factor is equal to 1. Alternatively, the proportion of observations in the negative class can be multiplied by the inverse. The *theoretical weights* can be a function of the theoretical threshold if the target threshold equals the theoretical value, $t^*$. 

```{r}
# theoretical weights for positive class corresponding to theoretical threshold
w <- (1-th)/th
w
```

In binary classification, if we choose to set a weight for the positive class then the negative class receives a weight of 1 automatically. The mlr package offers a `makeWeightedClassWrapper` which allows to assign class weights to a learner using the argument `wcw.weight`. If the learner supports observation weights then these are internally generated during training or resampling. For available class weight support, the weights are simply passed on to the appropriate learner parameter.

```{r}
lrn <- makeLearner("classif.multinom", trace = FALSE)
lrn <- makeWeightedClassesWrapper(lrn, wcw.weight = w)
lrn
```

We use the resampling instance created above (`rin`) so that we can compare the performance of *rebalancing* against *thresholding*.

```{r}
r <- resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE)
r
```

Now, the `classif.multinom` learner supports observation weights. If we were to choose a learner that supports class weights like, for instance, `classif.ksvm`, we can pass them directly or using the `makeWeightedClassesWrapper`:

```{r}
# directly
lrn <- makeLearner("classif.ksvm", class.weights = c(Bad = w, Good = 1))
# using wrapper
lrn <- makeWeightedClassesWrapper("classif.ksvm", wcw.weight = w)

r <- resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE)
r
```

Just like with the theoretical threshold, the theoretical weights may not always be suitable. We can tune the `wcw.weight` parameter like any other parameter using `tuneParams`. Using the threshold weight however can help us narrow down the search.

```{r}
lrn <- makeLearner("classif.multinom", trace = FALSE)
lrn <- makeWeightedClassesWrapper(lrn)
ps <- makeParamSet(makeDiscreteParam("wcw.weight", values = seq(4, 12, 0.5) ))

ctrl <- makeTuneControlGrid()

tune.res <- tuneParams(lrn, credit.task, rin, measures = list(credit.costs, mmce), ps, ctrl, show.info = FALSE)

as.data.frame(tune.res$opt.path)[1:3]
```

**Over- and undersampling**

Not all learners support observations or class weights and so in those cases we *change* the training data by over- or undersampling. With the theoretical weighting, we calculated that the positive class in the German credit data should receive a weight of 5 (or, equivalently, the negative class should receive a weight of 1/5). This can be achieved by the function `oversample` (`undersample`) giving it a `rate` of 5 (1/5).

```{r}
credit.task.over <- oversample(credit.task, rate = 5, cl = "Bad")
lrn <- makeLearner("classif.multinom", trace = FALSE)
mod <- train(lrn, credit.task.over)
```

We trained our learner on the changed data in `credit.task.over` but our predictions and the calculated traiining performance needs to be assessed using the original data in `credit.task`. 

```{r}
pred <- predict(mod, task = credit.task)
performance(pred, measures = list(credit.costs, mmce))
```

Of course the usual strategy for more accurate performance measures is resampled performances but calling `resample` with `credit.task.over` does not work since predictions need to be based on the original data. To overcome this issue, we create a wrapped learner with `makeOversampleWrapper`.

```{r}
lrn <- makeLearner("classif.multinom", trace = FALSE)
lrn <- makeOversampleWrapper(lrn, osw.rate = w, osw.cl = "Bad")
lrn

r <- resample(lrn, credit.task, rin, measures = list(credit.costs, mmce), show.info = FALSE)
r
```

We might want to tune the `rate` parameter to minimize the costs. To do so we create a wrapped learner as above and tune `osw.rate` using `tuneParams`.

```{r}
lrn <- makeLearner("classif.multinom", trace = FALSE)
lrn <- makeOversampleWrapper(lrn, osw.cl = "Bad")
ps <- makeParamSet(makeDiscreteParam("osw.rate", seq(3, 7 , 0.25)))
ctrl <- makeTuneControlGrid()
tune.res <- tuneParams(lrn, credit.task, rin, measures = list(credit.costs, mmce), control = ctrl, show.info = FALSE, par.set = ps)
tune.res
```

#### Multi-class examples

It is possible to calculate thresholds and rebalancing weights in a manner similar to that of the binary case if the cost matrix has a special strcuture where $C(k|l)=C(l)$ for $k=1,...,K$ and $k \neq l$. In $C(k|l)$, $k$ is the *predicted class* and $l$ is the *true label*. The above structure implies that the cost of misclassifying an observation is independent of the predicted class label. For our multi-class examples below, we use threshold and rebalancing using the `waveform` data and an associated cost matrix that does *not* conform to the special structure. 

**Theoretical thresholding**

```{r}
df <- mlbench::mlbench.waveform(500)
wf.task <- makeClassifTask(id = "waveform", data = as.data.frame(df), target = "classes")

costs <- matrix(c(0, 5, 10, 30, 0 ,8, 80, 4, 0), 3)
colnames(costs) <- rownames(costs) <- getTaskClassLevels(wf.task)

wf.costs <- makeCostMeasure(id = "wf.costs", name = "waveform costs", costs = costs, task = wf.task, best = 0, worst = 10) # why 10?
```

Here, we calculate a vector of threshold values as long as the number of classes $K$. The threshold vector is calculated as:

threshold = $\frac{1}{(\text{average costs of true classes})}$

The threshold value is chosen to have the artificial structure mentioned previously, i.e. $C(k,l) = C(l)$ for any $k \neq l$ and we choose $C(l)$ to be the average of the true classes. For example, for class 1, the average would be 110/2 = 55 and hence the threshold value is 1/55. 

Once we have the threshold vector, we divide the predicted probabilities by the threshold vector to obtain adjusted probabilities. The class with the highest adjusted probability is predicted. We set the threshold using `setThreshold`, as before.

```{r}
lrn <- makeLearner("classif.rpart", predict.type = "prob")
rin <- makeResampleInstance("CV", iters = 3, task = wf.task)
r <- resample(lrn, wf.task, rin, measures = list(wf.costs, mmce), show.info = FALSE)
r

avg <- rowSums(costs)/2
th <- 1/avg
# important: the threshold vector needs to have names which correspond to the class labels
names(th) = getTaskClassLevels(wf.task)
th

pred.th <- setThreshold(r$pred, threshold = th)
performance(pred.th, measures = list(wf.costs, mmce))
```

It seems that the data on class probabilities found in `pred.th$data` and `r$pred$data` show the original predicted probabilities but it appears that in `pred.th` the class label is predicted using the adjusted probabilities so all is good. 

**Empirical thresholding**

Once again, we can tune the threshold using `tuneThreshold`. The function returns values that lie in [0,1] and sum up to 1 (the scaling does not change the predicted class labels). 

```{r}
tune.res <- tuneThreshold(pred = r$pred, measure = wf.costs)
tune.res

# compare with the standaridized theoretical vector
th/sum(th)
```


**Rebalancing: weighting**

With weights, the function `makeWeightedClassesWrapper` is used just like in the binary case but for multi-class, we also need to specify the length of the vector corresponding to the number of classes. The weight vector can be tuned using `tuneParams`. 

```{r}
lrn <- makeLearner("classif.multinom", trace = FALSE)
lrn <- makeWeightedClassesWrapper(lrn)

ps <- makeParamSet(makeNumericVectorParam("wcw.weight", len = 3, lower = 0, upper = 1))
ctrl <- makeTuneControlRandom()

tune.res <- tuneParams(lrn, wf.task, resampling = rin, par.set = ps, measures = list(wf.costs, mmce), control = ctrl, show.info = FALSE)
tune.res
```


Note that `oversample` and `undersample` are for *binary* classification only.

### Imbalanced classification problems

In imbalanced classification problems, the smaller classes tend to be ignored as noise in classifiers thus failing to predict them when given new data. The mlr package offers various correction methods to tackle this problem. The methods are divided into sampling and cost-based approaches. 

#### Sampling-based approaches

The idea is to adjust the proportion of the classes to increase the weight of the minority class observations within the model. 

1. **Undersampling** : randomly chosen cases of the majority class are eliminated while all of the minority class cases are kept.

2. **Oversampling** : additional cases (copies, artifical observations) of the minority class are generated to increase their effect on the classifier while all majority class cases are kept.

3. **Hybrid** : mixture of over- and undersampling strategies.

The above methods directly access the data which means that the sampling is done as part of the *preprocessing* and may be used with every appropriate classifier. Note that the mlr package supports the first 2 approaches currently. 

**Simple over- and undersampling**

In *undersampling*, the majority class is effected while in *oversampling*, copies of the minority class are generated. In oversampling, the minority cases are considered at least once when fitting the model while exact copies are generated by random sampling with repetitions.

We first look at some simulated data for a binary problem with classes `A` and `B`. 

```{r}
data.imbal.train <- rbind(
  data.frame(x = rnorm(100, mean = 1), class = "A"),
  data.frame(x = rnorm(5000, mean = 2 ), class = "B")
)

task <- makeClassifTask(data = data.imbal.train, target = "class")
task.over <- oversample(task, rate = 8)
task.under <- undersample(task, rate = 1/8)

table(getTaskTargets(task))
table(getTaskTargets(task.under))
table(getTaskTargets(task.over))
```

In `undersample` the rate is in [0,1]; here, `rate = 1/8` implies that the number of majority class observations are reduced to 1/8th of their original size. In `oversample`, rate $\geq$ 1; here, `rate = 8` means the minority class observations have been increased to 8 times of their original size. We compare the performance on each task.

```{r}
lrn <- makeLearner("classif.rpart", predict.type = "prob")
mod <- train(lrn, task)
mod.over <- train(lrn, task.over)
mod.under <- train(lrn, task.under)
data.imbal.test <- rbind(
  data.frame(x = rnorm(10, mean = 1), class = "A"),
  data.frame(x = rnorm(500, mean = 2 ), class = "B")
)

performance(predict(mod, newdata = data.imbal.test), measures = list(mmce, ber, auc))

performance(predict(mod.over, newdata = data.imbal.test), measures = list(mmce, ber, auc))

performance(predict(mod.under, newdata = data.imbal.test), measures = list(mmce, ber, auc))
```

Note that since `mmce` evaluates the overall accuracy of the predictions, the balanced error rate (`ber`) which gives the average of the errors in each class and the area under the ROC curver (`auc`) may be more suitable performance measures.

```{r}
getConfMatrix(predict(mod, newdata = data.imbal.test))
getConfMatrix(predict(mod.under, newdata = data.imbal.test))
getConfMatrix(predict(mod.over, newdata = data.imbal.test))
```

**Over- and undersampling wrappers**

The over- and undersampling may be dont via the use of wrapped learners as well thus keeping the task unmodified.

```{r}
lrn.over <- makeOversampleWrapper(lrn, osw.rate = 8)
lrn.under <- makeUndersampleWrapper(lrn, usw.rate = 1/8)
mod <- train(lrn, task)
mod.over <- train(lrn.over, task)
mod.under <- train(lrn.under, task)


performance(predict(mod, newdata = data.imbal.test), measures = list(mmce, ber, auc))

performance(predict(mod.over, newdata = data.imbal.test), measures = list(mmce, ber, auc))

performance(predict(mod.under, newdata = data.imbal.test), measures = list(mmce, ber, auc))
```

**Extensions to oversampling**

1. SMOTE (Synthetic Minority Oversampling Technique)

As mentioned previously, in oversampling, copies of minority class observations are created; however, this may lead to overfitting. *SMOTE* constructs the "new" cases by randomly choosing an observation and interpolating it with randomly chosen next neighbors (this number can be set in the function) such that an artificial "new" observation is created. Both numeric and factor features are handled within *SMOTE*. Again, we can either modify the task or use a wrapped learner.

```{r}
#  modify a task
task.smote <- smote(task, rate = 8, nn = 5)
table(getTaskTargets(task))

# wrap a learner
lrn.smote <- makeSMOTEWrapper(lrn, sw.rate = 8, sw.nn = 5 )
mod.smote <- train(lrn.smote, task)
performance(predict(mod.smote, newdata = data.imbal.test), measures = list(mmce, ber, auc))
```

2. Overbagging

A second approach is supported in the mlr package for oversampling which is combined with bagging. For each bagging iteration, minority classes are oversampled using `obw.rate`. The majority class cases are either all taken into account with `obw.maxcl = "all"` or bootstrapped with replacement to increase variability between training data sets during iterations using `obw.maxcl = "boot"`. 

To create the wrapped learner for bagging + oversampling wrapped learner, we use the function `makeOverBaggingWrapper` in a similar way as the `makeBaggingWrapper`. The number of iters or fitted models is set through the argument `iters`.

```{r}
# first make the base learner
lrn <- makeLearner("classif.rpart", predict.type = "response") # needs to be response for wrapper
obw.lrn <- makeOverBaggingWrapper(lrn, obw.rate = 8, obw.iters = 3)
```

Prediction works as follows: for classification, we do majority voting to create a discrete label and probabilities are predicted by considering the proportions of all predicted labels. The benefits of overbagging are strongly dependent on the learner specified. For instance, overbagging with the *random forest* as the learning algorithm may be of little use since the learner is already a strong, bagged learner. This is shown in the next example. We train on a wrapped decision tree (`rpart`) and a random forest learner with overbagging and compare the performance. 

```{r}
lrn <- setPredictType(lrn, "prob")
set.seed(34)
rin <- makeResampleInstance("CV", iters = 5, task = task)
r1 <- resample(learner = lrn, task = task, resampling = rin, show.info = FALSE, measures = list(mmce, ber, auc))
r1$aggr

obw.lrn <- setPredictType(obw.lrn, predict.type = "prob")
r2 <- resample(learner = obw.lrn, task = task, resampling = rin, show.info = FALSE, measures = list(mmce, ber, auc))
r2$aggr
```

Now with *random forest*:

```{r}
lrn <- makeLearner("classif.randomForest")
obw.lrn <- makeOverBaggingWrapper(lrn, obw.rate = 8, obw.iters = 3)

lrn <- setPredictType(lrn, "prob")
r1 <- resample(learner = lrn, task = task, resampling = rin, show.info = FALSE, measures = list(mmce, ber, auc))
r1$aggr

# does this bagging step improve performance?
obw.lrn <- setPredictType(obw.lrn, predict.type = "prob")
r2 <- resample(learner = obw.lrn, task = task, resampling = rin, measures = list(mmce, ber, auc), show.info = FALSE)
r2$aggr
```


#### Cost-based approaches

We saw applications of weighted class wrappers (through `makeWeightedClassesWrapper`) in the previous section under *cost-sensitive classification*.

### ROC analysis and performance curves

Threshold values control how predicted posterior probabilities are converted into class labels. *Receiver operating characterstic* (ROC) curves plot the true positive rate (TPR) which is equivalent to the sensitivity (or recall) on the vertical axis against the false positive rate (FPR) which is equivalent to 1-specificity (SPC) on the horizontal axis for all possible threshold values. 

TPR = $\frac{TP}{TP+FN}$ and FPR = 1- SPC, where SPC = $\frac{TN}{TN+FP}$

The ROC curves are useful in:

- assessing performance visualization;

- determining an optimal decision threshold for given class prior probabilities and misclassification costs (helps with cost-sensitive classification?);

- identifying regions where one classifier outperforms another;

- obtaining calibrated estimates of the posterior probabilities.

For more information: [Fawcett (2004)](http://binf.gmu.edu/mmasso/ROC101.pdf), [Fawcett (2006)](https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf), [Flach (ICML 2004)](http://www.cs.bris.ac.uk/~flach/ICML04tutorial/index.html).

Often, in the absence of background knowledge, there is uncertainty about the class priors or misclassification errors at the time of predictions (difficult to quantify costs, or cost variability with time). A good classifier would therefore be required to work well over a range of decision thresholds; the area under the ROC curve (AUC) gives a scalar measure to compare and select classifiers. We have already used `auc` as a measure in the previous section on *imbalanced classification problems*. The `auc` measure is offered by the `ROCR` package and a generalization to multi-class problems is offered by `pROC` through `multiclass.auc`. The latter builds multiple ROC curves to compute the multi-class AUC. 

The following methods are available in the `mlr` package for plotting ROC curves and other performance curves:

1. `plotROCCurves`: using the result of `generateThreshVsPerfData`, choose any two measures to plot the performance curve.

2. Use the function `asROCRPrediction` to generate a `ROCR` prediction object which can then be used with `ROCR`'s `performance` function which, in turn, creates performance objects that are then used with `plot` for visualization.

3. ViperCharts is a web-based platform for visual performance evaluation of classification, prediction, and information retrieval algorithms. Using `plotViperCharts` with a prediction object, mlr provides an interface to ViperCharts.

We show some examples of the three methods available in mlr next. Note that to use the above, we need learners that are capable of predicting probabilities.  

```{r, echo=FALSE}
# learners that support prob
listLearners("classif", properties = c("twoclass", "prob"))
```

#### Using plotROCCurves

As we have already seen, the function `generateThreshVsPerfData` generates data on threshold vs performance for 2-class classification that can be used for plotting. The function takes in an object which is a list of `Prediction`, `ResampleResult` or `BenchmarkResult`. The resulting object can then be used with `plotROCCurves` for plotting using `ggplot2`. 

*Example 1: single predictions*

```{r}
n <- getTaskSize(sonar.task)
train.set <- sample(n, size = round(2/3 * n))
test.set <- setdiff(seq_len(n), train.set)

# linear discriminant analysis learner
lrn1 <- makeLearner("classif.lda", predict.type = "prob")
mod1 <- train(lrn1, sonar.task, subset = train.set)
pred1 <- predict(mod1, task = sonar.task, subset = test.set)
```

Since the objective is to plot ROC curves, we generate data on FPR and TPR. We also compute the `mmce`.

```{r}
df <- generateThreshVsPerfData(pred1, measures = list(fpr, tpr, mmce))
```

We now use `df` with `plotROCCurves` which, by default, plots the first 2 measures passed to `generateThreshVsPerfData` and a dashed diagonal indicating the performance of a random classifier (this can be turned off with `diagonal = FALSE`.

```{r}
plotROCCurves(df)
# corresponding auc:
performance(pred1, auc)
```

The function `plotROCCurves` plots a pair of performance measures against each other. The individual performance measures against the threshold value may be plotted by calling `plotThreshVsPerf(df)`:

```{r}
plotThreshVsPerf(df)
```

We now try a support vector machine with RBF kerner (`ksvm`) on the `sonar.task` and compare the ROC curve between the `lda` and `ksvm` fitted models.

```{r}
lrn2 <- makeLearner("classif.ksvm", predict.type = "prob")
mod2 <- train(lrn2, sonar.task, subset = train.set)
pred2 <- predict(mod2, task = sonar.task, subset = test.set)

# generateThreshVsPerfData generated using a list of predictions to compare performance side-by-side

df <- generateThreshVsPerfData(list(lda = pred1, ksvm = pred2), measures = list(fpr, tpr, mmce))
plotROCCurves(df)
performance(pred2, auc)
```


The `df` object contains the element `$data` from which we can generate custom plots like, for example, superimposed ROC curves which clearly depict the AUC.

```{r}
qplot(x = fpr, y = tpr, data = df$data, color = learner, geom = "path")
```

With `plotROCCurves` we can plot other performance measures by passing the various measures as arguments in `measures` in `generateThreshVsPerfData`. We then choose the ones to be plotted via the `measures` argument in `plotROCCurves`.

```{r}
df <- generateThreshVsPerfData(list(lda = pred1, ksvm = pred2), measures = list(tpr, ppv, tnr))
```

Note that TPR = sensitivity = recall, TNR = specificity. The *positive predictive value* (PPV) is:

PPV = precision = $\frac{TP}{TP+FP}$.

```{r}
# precision/recall plot
plotROCCurves(df, measures = list(tpr, ppv), diagonal = FALSE)

# sensitivity/specificity plot
plotROCCurves(df, measures = list(tnr, tpr), diagonal = FALSE)
```


**Example 2: benchmark experiment**

We can combine some of the steps above using `benchmark` and, in addition, tune some of the parameters in the learners. We use the `sonar.task` again to fit `lda` and `ksvm` learners. The cost of constraint violation (or regularization parameter), `C` is tuned via a tune wrapper. We want a good performance over the whole threshold range so we set out to *maximize* the AUC. 

```{r}
# tune wrapper for ksvm
rdesc.inner <- makeResampleDesc("Holdout")
ms <- list(auc, mmce)
ps <- makeParamSet(
  makeDiscreteParam("C", 2^(-1:1))
)
ctrl <- makeTuneControlGrid()
lrn2 <- makeTuneWrapper(lrn2, resampling = rdesc.inner, measures = ms, par.set = ps, control = ctrl, show.info = FALSE)
```

The becnhmark experiment is conducted with the `lda` learner and the tuned `ksvm` learner with an outer resampling strategy. 

```{r}
# the benchmark experiment
lrns <- list(lrn1, lrn2)
rdesc.outer <- makeResampleDesc("CV", iters = 5)

bmr <- benchmark(lrns, tasks = sonar.task, resamplings = rdesc.outer, measures = ms, show.info = FALSE)
bmr

# the ROC curves
df <- generateThreshVsPerfData(bmr, measures = list(fpr, tpr, mmce))
plotROCCurves(df)
```

The object returned from `generateThreshVsPerf` calculates aggregated performances according to the resampling strategy. The resulting ROC curves are threshold-averaged. If we want to keep the performances from the individual iterations, we can set `aggregate = FALSE`. 

```{r}
# performance measure from individual iterations
df <- generateThreshVsPerfData(bmr, measures = list(fpr, tpr, mmce), aggregate = FALSE)
plotROCCurves(df)
plotThreshVsPerf(df)
```

The 5 test folds may also be merged and draw a single ROC curve instead of averaging them as done above. Averaging methods are preferred as they take into account variability which is needed to compare classifier performance. 


```{r}
preds <- getBMRPredictions(bmr)[[1]]

# merging is achieved by changing the class attribute to Prediction
preds2 <- lapply(preds, function(x) {class(x) = "Prediction"; return(x)})

df <- generateThreshVsPerfData(preds2, measures = list(fpr, tpr, mmce))
plotROCCurves(df)
```


#### Using asROCRPrediction

As mentioned before, the mlr package provides an interface with the `ROCR` package to draw performance plots. To create the plots, we need a ROCR prediction object which we use with `ROCR::performance` to calculate one or more performance measures and finally, we use `ROCR::plot` to generate the performance plot. 

**Example 1: single predictions**

We revisit the `lda` learner trained on the `sonar.task`. 

```{r}
n <- getTaskSize(sonar.task)
train.set <- sample(n, size = (2/3 * n))
test.set <- setdiff(seq_len(n), train.set)

lrn1 <- makeLearner("classif.lda", predict.type = "prob")
mod1 <- train(lrn1, sonar.task, subset = train.set)
pred1 <- predict(mod1, task = sonar.task, subset = test.set)

# convert pred1 to ROCR prediction
ROCRpred1 <- asROCRPrediction(pred1)
# calculate perf measures
ROCRperf1 <- ROCR::performance(ROCRpred1, "tpr", "fpr")
# generate plots
ROCR::plot(ROCRperf1)
```

The advantages of using the ROCR interface lie in the graphical properties available: we can create an ROC curve which is color-coded according to threshold, print selected threshold values onto the curve, and superimpose the convex hull. The convex hull of a set of points in ROC space is a piecewise linear curve connecting a selection of points such that all other points lie below it. The curve is *convex* because each line segment has a slope that it not steeper than the previous one. The curve is drawn with a black, dashed line in the plot below.

```{r}
ROCR::plot(ROCRperf1, colorize = TRUE, print.cutoffs.at = seq(0.1, 0.9, 0.2), lwd = 2)

ch <- ROCR::performance(ROCRpred1, "rch")
ROCR::plot(ch, add = TRUE, lty = 2)
```

**Example 2: benchmark experiments**

Using the same benchmark experiments conducted above, we show the functionality of performance plots with ROCR. 

```{r}
preds <- getBMRPredictions(bmr)[[1]]
ROCRpreds <- lapply(preds, asROCRPrediction)
ROCRperfs <- lapply(ROCRpreds, function(x) ROCR::performance(x, "tpr", "fpr"))
```

The resampling strategy used in the benchmark experiments was a 5-fold, cross-validation so we obtain 5 different models for each classifier trained. We now plot the ROC curves for each iteration (dashed), the averaged ROC (solid), as well as standard error bars for selected fpr. Note that horizontal averaging is also possible as well as threshold averaging (see later plot). As seen previously, different plot commands can be used in sequence to add to the same figure by selecting `add = TRUE`. Note that the function `plotCI` used below is used internally to plot the error bars.

```{r}
# averaged learner 1
plot(ROCRperfs[[1]], col = "blue", avg = "vertical", spread.estimate = "stderror", show.spread.at = seq(0.1, 0.8, 0.1), plotCI.col = "blue", plotCI.lwd = 2, lwd = 2)
# individual perf 5-fold learner 1
plot(ROCRperfs[[1]], col = "blue", lty = 2, lwd = .25, add = TRUE)

# averaged learner 2
plot(ROCRperfs[[2]], col = "red", avg = "vertical", spread.estimate = "stderror", show.spread.at = seq(0.1, 0.6, 0.1), plotCI.col = "red", plotCI.lwd = 2, lwd = 2, add = TRUE)
# individual perf 5-fold learner 2
plot(ROCRperfs[[2]], col = "red", lty = 2, lwd = .25, add = TRUE)

legend("bottomright", legend = getBMRLearnerIds(bmr), lty = 1, lwd = 2, col = c("blue", "red"))
```

It is possible to plot other performance measures. In the next plot, we generate a precision/recall evaluation plot. Here, we obtain a threshold-averaged curve by setting `avg = "threshold"` in `plot`. Note that the ids for the precision and recall performance measures as defined in ROCR are given as `prec` and `rec`, respectively. 

```{r}
preds <- getBMRPredictions(bmr)[[1]]
ROCRpreds <- lapply(preds, asROCRPrediction)

ROCRperfs <- lapply(ROCRpreds, function(x) ROCR::performance(x, "prec", "rec"))

plot(ROCRperfs[[1]], col = "blue", avg = "threshold")
plot(ROCRperfs[[2]], col = "red", avg = "threshold", add = TRUE)
legend("bottomleft", legend = getBMRLearnerIds(bmr), lty = 1, col = c("blue", "red"))
```

It may be of use to plot a single performance measure against the threshold values; this is achieved by calling `ROCR::performance` with only one performance measure (e.g. `acc`). The following plot shows the vertically averaged curve of accuracy vs threshold with the vertical variation around the curve visualized as boxplots.

```{r}
preds <- getBMRPredictions(bmr)[[1]]
ROCRpreds <- lapply(preds, asROCRPrediction)

ROCRperfacc <- lapply(ROCRpreds, function(x) ROCR::performance(x, "acc"))

plot(ROCRperfacc[[1]], avg = "vertical", spread.estimate = "boxplot", show.spread.at = seq(.1, .9, .1), ylim = c(0,1), xlab = "threshold", lwd = 2, col = "blue")
```


#### Viper charts

Using `mlr::plotViperCharts` with an object of class `Prediction`, `ResampleResult` or `BenchmarkResult` gererates a url which hosts the ViperCharts curves. The argument `chart` takes a character which represents the first chart to display in focus in browser. All other charts can be displayed by clicking on the browser page menu. The default chart is `rocc`. If `browse = TRUE`, the url opens in the default browser.

```{r}
z = plotViperCharts(bmr, chart = "rocc", browse = FALSE)
```

Click [here](`r z`) for the plot.

### Multilabel classification

In multilabel classification, there exist multiple targets that can be assigned to each observation vs just one target in multiclass classification. An example is the following: predict topics that are relevant for a document. For instance, a text might be about any of religion, politics, finance or education at the same time or none of these. Two approaches exist to dealing with multilabel problems:

1. *Problem transformation* : transform the multilabel classification problem to binary or multiclass problems. 

2. *Algorithm adaptation* : adapt multiclass algorithms so that they can be directly applied to the problem.

#### Creating a task

We need to create a task using data in the right format. The data frame needs to consist of the features and a logical vector indicating whether each label is present in the observation or not. Once we have the data in the right format, we can use `makeMultilabelTask`. In what follows, we use the *yeast* data which has 2417 observations, 14 target labels and 103 features. We recreate the multilabel classification `yeast.task` already included in mlr by extracting the data, labels and feeding them in`makeMultilabelTask`.

```{r}
yeast <- getTaskData(yeast.task)
labels <- colnames(yeast)[1:14]
yeast.task <- makeMultilabelTask(id = "multi", data = yeast, target = labels)
yeast.task
```


#### Constructing a learner

**1. Problem transformation: binary relevance method**

To use this method we create a classification learner the usual way and then use the *binary relevance method* to convert the multilabel classification problem into simple binary classifications for each target on which the binary learner is applied. In mlr, any learner which supports binary classification can be converted into a binary relevance wrapper. 

```{r}
lrn <- makeLearner("classif.rpart", predict.type = "prob")
multilabel.lrn1 <- makeMultilabelBinaryRelevanceWrapper(lrn)
multilabel.lrn1
```

**2. Agorithm adaptation**

Currently, the only available algorithm adaptation method in `R` which can be used directly to the multilabel classification task is the *random ferns* multilabel algorithm from package `rFerns`. 

```{r}
multilabel.lrn2 <- makeLearner("multilabel.rFerns")
multilabel.lrn2
```


#### Train, predict, performance

Once we have the task and the learner, training, predicting, and performance evaluation follow from the previous sections of this document. 

```{r}
mod1 <- mlr::train(multilabel.lrn1, task = yeast.task)
# can also pass subsets + weights
mod1 <- mlr::train(multilabel.lrn1, task = yeast.task, subset = 1:1500, weights = rep(1/1500, 1500))

mod2 <- mlr::train(multilabel.lrn2, yeast.task, subset = 1:100)
mod2


pred1 <- predict(mod1, task = yeast.task, subset = 1:10) # or
pred1 <- predict(mod1, newdata = yeast[1501:1600, ])
names(as.data.frame(pred1))

pred2 <- predict(mod2, task = yeast.task)
names(as.data.frame(pred2))
```

The prediction object gives us true and predicted values and, depending on the `predict.type` of the learner, we can also get probabilities for each class label. The `get` functions `getPrediction`(Truth, Response, Probabilites) can be used with the prediciton object to extract the associated information. 

For performance evaluation, we use the `performance` function with the prediction object and we optionally define the measures we want to evaluate. The default measure for multilabel classification is the *Hamming loss* (`hamloss`). The Hamming loss is the fraction of labels that are incorrectly predicted and therefore, the smaller the Hamming loss, the better the performance.


```{r}
# available measures
listMeasures("multilabel")

performance(pred1)

performance(pred2, measures = list(hamloss, timepredict))
```


#### Resampling

In the above examples we did not specify a resampling strategy. As usual, we prefer to evaluate the performance of a learning algorithm when trained with resampling. The function `resample` may be used as in other parts of this tutorial.

```{r}
rdesc <- makeResampleDesc("CV", stratify = FALSE, iters = 3)
r1 <- resample(multilabel.lrn1, yeast.task, rdesc, show.info = FALSE)
r1

r2 <- resample(multilabel.lrn2, yeast.task, rdesc, show.info = FALSE)
r2
```


#### Binary performance

We can also calculate binary performance measures such as the `mmce`, `auc`, and `acc` for each label using the prediction object returned from the model based on the binary relevance wrapper. The function to use here is `getMultilabelBinaryPerformances`. Note that to claclulate `auc`, we need predicted probabilities. 

```{r}
# on prediction object returned from predict
getMultilabelBinaryPerformances(pred1, measures = list(mmce, acc, auc))

# on prediction object returned from resample
getMultilabelBinaryPerformances(r1$pred, measures = list(mmce, acc, auc))
```


### Learning curves

The mlr package has functions used to generate learning curves and visualize them to improve the performance of a learner. Learning curves usually depict the training error and cross-validation (test) error with increasing data size. These curves are useful in the sense that they can be indicative of a high bias or high variance model. The function `generateLearningCurveData` controls the size of the training observations (through `percs`, the vector of percentages to be drawn from the training split), trains the learner on a single percentage of the training set and evaluates the chosen (aggregated) performance measures on the *complete* test set. The complete test set is specified in the resampling strategy (default is `Holdout`). This is repeated for all values in the percentage vector. Note that for each percentage value, randomly selected observations are drawn from the training set which can result in noisy performance measures. The noise can be reduced by increasing the number of iterations in `resampling`.


```{r}
r <- generateLearningCurveData(
  learners = list("classif.rpart", "classif.knn"),
  task = sonar.task,
  percs = seq(0.1, 1, by = 0.2),
  measures = list(tp, fp, tn, fn),
  resampling = makeResampleDesc("CV", iters = 5),
  show.info = FALSE
)

# facetted plots created for each measure, learners mapped to color
plotLearningCurve(r, facet = "measure")

# facetted plots created for each learner, measures mapped to color
plotLearningCurve(r, facet = "learner")
```

We can create more complicated learners and pass them on the `generateLearningCurveData` function, as shown in the example below.

```{r}
lrns <- list(
  makeLearner("classif.ksvm", id = "ksvm1", sigma = 0.2, C = 2),
  makeLearner("classif.ksvm", id = "ksvm2", sigma = 0.1, C = 1),
  "classif.randomForest"
)

rin <- makeResampleDesc("CV", iters = 5)

lc <- generateLearningCurveData(learners = lrns, task = sonar.task, percs = seq(.1, 1, by = 0.1), measures = acc, resampling = rin, show.info = FALSE)

plotLearningCurve(lc)
```

The following is experimental `plotLearningCurveGGVIS(r, interaction = "measure")`

### Partial prediction plots

#### Introduction

Machine learning algorithms use available features to make predictions but it is not always obvious how those features are used. Using the mlr package, we can use available functions which aim to estimate the depedence of a learned function on a subset of the feature space. Our response, $Y$ is described as:

$Y = f(X) + \epsilon$,

where $f(X)$ is some unknown function of the feature space, $f(X)$ and $\epsilon$ is a random error term independent of $X$ with mean zero. A learner gives some estimate $\hat{f}(X)$ to give $\hat{Y}$ which denotes predictions on $Y$. Essentially, after the most relevant variables have been identified, we also want to understand the nature of the dependence of the approximation $\hat{f}(X)$ on their joint values. For a feature space that is highly dimensional, $\hat{f}$ may be uninterpretable. Through partial dependence plots, we can more easily visualize how $\hat{f}$ uses the features to make predictions. 

Suppose $X$ is our feature space; we partition the space into two sets, $X_s$ and $X_c$ where $X = X_c \cup X_s$ and $X_s$ is the subset of features which are of interest. The partial dependence of $f$ on $X_s$ is:

$f_s = \mathbb{E}_{X_c}[f(X_s, X_c)] = \int f(X_s, X_c)\,dP(X_c)$;

this is the averaged value of $f$ when $X_s$ is fixed and $X_c$ varies over its marginal distribution, $dP(X_c)$. Since neither $f$ nor $dP(X_c)$ are known, we need to use an estimator to compute $\hat{f}_s$. Note that each subset of predictors $s$, has its own partial dependence function, $f_s$. The estimator function is:

$\hat{f}_s = \frac{1}{N} \sum _{i=1}^N \hat{f}(X_s, X_{ci})$,

where $\brace X_{c1}, \cdots, X_{cN}\rbrace$ represent the different values of $X_c$ that are observed in the training data.  The partial dependence functions defined represent the effect of $X_s$ on $f(X)$ after accounting for the (av- erage) effects of the other variables $X_c$ on $f(X)$. 

The conditional expectation of an observation $i$ can be estimated using the estimator function above (without the averaging). This has the advantage of discovering features which would otherwise be made unclear through the averaging. The partial prediction function, the individual conditional expectation as well as the partial derivatives of the above with respect to the features are computed. The partial derivatives can be useful in the following context. Suppose the estimate $\hat{f}_{X_s}$ is an additive function because of the lack of interactions between $X_s$ and other features $X_c$. Then:

$\hat{f}_{X_s, X_c} = g(X_s) + h(X_c)$

and if $\frac{\partial \hat{f}_{X_s}}{\partial X_s} = g'$, we can imply that $\hat{f}$ does not depend on $X_c$. If there exists variation in estimated partial derivative then this could mean that there is a region of interaction between $X_s$ and $X_c$ in $\hat{f}$. 

More information can be found [here](http://arxiv.org/pdf/1309.6392v2.pdf).

#### Generating partial predictions

To use the function `generatePartialPredictionData`, we need an object from `train`, input data (which can be a `data.frame` or a `task`), and a features character vector for which we want to want to calculate the partial prediction of `\hat{f}`. We need a feature grid for every element of the character vector features passed. The default is a uniform grid of length 10 (this may be set by `gridsize`) from the empirical min to the empirical max (these may also be set through `fmin` and `fmax`). Resampling is also available for the feature data (in the `features` vector) through the `bootstrap` or `resample` methods. 

```{r}
lrn.classif <- makeLearner("classif.ksvm", predict.type = "prob")
fit.classif <- train(lrn.classif, iris.task)
pd <- generatePartialPredictionData(fit.classif, input = iris.task, features = "Petal.Width")
pd$data[1:5, ]
pd$data[11:15, ]
```

Suppose now we pass two features in the `features` vector. If `interaction = FALSE` (default) then $X_s$ is assumed to be unidimensional and partial predictions are generated for each feature separately (`NA` is shown for the feature that has not been used for the prediction). If `interaction = TRUE`, then the individual feature grids are combined using the Cartesian product and the estimator produces a partial prediction for every combination of unique feature values.

```{r}
# interaction = FALSE
pd.lst <- generatePartialPredictionData(fit.classif, iris.task, c("Petal.Width", "Petal.Length"), interaction = FALSE)
head(pd.lst$data)
tail(pd.lst$data)

# interaction = TRUE
pd.int <- generatePartialPredictionData(fit.classif, iris.task, c("Petal.Width", "Petal.Length"), interaction = TRUE)
pd.int
```

The object `pd.int` creates a feature grid of $10 \times 10$ for every combination of feature values and estimates probabilities corresponding to each unique combination for each class in the target variable. From the definition of the estimator function, the mean prediction is returned which, here, are represented by the mean class probabilites. It is possible to return other summaries of the predictions using the `fun` argument. We show this using a `regression` task.

```{r}
lrn.regr <- makeLearner("regr.ksvm")
fit.regr <- train(lrn.regr, bh.task)
pd.regr <- generatePartialPredictionData(fit.regr, bh.task, "lstat", fun = median)
pd.regr
```

Note that the function argument must return a numeric vector of length 1 or 3.

```{r}
pd.ci <- generatePartialPredictionData(fit.regr, bh.task, "lstat", fun = function(x) quantile(x, c(.25, .5, .75)))
pd.ci
```


```{r}
pd.classif <- generatePartialPredictionData(fit.classif, iris.task, "Petal.Length", fun = median)
pd.classif
```

Using the argument `individual = TRUE`, we obtain te conditional expectation of the learned function at observation $i$. This is shown with a regression task below. The resulting object contains $N$ predictions for each point in the feature grid (so, for the `bh.task`, we have 506 obs times the 10 points in the gird therefore 5060 predictions). 

```{r}
pd.ind.regr <- generatePartialPredictionData(fit.regr, bh.task, "lstat", individual = TRUE)
pd.ind.regr
```

The `idx` column in the `$data` element gives the index of the observation. In classification tasks, `idx` gives the index corresponding to the observation and the class target label.

```{r}
pd.ind.classif <- generatePartialPredictionData(fit.classif, iris.task, "Petal.Length", individual = TRUE)
pd.ind.classif
```

If `individual = TRUE`, through the argument `center`, we can pass a fixed value for each feature in the `features` vector. This fixed value is used to center the data by subtracting it from each individual prediction made across the prediction grid. 

```{r}
iris <- getTaskData(iris.task)
pd.ind.classif <- generatePartialPredictionData(fit.classif, iris.task, "Petal.Length", individual = TRUE, center = list("Petal.Length" = min(iris$Petal.Length)))
```

Partial derivatives can also be computed for individual and aggregate partial predictions. This is restricted to one feature at a time. 

```{r}
pd.regr.der <- generatePartialPredictionData(fit.regr, bh.task, "lstat", derivative = TRUE)
head(pd.regr.der$data)

pd.regr.der.ind <- generatePartialPredictionData(fit.regr, bh.task, "lstat", individual = TRUE, derivative = TRUE)
head(pd.regr.der.ind$data)

pd.classif.der <- generatePartialPredictionData(fit.classif, iris.task, "Petal.Width", derivative = TRUE)
head(pd.classif.der$data)

pd.classif.der.ind <- generatePartialPredictionData(fit.classif, iris.task, "Petal.Width", derivative = TRUE, individual = TRUE)
head(pd.classif.der.ind$data)
```

#### Plotting partial predictions

The resulting object from `generatePartialPredictionData` can be visualized with the functions `plotPartialPrediction` and `plotPartialPredictionGGVIS`. 

**Regression: single feature**

The result is a line plot showing the dependence on the target on the chosen feature. The markers on the line correspond to the points in the prediction plot. 
```{r}
plotPartialPrediction(pd.regr)
```

**Classification: single feature**

In classification, we have a line plot of the class probability against the selected feature for each class in the target variable.

```{r}
plotPartialPrediction(pd.classif)
```


**Regression: single feature, `fun` enabled**

With the `fun` argument used, the bounds are automatically shown with a grey region. This is also true when the learner has `predict.type = "se"`.

```{r}
plotPartialPrediction(pd.ci)
```

**Regression: multiple features, `fun` enabled**

When multiple features are passed in the features vector but with `interaction = FALSE`, we obtain facetted plots to show the dependence of target on each feature.

```{r}
fit.se <- train(makeLearner("regr.randomForest", predict.type = "se"), bh.task)
pd.se <- generatePartialPredictionData(fit.se, bh.task, c("lstat", "crim"))
plotPartialPrediction(pd.se)
```

**Classification: multiple features**

```{r}
plotPartialPrediction(pd.lst)
```


**Classification example: `interaction = TRUE`**

When `interaction = TRUE`, one variable is used for facetting and for each value in the grid for the chosen variable, we get a partial prediction plot against the variable not used in facetting.

```{r}
plotPartialPrediction(pd.int, facet = "Petal.Length")
```

**Individual predictions**

With `individual = TRUE`, we obtain all individual conditional expectation curves.

```{r}
plotPartialPrediction(pd.ind.regr)
```

When the data is centered (by some given fixed value of $X_s$), we obtain a fixed intercept, as shown below.

```{r}
plotPartialPrediction(pd.ind.classif)
```

**Plotting derivatives**

This works the same way: we use the returned object when `derivative = TRUE` to return the derivatives. What we are looking for here is variation in the derivative with the selected feature. If no variation exists, the result would suggest that the model is additive in the selected feature and no interaction occurs with other predictors.

```{r}
plotPartialPrediction(pd.regr.der)
```

```{r}
plotPartialPrediction(pd.regr.der.ind)
```

```{r}
plotPartialPrediction(pd.classif.der.ind)
```

These plots can identify regions where there may be interactions with the selected features, which can then further be investigated.

### Classifier calibration

A calibrated classifiers is one where the predicted probability of a class closely matches the frequency of that class. The function `generateCalibrationData` takes as an input an object or a list of objects returned from `predict`, `resample`, or an object returned from  `benchmark`. The objects must be from a binary or multiclass classification task with learners that support `predict.type = "prob"`. The function groups data points with similar predicted probabilites for each class. The object has elements `$data`, `$proportion`, and `$task`. The `$proportion` element is a `data.frame` with `Learner`, `bin`, `Class`, and `Proportion` columns. The `Proportion` column gives the proportion of observations from class `Class` for each bin among all observations with posterior probabilities of that class. For example, for observations that are predicted to have class "A" with probability (0,0.1], what is the proportion of said observations which have class "A"? The bins are calculated according to the `breaks` or `groups` argument (default is `breaks = "Sturges"`).

```{r}
lrn <- makeLearner("classif.rpart", predict.type = "prob")
mod <- train(lrn, task = sonar.task)
pred <- predict(mod, task = sonar.task)
cal <- generateCalibrationData(pred)
cal$proportion
```

With `groups = n` where `n` is the number of groups, `cut2` is used to create bins with approximately equal number of observations in each group. 

```{r}
cal <- generateCalibrationData(pred, groups = 3)
cal$proportion
```

The function `plotCalibration` uses the object returned from `generateCalibrationData` to plot the proportion of each class in each bin against the bins. 

```{r}
plotCalibration(cal)
```

The function also plots a reference line representing the perfect classifier. A rug plot (this draws ticks for each value) is also shown at the top and bottom. At the top, the ticks represent the "positive" class in each bin while at the bottom, the ticks represent the "negative" class. A perfect classifier would have all the positive classes at the top right (correct class predicted with high probability) and all the negative classes at the bottom left. 

In the plot above, we only had the three bins but, in situations where we have multiple bins, because of the discretization of the probabilities, it might be best to use `smooth = TRUE` which replaces the estimated proportions with a loess smoother. 

```{r}
cal <- generateCalibrationData(pred)
plotCalibration(cal, smooth = TRUE)
```

While the plots were generated for a binary classification task, they do generalize to mutliclass classification examples. We show this with our favorite `iris.task`.

```{r}
lrns <- list(
  makeLearner("classif.randomForest", predict.type = "prob"),
  makeLearner("classif.nnet", predict.type = "prob", trace = FALSE)
)

mod <- lapply(lrns, train, task = iris.task)
pred <- lapply(mod, predict, task = iris.task)
names(pred) = c("randomForest", "nnet")
cal <- generateCalibrationData(pred, breaks = c(0, .3, .6, 1))
plotCalibration(cal)
```


