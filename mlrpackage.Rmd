---
title: "Working with the mlr package"
output: html_document
---

To get acquainted with the mlr package, I created an Rmarkdown document with basic and advanced features from the mlr tutorial on regression and classification problems with data available in R. 

```{r, echo=FALSE, message=FALSE}
library(mlr)
library(randomForest)
library(gbm)
library(mlbench)
library(rattle)
```
## Getting started

The first step is to create a **task**; this can be a classification, regression, survival, cluster, cost-sensitive classification or multilabel task. For example, let us start with the `iris` data set from which we wish to predict the `Species` based on features comprised of width and length measurements of sepals and petals. 

```{r}
task <- makeClassifTask(data = iris, target = "Species")
```

There exist other options that can be set when calling the task function like, for instance, weights and blocking. The latter is for observations that are required to be considered together such that when resampling or cross-validating, grouped observations are included either all in the train or all in the test set.

Next, we choose a learner. Here, we choose to learn based on a classification tree fitted through the `rpart` function. 

```{r}
lrn <- makeLearner("classif.rpart")
```

We can get a description of all possible parameter settings for a learner using `?getParamSet(lrn)`:

```{r, results='markup'}
getParamSet(lrn)
```

Further, we can create a description object for a resampling startegy using `makeResampleDesc`. For example, we may wish to carry out a 3-fold cross-validation of `rpart` on `iris`.

```{r}
cv3f <- makeResampleDesc("CV", iters = 3, stratify = TRUE )
```

Finally, we can fit the model specified by `lrn` on the `task` and calculate predictions and performance measures for all training and all test sets specified by the above resampling description.

```{r, message=FALSE}
set.seed(123)
r <- resample(lrn, task, cv3f)
r$aggr 
```

The mean misclassification error is given by `r r$aggr`. The default measure for a classification task is the misclassification error, `mmce` but it is possible to choose a different measure, say *accuracy*, `acc` which is given by  1 - `mmce`. We will see later that we can also define our own measures. 

```{r, message=FALSE}
set.seed(123)
r <- resample(lrn, task, cv3f, measures = acc)
r$aggr 
```



## Basics

### Tasks

As previously mentioned, depending on the type of problem at hand, one can define an appropriate task. We have seen an instance of a classification problem above, let us now look at a supervised regression problem using the `BostonHousing` data. By printing task, we get some information on the task object we have just created and the associated data.

```{r}
data(BostonHousing, package = "mlbench")
task <- makeRegrTask(data = BostonHousing, target = "medv")
print(task)
```

#### Modifying a task

Once the task is created, it can be modified in several ways. For example, the function `subsetTask` allows the selection of certain observations and/or features. 

```{r}
names(BostonHousing)
modifiedtask <- subsetTask(task, subset = 1:400, features = c("crim", "age", "dis", "lstat"))
str(getTaskData(modifiedtask))
```

Using `getTaskData` we can see that the modified task now includes 400 observations and 5 variables (the 4 included in the features character vector above and the target variable which will always be included). Some other useful functions are the following:

  1. `removeConstantFeatures(<task name>)`:
  
  constant features may arise dur to an inherent feature in the data collected or as a result of choosing a subset of observations.
  
  2. `dropFeatures(task, c("rm", "nox"))`:
  
  remove selected features from the task.
  
  3. `normalizeFeatures(task, method = "standardize")`:
  
  normalize numerical features by different methods (nonnumerical features are left untouched). The normalizing method can be one of `center` (subtract mean), `scale` (divide by standard deviation), `standardize` (center and scale), `range` (scale to a given range - default is [0, 1]). The optional argument `exclude` may be used to supply a character vector of columns to be excluded from the normalization. Type `?normalizeFeatures` for more info.

### Learners

Many of the popular learning algorithms are already implemented in `mlr`. The `makeLearner` function requires the user to specify the learning method. Additionally, it is possible to modify defaults on the prediction type (i.e. for classification, we may choose `predict.type = "prob"` for probabilities), or set hyperparameters using a `list` passed to the `par.vals` argument.

```{r}
class.lrn <- makeLearner("classif.randomForest", predict.type = "prob", fix.factors.prediction = TRUE)
regr.lrn <- makeLearner("regr.gbm", par.vals = list(n.trees = 500, interaction.depth = 4))
#cluster.lrn <- makeLearner("cluster.SimpleKMeans", N = 5)
```

Note that the `fix.factors.prediction = TRUE` argument is useful in situations where a factor level is present in the training data set but not the test data set; it adds a factor level for missing data in the test set thus avoiding problems. 

The Learner object is a list and information can be extracted using the `$`, e.g. `regr.lrn$par.vals`. This information can also be accessed using other functions in `mlr`, for instance, `getHyperPars(lrn)` retrieves the current hyperparameter settings of the learner `lrn`. We also used above `getParamSet(lrn)` to get a description of all possible parameter settings for `lrn`. 

#### Modifying a learner

Just like it was possible to modify an existing task, we can also do so for a learner. Modifications include changing the `id` (this is either user specified or, if omitted, it is automatically set to the algorithm name), the prediction type, hyperparameter values, and more.

```{r}
class.lrn <- setPredictType(class.lrn, "response")
regr.lrn <- setHyperPars(regr.lrn, n.trees = 400)
regr.lrn <- removeHyperPars(regr.lrn, c("n.trees", "interaction.depth"))
```

Note `removeHyperPars` sets the hyperparameters back to their default values.

### Training

Once we create the task from the data set and identify the learning algorithm, the next step is to train the learner using the `train` command.

```{r}
task <- makeClassifTask(data = iris, target = "Species")
lrn <- makeLearner("classif.lda")
mod <- train(lrn, task)
mod
```

The function `train` returns a list. The fitted model can be extracted using the `getLearnerModel(mod)` command. It is possible to choose a subset of observations to be used to train the model; this is achieved via the `subset` argument in `train`. Note that this is usually not needed since resampling strategies are supported. In the following example, we use the `BostonHousing` data to fit a linear model to the regression task. Note that `getTaskSize` returns the number of observations in a task.

```{r}
data(BostonHousing, package = "mlbench")
task <- makeRegrTask(data = BostonHousing, target = "medv")
lrn <- makeLearner("regr.lm")
train.set <- sample(getTaskSize(task), size = getTaskSize(task)/3)
mod <- train(lrn, task, subset = train.set)
getLearnerModel(mod)
```

Finally, a note on weights passed as an argument in the `train` function. As an example for an application of weights[^ note that mlr offers alternatives with more functionality for imbalanced classification problems] used in `train`, consider the `BreastCancer` data for which the target variable `Class` identifies 241 malignant and 458 benign cases. To deal with the imbalanced classes, we can incorporate weights in an attempt to allow the two classes to be equally represented in training the classifier. Here we use the predefined task `bc.task` in mlr. The `getTaskTargets` function gets the target data from a task (in this example, this is equivalent to the vector `BreastCancer$Class`). Note that if weights are defined in task as well then those would be overwritten by the weights in `train`. 

```{r}
target <- getTaskTargets(bc.task)
tab <- as.numeric(table(target))
#obtain inverse class frequencies for the weights
w <- 1/tab[target] 
mod <- train("classif.rpart", task = bc.task, weights = w)
fancyRpartPlot(getLearnerModel(mod), sub = "")
```

Weights can also be useful as a means to grant more importance to recently collected data versus older data or to reduce the influence of outliers.



